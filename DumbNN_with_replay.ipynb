{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(5, 64)\n",
    "        #self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        #self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "η = 1e-3\n",
    "model = Net()\n",
    "opt = Adam(model.parameters(), lr = η)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "replay_array = []\n",
    "\n",
    "for epoch in range(1000): \n",
    "    \n",
    "    curr = env.reset()\n",
    "    curr = torch.from_numpy(curr).float()\n",
    "    for i in range(200):\n",
    "\n",
    "        # Generate a random step \n",
    "        st = random.randint(0,1)\n",
    "        nex, rew, done, info = env.step(st)\n",
    "        nex = torch.from_numpy(nex).float()\n",
    "        replay_array.append((curr, st, nex, rew, done))\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Create input for our network and generate prediction\n",
    "#         input = torch.from_numpy(np.append(curr,st)).float()\n",
    "#         nex_pred = model(input)\n",
    "\n",
    "#         # Calculate loss\n",
    "#         loss = loss_fn(nex_pred, nex)\n",
    "\n",
    "#         # Backprop\n",
    "#         opt.zero_grad()\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "        \n",
    "        curr = nex\n",
    "        \n",
    "#     l.append(loss.item())\n",
    "    \n",
    "#     epoch % 1000 == 0 and print(\"Epoch %d done\" % epoch)\n",
    "    \n",
    "#plt.plot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-87502e80e56f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreplay_array_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "replay_array_temp = np.array(replay_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13685868c88>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = []\n",
    "replay_size = len(replay_array)\n",
    "for epoch in range(2*replay_size): \n",
    "    i = random.randint(0, replay_size-1)\n",
    "    curr = replay_array[i][0]\n",
    "    st = replay_array[i][1]\n",
    "    nex = replay_array[i][2]\n",
    "    # Create input for our network and generate prediction\n",
    "    input = torch.from_numpy(np.append(curr,st)).float()\n",
    "    nex_pred = model(input)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_fn(nex_pred, nex)\n",
    "\n",
    "    # Backprop\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    curr = nex\n",
    "\n",
    "    l.append(loss.item())\n",
    "    \n",
    "    #epoch % 1000== 0 and print(\"Epoch %d done\" % epoch)\n",
    "    \n",
    "plt.plot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00118953175842762"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([ 0.0102,  0.2202,  0.0297, -0.2402])\n",
      "nex_pred =  tensor([ 0.0080,  0.2193,  0.0226, -0.2332], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010612431651679799\n",
      "nex =  tensor([-0.0208, -0.1583, -0.0390,  0.2653])\n",
      "nex_pred =  tensor([-0.0114, -0.1668, -0.0219,  0.2661], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00045045497245155275\n",
      "nex =  tensor([ 0.0329,  0.1894,  0.0230, -0.2361])\n",
      "nex_pred =  tensor([ 0.0267,  0.1886,  0.0097, -0.2274], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002911727933678776\n",
      "nex =  tensor([ 0.0379,  0.1977,  0.0332, -0.2928])\n",
      "nex_pred =  tensor([ 0.0273,  0.1974,  0.0125, -0.2823], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0006513827829621732\n",
      "nex =  tensor([-0.0307,  0.2075,  0.0356, -0.2670])\n",
      "nex_pred =  tensor([-0.0296,  0.2067,  0.0327, -0.2607], grad_fn=<ThAddBackward>)\n",
      "loss =  5.026323196943849e-05\n",
      "nex =  tensor([-0.0477, -0.2423, -0.0247,  0.3343])\n",
      "nex_pred =  tensor([-0.0402, -0.2517, -0.0092,  0.3374], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003934009582735598\n",
      "nex =  tensor([-0.0315,  0.1506, -0.0290, -0.2671])\n",
      "nex_pred =  tensor([-0.0240,  0.1466, -0.0205, -0.2643], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00015217244799714535\n",
      "nex =  tensor([-0.0372, -0.1612,  0.0448,  0.3012])\n",
      "nex_pred =  tensor([-0.0348, -0.1672,  0.0441,  0.3088], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010006575030274689\n",
      "nex =  tensor([-0.0238, -0.1710,  0.0245,  0.2747])\n",
      "nex_pred =  tensor([-0.0218, -0.1762,  0.0237,  0.2821], grad_fn=<ThAddBackward>)\n",
      "loss =  8.579536370234564e-05\n",
      "nex =  tensor([ 0.0353, -0.1599,  0.0080,  0.2974])\n",
      "nex_pred =  tensor([ 0.0338, -0.1638,  0.0026,  0.3040], grad_fn=<ThAddBackward>)\n",
      "loss =  9.043738100444898e-05\n",
      "nex =  tensor([-0.0225,  0.2254,  0.0179, -0.2609])\n",
      "nex_pred =  tensor([-0.0186,  0.2239,  0.0202, -0.2566], grad_fn=<ThAddBackward>)\n",
      "loss =  4.1217776015400887e-05\n",
      "nex =  tensor([-0.0043,  0.2059, -0.0492, -0.3395])\n",
      "nex_pred =  tensor([ 0.0033,  0.2027, -0.0408, -0.3380], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001424989168299362\n",
      "nex =  tensor([-0.0166, -0.1858,  0.0429,  0.3017])\n",
      "nex_pred =  tensor([-0.0175, -0.1897,  0.0362,  0.3111], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001493704185122624\n",
      "nex =  tensor([ 0.0253, -0.1606,  0.0360,  0.3137])\n",
      "nex_pred =  tensor([ 0.0218, -0.1641,  0.0257,  0.3221], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00019986339611932635\n",
      "nex =  tensor([ 0.0072, -0.2299, -0.0281,  0.3232])\n",
      "nex_pred =  tensor([ 0.0110, -0.2357, -0.0217,  0.3281], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011232907127123326\n",
      "nex =  tensor([-0.0025, -0.1655,  0.0470,  0.3355])\n",
      "nex_pred =  tensor([-0.0034, -0.1702,  0.0406,  0.3435], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012710093869827688\n",
      "nex =  tensor([-0.0177,  0.2291,  0.0098, -0.2439])\n",
      "nex_pred =  tensor([-0.0123,  0.2271,  0.0149, -0.2406], grad_fn=<ThAddBackward>)\n",
      "loss =  7.043121149763465e-05\n",
      "nex =  tensor([-0.0117,  0.2254, -0.0463, -0.3448])\n",
      "nex_pred =  tensor([-0.0028,  0.2221, -0.0355, -0.3440], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002077723911497742\n",
      "nex =  tensor([-0.0214, -0.2293,  0.0420,  0.3217])\n",
      "nex_pred =  tensor([-0.0233, -0.2324,  0.0338,  0.3322], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001908379199448973\n",
      "nex =  tensor([-0.0005, -0.2195,  0.0212,  0.2936])\n",
      "nex_pred =  tensor([-0.0025, -0.2224,  0.0136,  0.3032], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016139072249643505\n",
      "nex =  tensor([ 0.0074, -0.1965, -0.0116,  0.2892])\n",
      "nex_pred =  tensor([ 0.0096, -0.2014, -0.0096,  0.2950], grad_fn=<ThAddBackward>)\n",
      "loss =  6.714369374094531e-05\n",
      "nex =  tensor([-0.0004,  0.2152, -0.0243, -0.3404])\n",
      "nex_pred =  tensor([ 0.0034,  0.2133, -0.0226, -0.3367], grad_fn=<ThAddBackward>)\n",
      "loss =  3.455264959484339e-05\n",
      "nex =  tensor([-0.0425, -0.2238,  0.0449,  0.2985])\n",
      "nex_pred =  tensor([-0.0431, -0.2275,  0.0388,  0.3092], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001654969237279147\n",
      "nex =  tensor([-0.0054,  0.1693,  0.0290, -0.2868])\n",
      "nex_pred =  tensor([-0.0093,  0.1689,  0.0168, -0.2779], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000241386384004727\n",
      "nex =  tensor([-0.0272,  0.2210,  0.0190, -0.2754])\n",
      "nex_pred =  tensor([-0.0237,  0.2196,  0.0205, -0.2708], grad_fn=<ThAddBackward>)\n",
      "loss =  3.802341962000355e-05\n",
      "nex =  tensor([-0.0201, -0.1572, -0.0277,  0.2508])\n",
      "nex_pred =  tensor([-0.0125, -0.1645, -0.0150,  0.2532], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000278439256362617\n",
      "nex =  tensor([ 0.0363,  0.2355, -0.0340, -0.2995])\n",
      "nex_pred =  tensor([ 0.0394,  0.2338, -0.0327, -0.2963], grad_fn=<ThAddBackward>)\n",
      "loss =  2.457927621435374e-05\n",
      "nex =  tensor([ 0.0014, -0.2220,  0.0163,  0.2851])\n",
      "nex_pred =  tensor([-0.0006, -0.2247,  0.0089,  0.2946], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00015789509052410722\n",
      "nex =  tensor([-0.0299, -0.2445,  0.0492,  0.3013])\n",
      "nex_pred =  tensor([-0.0331, -0.2462,  0.0376,  0.3137], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00030001060804352164\n",
      "nex =  tensor([-0.0302, -0.2209,  0.0140,  0.3230])\n",
      "nex_pred =  tensor([-0.0275, -0.2269,  0.0163,  0.3298], grad_fn=<ThAddBackward>)\n",
      "loss =  9.571673581376672e-05\n",
      "nex =  tensor([-0.0187, -0.1490,  0.0130,  0.3258])\n",
      "nex_pred =  tensor([-0.0126, -0.1568,  0.0204,  0.3290], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001619131217012182\n",
      "nex =  tensor([ 0.0278,  0.1552,  0.0407, -0.2530])\n",
      "nex_pred =  tensor([ 0.0173,  0.1551,  0.0192, -0.2417], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000701495970133692\n",
      "nex =  tensor([ 0.0367,  0.1982,  0.0450, -0.2891])\n",
      "nex_pred =  tensor([ 0.0244,  0.1982,  0.0217, -0.2777], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0008249065722338855\n",
      "nex =  tensor([-0.0135,  0.1491,  0.0273, -0.3105])\n",
      "nex_pred =  tensor([-0.0177,  0.1488,  0.0135, -0.3012], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00029384251683950424\n",
      "nex =  tensor([ 0.0486,  0.2085,  0.0310, -0.3292])\n",
      "nex_pred =  tensor([ 0.0356,  0.2083,  0.0067, -0.3181], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0008800224168226123\n",
      "nex =  tensor([-0.0097,  0.1633, -0.0275, -0.3434])\n",
      "nex_pred =  tensor([-0.0070,  0.1609, -0.0284, -0.3387], grad_fn=<ThAddBackward>)\n",
      "loss =  3.569881300791167e-05\n",
      "nex =  tensor([-0.0222,  0.2311,  0.0422, -0.2450])\n",
      "nex_pred =  tensor([-0.0212,  0.2303,  0.0398, -0.2392], grad_fn=<ThAddBackward>)\n",
      "loss =  4.1002447687787935e-05\n",
      "nex =  tensor([ 0.0334, -0.2406,  0.0198,  0.3139])\n",
      "nex_pred =  tensor([ 0.0271, -0.2417,  0.0051,  0.3245], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003701325040310621\n",
      "nex =  tensor([ 0.0423,  0.2267,  0.0036, -0.2947])\n",
      "nex_pred =  tensor([ 0.0374,  0.2259, -0.0073, -0.2875], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00019604813132900745\n",
      "nex =  tensor([-0.0287,  0.1968,  0.0225, -0.2662])\n",
      "nex_pred =  tensor([-0.0264,  0.1954,  0.0216, -0.2605], grad_fn=<ThAddBackward>)\n",
      "loss =  4.068057387485169e-05\n",
      "nex =  tensor([-0.0469, -0.1794,  0.0064,  0.2792])\n",
      "nex_pred =  tensor([-0.0411, -0.1870,  0.0147,  0.2840], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00018431790522299707\n",
      "nex =  tensor([-0.0474,  0.2174,  0.0318, -0.3301])\n",
      "nex_pred =  tensor([-0.0455,  0.2171,  0.0296, -0.3241], grad_fn=<ThAddBackward>)\n",
      "loss =  4.3912594264838845e-05\n",
      "nex =  tensor([ 0.0409,  0.1773, -0.0346, -0.2885])\n",
      "nex_pred =  tensor([ 0.0406,  0.1755, -0.0389, -0.2833], grad_fn=<ThAddBackward>)\n",
      "loss =  4.883629299001768e-05\n",
      "nex =  tensor([ 0.0086, -0.1951,  0.0259,  0.3054])\n",
      "nex_pred =  tensor([ 0.0066, -0.1984,  0.0182,  0.3141], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00015163858188316226\n",
      "nex =  tensor([ 0.0442,  0.1925, -0.0335, -0.3052])\n",
      "nex_pred =  tensor([ 0.0434,  0.1909, -0.0387, -0.3000], grad_fn=<ThAddBackward>)\n",
      "loss =  5.72573444515001e-05\n",
      "nex =  tensor([-0.0289,  0.2408, -0.0460, -0.2917])\n",
      "nex_pred =  tensor([-0.0150,  0.2362, -0.0260, -0.2936], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0006190987187437713\n",
      "nex =  tensor([ 0.0468,  0.1992, -0.0220, -0.3385])\n",
      "nex_pred =  tensor([ 0.0427,  0.1981, -0.0328, -0.3319], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001791975082596764\n",
      "nex =  tensor([ 0.0129, -0.2230, -0.0226,  0.2973])\n",
      "nex_pred =  tensor([ 0.0148, -0.2276, -0.0203,  0.3034], grad_fn=<ThAddBackward>)\n",
      "loss =  6.774545909138396e-05\n",
      "nex =  tensor([ 0.0451, -0.1538, -0.0416,  0.2844])\n",
      "nex_pred =  tensor([ 0.0490, -0.1591, -0.0349,  0.2871], grad_fn=<ThAddBackward>)\n",
      "loss =  9.713417966850102e-05\n",
      "nex =  tensor([ 0.0394,  0.1908,  0.0224, -0.2804])\n",
      "nex_pred =  tensor([ 0.0306,  0.1903,  0.0046, -0.2707], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004888635012321174\n",
      "nex =  tensor([-0.0275, -0.1614, -0.0278,  0.2427])\n",
      "nex_pred =  tensor([-0.0198, -0.1690, -0.0147,  0.2452], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002936850651167333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([-0.0299, -0.2218, -0.0466,  0.3219])\n",
      "nex_pred =  tensor([-0.0205, -0.2314, -0.0272,  0.3231], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0005592486704699695\n",
      "nex =  tensor([-0.0217,  0.2438, -0.0364, -0.2740])\n",
      "nex_pred =  tensor([-0.0091,  0.2398, -0.0188, -0.2751], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004857191815972328\n",
      "nex =  tensor([ 0.0393,  0.1841,  0.0017, -0.2652])\n",
      "nex_pred =  tensor([ 0.0343,  0.1830, -0.0099, -0.2574], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002217357832705602\n",
      "nex =  tensor([-0.0417,  0.1925, -0.0337, -0.3262])\n",
      "nex_pred =  tensor([-0.0324,  0.1885, -0.0221, -0.3249], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00023885100381448865\n",
      "nex =  tensor([-0.0216, -0.1507,  0.0345,  0.3180])\n",
      "nex_pred =  tensor([-0.0184, -0.1573,  0.0357,  0.3238], grad_fn=<ThAddBackward>)\n",
      "loss =  8.832026651361957e-05\n",
      "nex =  tensor([-0.0076, -0.1902,  0.0481,  0.3389])\n",
      "nex_pred =  tensor([-0.0092, -0.1943,  0.0403,  0.3479], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016093887097667903\n",
      "nex =  tensor([-0.0408,  0.1756, -0.0335, -0.3519])\n",
      "nex_pred =  tensor([-0.0335,  0.1720, -0.0258, -0.3496], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00013100457726977766\n",
      "nex =  tensor([-0.0033,  0.1986, -0.0135, -0.3112])\n",
      "nex_pred =  tensor([-0.0005,  0.1968, -0.0136, -0.3066], grad_fn=<ThAddBackward>)\n",
      "loss =  3.2227279007202014e-05\n",
      "nex =  tensor([ 0.0180,  0.1526,  0.0209, -0.2829])\n",
      "nex_pred =  tensor([ 0.0112,  0.1521,  0.0044, -0.2732], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00041212307405658066\n",
      "nex =  tensor([-0.0246, -0.1878, -0.0382,  0.2825])\n",
      "nex_pred =  tensor([-0.0160, -0.1962, -0.0220,  0.2843], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00040680819074623287\n",
      "nex =  tensor([-0.0212, -0.1818, -0.0194,  0.3232])\n",
      "nex_pred =  tensor([-0.0131, -0.1904, -0.0056,  0.3250], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003326456935610622\n",
      "nex =  tensor([-0.0102, -0.1950,  0.0142,  0.2673])\n",
      "nex_pred =  tensor([-0.0100, -0.1989,  0.0109,  0.2756], grad_fn=<ThAddBackward>)\n",
      "loss =  9.491747914580628e-05\n",
      "nex =  tensor([ 0.0036,  0.2170,  0.0151, -0.2902])\n",
      "nex_pred =  tensor([ 0.0026,  0.2161,  0.0093, -0.2837], grad_fn=<ThAddBackward>)\n",
      "loss =  7.786432252032682e-05\n",
      "nex =  tensor([-0.0296, -0.2097,  0.0463,  0.2611])\n",
      "nex_pred =  tensor([-0.0320, -0.2119,  0.0359,  0.2730], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002603476750664413\n",
      "nex =  tensor([-0.0311, -0.2189, -0.0062,  0.3325])\n",
      "nex_pred =  tensor([-0.0256, -0.2266,  0.0029,  0.3369], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000193518862943165\n",
      "nex =  tensor([ 0.0342,  0.1807, -0.0059, -0.2684])\n",
      "nex_pred =  tensor([ 0.0310,  0.1794, -0.0148, -0.2614], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00014154774544294924\n",
      "nex =  tensor([-0.0407,  0.2102, -0.0132, -0.3383])\n",
      "nex_pred =  tensor([-0.0339,  0.2076, -0.0063, -0.3356], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010794660920510069\n",
      "nex =  tensor([ 0.0420,  0.1992, -0.0393, -0.2990])\n",
      "nex_pred =  tensor([ 0.0431,  0.1974, -0.0413, -0.2947], grad_fn=<ThAddBackward>)\n",
      "loss =  2.656241849763319e-05\n",
      "nex =  tensor([-0.0427,  0.1806, -0.0242, -0.2511])\n",
      "nex_pred =  tensor([-0.0323,  0.1761, -0.0103, -0.2500], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003206244728062302\n",
      "nex =  tensor([-0.0236, -0.2258, -0.0278,  0.3151])\n",
      "nex_pred =  tensor([-0.0172, -0.2335, -0.0159,  0.3188], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002558198175393045\n",
      "nex =  tensor([ 0.0250, -0.1699,  0.0465,  0.3491])\n",
      "nex_pred =  tensor([ 0.0209, -0.1735,  0.0350,  0.3577], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002383185928920284\n",
      "nex =  tensor([ 0.0304,  0.2446, -0.0296, -0.2654])\n",
      "nex_pred =  tensor([ 0.0355,  0.2425, -0.0247, -0.2631], grad_fn=<ThAddBackward>)\n",
      "loss =  5.996932304697111e-05\n",
      "nex =  tensor([ 0.0417, -0.2419, -0.0151,  0.2603])\n",
      "nex_pred =  tensor([ 0.0372, -0.2432, -0.0251,  0.2698], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00021317256323527545\n",
      "nex =  tensor([-0.0043, -0.2286,  0.0247,  0.3172])\n",
      "nex_pred =  tensor([-0.0060, -0.2318,  0.0177,  0.3266], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00015037263801787049\n",
      "nex =  tensor([ 0.0083, -0.1776,  0.0069,  0.2580])\n",
      "nex_pred =  tensor([ 0.0080, -0.1811,  0.0029,  0.2657], grad_fn=<ThAddBackward>)\n",
      "loss =  8.83635802892968e-05\n",
      "nex =  tensor([-0.0073, -0.2114, -0.0218,  0.3236])\n",
      "nex_pred =  tensor([-0.0019, -0.2183, -0.0128,  0.3275], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00017481295799370855\n",
      "nex =  tensor([ 0.0125, -0.1881, -0.0345,  0.2796])\n",
      "nex_pred =  tensor([ 0.0172, -0.1938, -0.0268,  0.2835], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012883456656709313\n",
      "nex =  tensor([-0.0181, -0.1697, -0.0358,  0.2323])\n",
      "nex_pred =  tensor([-0.0112, -0.1765, -0.0237,  0.2353], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00024819147074595094\n",
      "nex =  tensor([ 0.0404, -0.1748,  0.0148,  0.2554])\n",
      "nex_pred =  tensor([ 0.0350, -0.1766,  0.0020,  0.2648], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00028649054002016783\n",
      "nex =  tensor([ 0.0316, -0.2199, -0.0274,  0.3242])\n",
      "nex_pred =  tensor([ 0.0333, -0.2247, -0.0252,  0.3297], grad_fn=<ThAddBackward>)\n",
      "loss =  6.042501991032623e-05\n",
      "nex =  tensor([-0.0151, -0.1814,  0.0207,  0.2910])\n",
      "nex_pred =  tensor([-0.0136, -0.1863,  0.0195,  0.2983], grad_fn=<ThAddBackward>)\n",
      "loss =  8.130916103255004e-05\n",
      "nex =  tensor([ 0.0316, -0.2102, -0.0118,  0.3143])\n",
      "nex_pred =  tensor([ 0.0313, -0.2141, -0.0140,  0.3209], grad_fn=<ThAddBackward>)\n",
      "loss =  6.421899888664484e-05\n",
      "nex =  tensor([ 0.0254,  0.2286,  0.0199, -0.2830])\n",
      "nex_pred =  tensor([ 0.0210,  0.2280,  0.0095, -0.2754], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001846973318606615\n",
      "nex =  tensor([-0.0342,  0.2314, -0.0073, -0.3052])\n",
      "nex_pred =  tensor([-0.0266,  0.2289,  0.0015, -0.3031], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001460172497900203\n",
      "nex =  tensor([ 0.0156,  0.1516, -0.0396, -0.3461])\n",
      "nex_pred =  tensor([ 0.0163,  0.1493, -0.0439, -0.3411], grad_fn=<ThAddBackward>)\n",
      "loss =  4.9506761570228264e-05\n",
      "nex =  tensor([ 0.0236,  0.2200, -0.0009, -0.2757])\n",
      "nex_pred =  tensor([ 0.0229,  0.2188, -0.0055, -0.2700], grad_fn=<ThAddBackward>)\n",
      "loss =  5.639019946102053e-05\n",
      "nex =  tensor([ 0.0460, -0.1985,  0.0317,  0.3087])\n",
      "nex_pred =  tensor([ 0.0380, -0.1999,  0.0144,  0.3192], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00047553435433655977\n",
      "nex =  tensor([ 0.0277,  0.1818, -0.0026, -0.2658])\n",
      "nex_pred =  tensor([ 0.0251,  0.1805, -0.0107, -0.2588], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012175742449471727\n",
      "nex =  tensor([-0.0079,  0.2375, -0.0277, -0.3247])\n",
      "nex_pred =  tensor([-0.0008,  0.2351, -0.0200, -0.3229], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011870480375364423\n",
      "nex =  tensor([ 0.0070,  0.2036,  0.0098, -0.2776])\n",
      "nex_pred =  tensor([ 0.0061,  0.2025,  0.0043, -0.2711], grad_fn=<ThAddBackward>)\n",
      "loss =  7.458821346517652e-05\n",
      "nex =  tensor([ 0.0408,  0.2206,  0.0232, -0.3196])\n",
      "nex_pred =  tensor([ 0.0314,  0.2203,  0.0048, -0.3101], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0005172422388568521\n",
      "nex =  tensor([-0.0015, -0.2065,  0.0293,  0.3090])\n",
      "nex_pred =  tensor([-0.0033, -0.2098,  0.0219,  0.3182], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001536969793960452\n",
      "nex =  tensor([ 0.0185,  0.2053, -0.0480, -0.2949])\n",
      "nex_pred =  tensor([ 0.0248,  0.2025, -0.0415, -0.2929], grad_fn=<ThAddBackward>)\n",
      "loss =  9.38319499255158e-05\n",
      "nex =  tensor([-0.0179,  0.1855,  0.0016, -0.3053])\n",
      "nex_pred =  tensor([-0.0160,  0.1839, -0.0003, -0.2997], grad_fn=<ThAddBackward>)\n",
      "loss =  4.0894319681683555e-05\n",
      "nex =  tensor([-0.0100, -0.1706, -0.0391,  0.2744])\n",
      "nex_pred =  tensor([-0.0018, -0.1783, -0.0245,  0.2761], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00034013402182608843\n",
      "nex =  tensor([-0.0390, -0.2321, -0.0059,  0.3391])\n",
      "nex_pred =  tensor([-0.0334, -0.2401,  0.0039,  0.3436], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00021331421157810837\n",
      "nex =  tensor([-0.0009, -0.2104,  0.0017,  0.3279])\n",
      "nex_pred =  tensor([ 0.0012, -0.2158,  0.0031,  0.3340], grad_fn=<ThAddBackward>)\n",
      "loss =  7.266971078934148e-05\n",
      "nex =  tensor([ 0.0162, -0.2234,  0.0391,  0.3397])\n",
      "nex_pred =  tensor([ 0.0112, -0.2255,  0.0258,  0.3500], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00031456435681320727\n",
      "nex =  tensor([-0.0456, -0.1826, -0.0364,  0.2920])\n",
      "nex_pred =  tensor([-0.0351, -0.1930, -0.0157,  0.2922], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0006487024365924299\n",
      "nex =  tensor([ 0.0383,  0.1496,  0.0069, -0.2653])\n",
      "nex_pred =  tensor([ 0.0310,  0.1487, -0.0092, -0.2561], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00040061012259684503\n",
      "nex =  tensor([-0.0490,  0.1761, -0.0129, -0.3168])\n",
      "nex_pred =  tensor([-0.0423,  0.1730, -0.0062, -0.3135], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001087575510609895\n",
      "nex =  tensor([-0.0313,  0.2096,  0.0051, -0.2596])\n",
      "nex_pred =  tensor([-0.0251,  0.2072,  0.0112, -0.2563], grad_fn=<ThAddBackward>)\n",
      "loss =  9.142058115685359e-05\n",
      "nex =  tensor([ 0.0221,  0.2165, -0.0404, -0.3332])\n",
      "nex_pred =  tensor([ 0.0258,  0.2144, -0.0385, -0.3301], grad_fn=<ThAddBackward>)\n",
      "loss =  3.1783183658262715e-05\n",
      "nex =  tensor([ 0.0481, -0.1673, -0.0042,  0.3300])\n",
      "nex_pred =  tensor([ 0.0475, -0.1719, -0.0071,  0.3352], grad_fn=<ThAddBackward>)\n",
      "loss =  5.7315271988045424e-05\n",
      "nex =  tensor([ 0.0263,  0.1998, -0.0136, -0.3114])\n",
      "nex_pred =  tensor([ 0.0250,  0.1984, -0.0201, -0.3054], grad_fn=<ThAddBackward>)\n",
      "loss =  8.139523561112583e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([-0.0336, -0.1505,  0.0268,  0.2725])\n",
      "nex_pred =  tensor([-0.0299, -0.1568,  0.0292,  0.2787], grad_fn=<ThAddBackward>)\n",
      "loss =  9.85495062195696e-05\n",
      "nex =  tensor([-0.0199, -0.1668, -0.0232,  0.3161])\n",
      "nex_pred =  tensor([-0.0109, -0.1757, -0.0077,  0.3170], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004015479644294828\n",
      "nex =  tensor([ 0.0397,  0.2112,  0.0433, -0.2598])\n",
      "nex_pred =  tensor([ 0.0289,  0.2109,  0.0232, -0.2493], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0006304265698418021\n",
      "nex =  tensor([ 0.0118, -0.2166, -0.0047,  0.2916])\n",
      "nex_pred =  tensor([ 0.0117, -0.2204, -0.0072,  0.2991], grad_fn=<ThAddBackward>)\n",
      "loss =  7.644655124749988e-05\n",
      "nex =  tensor([ 0.0083,  0.2274, -0.0311, -0.2832])\n",
      "nex_pred =  tensor([ 0.0150,  0.2249, -0.0239, -0.2812], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010685599409043789\n",
      "nex =  tensor([-0.0088,  0.1515, -0.0031, -0.3434])\n",
      "nex_pred =  tensor([-0.0102,  0.1502, -0.0120, -0.3362], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00013414816930890083\n",
      "nex =  tensor([ 0.0399, -0.2031, -0.0478,  0.2627])\n",
      "nex_pred =  tensor([ 0.0417, -0.2072, -0.0441,  0.2679], grad_fn=<ThAddBackward>)\n",
      "loss =  6.085456698201597e-05\n",
      "nex =  tensor([ 0.0496,  0.2125, -0.0436, -0.3245])\n",
      "nex_pred =  tensor([ 0.0500,  0.2109, -0.0467, -0.3204], grad_fn=<ThAddBackward>)\n",
      "loss =  3.004605241585523e-05\n",
      "nex =  tensor([ 0.0347,  0.2140, -0.0129, -0.2971])\n",
      "nex_pred =  tensor([ 0.0333,  0.2128, -0.0190, -0.2913], grad_fn=<ThAddBackward>)\n",
      "loss =  7.432684651575983e-05\n",
      "nex =  tensor([ 0.0204,  0.1509,  0.0414, -0.2998])\n",
      "nex_pred =  tensor([ 0.0093,  0.1511,  0.0174, -0.2880], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0008397731580771506\n",
      "nex =  tensor([ 0.0414,  0.1935, -0.0448, -0.3067])\n",
      "nex_pred =  tensor([ 0.0429,  0.1915, -0.0462, -0.3027], grad_fn=<ThAddBackward>)\n",
      "loss =  2.417937503196299e-05\n",
      "nex =  tensor([-0.0156, -0.2394, -0.0051,  0.3214])\n",
      "nex_pred =  tensor([-0.0130, -0.2449, -0.0020,  0.3280], grad_fn=<ThAddBackward>)\n",
      "loss =  8.969702321337536e-05\n",
      "nex =  tensor([ 0.0369,  0.2444, -0.0037, -0.3267])\n",
      "nex_pred =  tensor([ 0.0338,  0.2437, -0.0120, -0.3204], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011847699352074414\n",
      "nex =  tensor([ 0.0259,  0.1526,  0.0123, -0.3235])\n",
      "nex_pred =  tensor([ 0.0179,  0.1520, -0.0067, -0.3138], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0005201163003221154\n",
      "nex =  tensor([ 0.0110, -0.1889, -0.0099,  0.2434])\n",
      "nex_pred =  tensor([ 0.0114, -0.1925, -0.0118,  0.2506], grad_fn=<ThAddBackward>)\n",
      "loss =  6.807591125834733e-05\n",
      "nex =  tensor([-0.0220, -0.2170, -0.0023,  0.3234])\n",
      "nex_pred =  tensor([-0.0179, -0.2236,  0.0035,  0.3288], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012261427764315158\n",
      "nex =  tensor([ 0.0032, -0.2246, -0.0366,  0.2820])\n",
      "nex_pred =  tensor([ 0.0071, -0.2300, -0.0296,  0.2871], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012024187890347093\n",
      "nex =  tensor([-0.0428,  0.1970, -0.0434, -0.2842])\n",
      "nex_pred =  tensor([-0.0302,  0.1919, -0.0252, -0.2849], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0005146277253516018\n",
      "nex =  tensor([-0.0133,  0.1547, -0.0016, -0.3213])\n",
      "nex_pred =  tensor([-0.0135,  0.1532, -0.0079, -0.3145], grad_fn=<ThAddBackward>)\n",
      "loss =  8.82574895513244e-05\n",
      "nex =  tensor([-0.0481, -0.1773, -0.0224,  0.2723])\n",
      "nex_pred =  tensor([-0.0393, -0.1866, -0.0062,  0.2742], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004306193732190877\n",
      "nex =  tensor([ 0.0305,  0.2367, -0.0217, -0.3388])\n",
      "nex_pred =  tensor([ 0.0307,  0.2355, -0.0253, -0.3341], grad_fn=<ThAddBackward>)\n",
      "loss =  3.726407521753572e-05\n",
      "nex =  tensor([ 0.0459,  0.1664,  0.0366, -0.2723])\n",
      "nex_pred =  tensor([ 0.0328,  0.1661,  0.0116, -0.2605], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0009346886072307825\n",
      "nex =  tensor([ 0.0264,  0.2392,  0.0256, -0.3041])\n",
      "nex_pred =  tensor([ 0.0205,  0.2389,  0.0129, -0.2960], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00026217621052637696\n",
      "nex =  tensor([ 0.0073,  0.1822, -0.0109, -0.3426])\n",
      "nex_pred =  tensor([ 0.0063,  0.1808, -0.0180, -0.3363], grad_fn=<ThAddBackward>)\n",
      "loss =  9.355038491776213e-05\n",
      "nex =  tensor([ 0.0349, -0.1884, -0.0164,  0.2616])\n",
      "nex_pred =  tensor([ 0.0340, -0.1916, -0.0198,  0.2685], grad_fn=<ThAddBackward>)\n",
      "loss =  7.051092688925564e-05\n",
      "nex =  tensor([-0.0080,  0.1499, -0.0019, -0.2603])\n",
      "nex_pred =  tensor([-0.0069,  0.1479, -0.0051, -0.2540], grad_fn=<ThAddBackward>)\n",
      "loss =  5.496999801835045e-05\n",
      "nex =  tensor([-0.0478,  0.1898, -0.0222, -0.2927])\n",
      "nex_pred =  tensor([-0.0383,  0.1858, -0.0101, -0.2912], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002551762736402452\n",
      "nex =  tensor([ 0.0024, -0.1597, -0.0374,  0.2460])\n",
      "nex_pred =  tensor([ 0.0087, -0.1659, -0.0268,  0.2488], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001985435956157744\n",
      "nex =  tensor([-0.0421, -0.1925, -0.0430,  0.2985])\n",
      "nex_pred =  tensor([-0.0314, -0.2029, -0.0215,  0.2986], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0006864200695417821\n",
      "nex =  tensor([ 0.0065, -0.1721, -0.0205,  0.2496])\n",
      "nex_pred =  tensor([ 0.0098, -0.1770, -0.0164,  0.2547], grad_fn=<ThAddBackward>)\n",
      "loss =  7.735419785603881e-05\n",
      "nex =  tensor([ 0.0336,  0.1547, -0.0245, -0.2824])\n",
      "nex_pred =  tensor([ 0.0317,  0.1530, -0.0317, -0.2761], grad_fn=<ThAddBackward>)\n",
      "loss =  9.858248813543469e-05\n",
      "nex =  tensor([ 0.0110,  0.1803, -0.0047, -0.2468])\n",
      "nex_pred =  tensor([ 0.0120,  0.1785, -0.0072, -0.2411], grad_fn=<ThAddBackward>)\n",
      "loss =  4.256146348780021e-05\n",
      "nex =  tensor([ 0.0223,  0.2404, -0.0067, -0.2568])\n",
      "nex_pred =  tensor([ 0.0248,  0.2388, -0.0060, -0.2529], grad_fn=<ThAddBackward>)\n",
      "loss =  2.4766495698713697e-05\n",
      "nex =  tensor([-0.0297,  0.2143,  0.0436, -0.3239])\n",
      "nex_pred =  tensor([-0.0318,  0.2147,  0.0346, -0.3159], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00014825219113845378\n",
      "nex =  tensor([ 0.0199,  0.1804, -0.0252, -0.3238])\n",
      "nex_pred =  tensor([ 0.0199,  0.1787, -0.0299, -0.3183], grad_fn=<ThAddBackward>)\n",
      "loss =  5.480548134073615e-05\n",
      "nex =  tensor([-0.0243, -0.2307,  0.0390,  0.3041])\n",
      "nex_pred =  tensor([-0.0262, -0.2336,  0.0308,  0.3149], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00019516004249453545\n",
      "nex =  tensor([ 0.0294,  0.2440,  0.0194, -0.3315])\n",
      "nex_pred =  tensor([ 0.0233,  0.2438,  0.0060, -0.3234], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00027981671155430377\n",
      "nex =  tensor([ 0.0104, -0.2411,  0.0388,  0.3209])\n",
      "nex_pred =  tensor([ 0.0047, -0.2422,  0.0238,  0.3325], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00039277347968891263\n",
      "nex =  tensor([ 0.0426,  0.2434,  0.0477, -0.2317])\n",
      "nex_pred =  tensor([ 0.0331,  0.2429,  0.0310, -0.2224], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004565916315186769\n",
      "nex =  tensor([ 0.0479,  0.2372,  0.0246, -0.3025])\n",
      "nex_pred =  tensor([ 0.0385,  0.2368,  0.0070, -0.2933], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004823581257369369\n",
      "nex =  tensor([-0.0216,  0.1500,  0.0198, -0.2878])\n",
      "nex_pred =  tensor([-0.0228,  0.1491,  0.0116, -0.2798], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00013280988787300885\n",
      "nex =  tensor([-0.0388, -0.2125,  0.0409,  0.3493])\n",
      "nex_pred =  tensor([-0.0370, -0.2184,  0.0401,  0.3572], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010203442798228934\n",
      "nex =  tensor([ 0.0383, -0.1932, -0.0062,  0.3064])\n",
      "nex_pred =  tensor([ 0.0370, -0.1969, -0.0105,  0.3131], grad_fn=<ThAddBackward>)\n",
      "loss =  7.932834705570713e-05\n",
      "nex =  tensor([-0.0114,  0.1897, -0.0008, -0.3314])\n",
      "nex_pred =  tensor([-0.0107,  0.1884, -0.0050, -0.3254], grad_fn=<ThAddBackward>)\n",
      "loss =  5.489980321726762e-05\n",
      "nex =  tensor([-0.0120,  0.1611,  0.0392, -0.2395])\n",
      "nex_pred =  tensor([-0.0151,  0.1605,  0.0286, -0.2306], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002032843476627022\n",
      "nex =  tensor([-0.0331, -0.2069,  0.0144,  0.2549])\n",
      "nex_pred =  tensor([-0.0317, -0.2113,  0.0134,  0.2634], grad_fn=<ThAddBackward>)\n",
      "loss =  9.508394578006119e-05\n",
      "nex =  tensor([-0.0294,  0.2161, -0.0292, -0.3313])\n",
      "nex_pred =  tensor([-0.0210,  0.2130, -0.0192, -0.3298], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00018222593644168228\n",
      "nex =  tensor([ 0.0107,  0.1811,  0.0397, -0.2773])\n",
      "nex_pred =  tensor([ 0.0035,  0.1811,  0.0230, -0.2673], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004321692686062306\n",
      "nex =  tensor([-0.0187,  0.1640,  0.0297, -0.2694])\n",
      "nex_pred =  tensor([-0.0204,  0.1633,  0.0211, -0.2612], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00014503570855595171\n",
      "nex =  tensor([-0.0418, -0.2066,  0.0488,  0.2932])\n",
      "nex_pred =  tensor([-0.0422, -0.2105,  0.0428,  0.3037], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016241629782598466\n",
      "nex =  tensor([-0.0207,  0.2049,  0.0204, -0.2804])\n",
      "nex_pred =  tensor([-0.0193,  0.2038,  0.0180, -0.2745], grad_fn=<ThAddBackward>)\n",
      "loss =  4.386131331557408e-05\n",
      "nex =  tensor([ 0.0433, -0.2229,  0.0196,  0.3363])\n",
      "nex_pred =  tensor([ 0.0373, -0.2249,  0.0061,  0.3458], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003139272448606789\n",
      "nex =  tensor([ 0.0360,  0.1462, -0.0196, -0.3064])\n",
      "nex_pred =  tensor([ 0.0318,  0.1448, -0.0314, -0.2990], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002119447017321363\n",
      "nex =  tensor([-0.0260, -0.2263,  0.0432,  0.2662])\n",
      "nex_pred =  tensor([-0.0291, -0.2280,  0.0317,  0.2784], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002931166673079133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([-0.0105,  0.1666,  0.0084, -0.2949])\n",
      "nex_pred =  tensor([-0.0110,  0.1654,  0.0021, -0.2878], grad_fn=<ThAddBackward>)\n",
      "loss =  9.230920841218904e-05\n",
      "nex =  tensor([-0.0459,  0.2278, -0.0075, -0.3178])\n",
      "nex_pred =  tensor([-0.0376,  0.2251,  0.0025, -0.3160], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00017765683878678828\n",
      "nex =  tensor([ 0.0037, -0.1670, -0.0307,  0.2819])\n",
      "nex_pred =  tensor([ 0.0101, -0.1736, -0.0203,  0.2846], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00020038429647684097\n",
      "nex =  tensor([-0.0196,  0.1516,  0.0264, -0.2893])\n",
      "nex_pred =  tensor([-0.0219,  0.1510,  0.0160, -0.2807], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00018740441009867936\n",
      "nex =  tensor([ 0.0123, -0.2288, -0.0250,  0.2876])\n",
      "nex_pred =  tensor([ 0.0139, -0.2331, -0.0231,  0.2941], grad_fn=<ThAddBackward>)\n",
      "loss =  6.655140896327794e-05\n",
      "nex =  tensor([-0.0305, -0.1738,  0.0257,  0.3494])\n",
      "nex_pred =  tensor([-0.0254, -0.1817,  0.0313,  0.3539], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00013886821398045868\n",
      "nex =  tensor([ 0.0114,  0.1849,  0.0244, -0.2820])\n",
      "nex_pred =  tensor([ 0.0065,  0.1844,  0.0117, -0.2733], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002603941538836807\n",
      "nex =  tensor([-0.0228,  0.2005, -0.0097, -0.2555])\n",
      "nex_pred =  tensor([-0.0158,  0.1976, -0.0021, -0.2527], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012170030822744593\n",
      "nex =  tensor([ 0.0281, -0.1664,  0.0407,  0.2718])\n",
      "nex_pred =  tensor([ 0.0218, -0.1682,  0.0250,  0.2825], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000405632279580459\n",
      "nex =  tensor([ 0.0106,  0.2185,  0.0344, -0.2776])\n",
      "nex_pred =  tensor([ 0.0059,  0.2182,  0.0229, -0.2693], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00022374748368747532\n",
      "nex =  tensor([-0.0371, -0.2108,  0.0253,  0.2779])\n",
      "nex_pred =  tensor([-0.0360, -0.2153,  0.0234,  0.2868], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010515356552787125\n",
      "nex =  tensor([ 0.0493,  0.1917, -0.0457, -0.2977])\n",
      "nex_pred =  tensor([ 0.0500,  0.1899, -0.0481, -0.2935], grad_fn=<ThAddBackward>)\n",
      "loss =  2.731845415837597e-05\n",
      "nex =  tensor([-0.0052,  0.1644,  0.0273, -0.3251])\n",
      "nex_pred =  tensor([-0.0104,  0.1642,  0.0121, -0.3157], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003460229781921953\n",
      "nex =  tensor([-0.0290, -0.1760,  0.0077,  0.3285])\n",
      "nex_pred =  tensor([-0.0228, -0.1840,  0.0162,  0.3321], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00018874694069381803\n",
      "nex =  tensor([ 0.0338,  0.1860, -0.0062, -0.2897])\n",
      "nex_pred =  tensor([ 0.0302,  0.1848, -0.0161, -0.2825], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001630521146580577\n",
      "nex =  tensor([ 0.0015,  0.1751, -0.0009, -0.2923])\n",
      "nex_pred =  tensor([ 0.0012,  0.1736, -0.0063, -0.2858], grad_fn=<ThAddBackward>)\n",
      "loss =  7.365654164459556e-05\n",
      "nex =  tensor([ 0.0313, -0.1899,  0.0400,  0.3028])\n",
      "nex_pred =  tensor([ 0.0245, -0.1916,  0.0238,  0.3135], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00042605807539075613\n",
      "nex =  tensor([-0.0306, -0.2183,  0.0428,  0.3282])\n",
      "nex_pred =  tensor([-0.0309, -0.2226,  0.0377,  0.3377], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00013466118252836168\n",
      "nex =  tensor([-0.0023, -0.2167,  0.0288,  0.3488])\n",
      "nex_pred =  tensor([-0.0031, -0.2210,  0.0236,  0.3570], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011255279969191179\n",
      "nex =  tensor([ 0.0109,  0.2296,  0.0367, -0.3136])\n",
      "nex_pred =  tensor([ 0.0048,  0.2297,  0.0228, -0.3048], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003048375074286014\n",
      "nex =  tensor([ 0.0497, -0.1478,  0.0396,  0.3173])\n",
      "nex_pred =  tensor([ 0.0429, -0.1507,  0.0244,  0.3262], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003639089409261942\n",
      "nex =  tensor([-0.0307,  0.2324,  0.0134, -0.2862])\n",
      "nex_pred =  tensor([-0.0257,  0.2308,  0.0175, -0.2825], grad_fn=<ThAddBackward>)\n",
      "loss =  5.728407631977461e-05\n",
      "nex =  tensor([ 0.0173, -0.2331, -0.0377,  0.2895])\n",
      "nex_pred =  tensor([ 0.0198, -0.2378, -0.0332,  0.2953], grad_fn=<ThAddBackward>)\n",
      "loss =  8.077981328824535e-05\n",
      "nex =  tensor([ 0.0502,  0.2240,  0.0456, -0.2975])\n",
      "nex_pred =  tensor([ 0.0361,  0.2239,  0.0208, -0.2860], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000947400345467031\n",
      "nex =  tensor([-0.0369,  0.1638, -0.0239, -0.3089])\n",
      "nex_pred =  tensor([-0.0304,  0.1604, -0.0175, -0.3057], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010622880654409528\n",
      "nex =  tensor([ 0.0485, -0.2153, -0.0450,  0.2423])\n",
      "nex_pred =  tensor([ 0.0476, -0.2180, -0.0464,  0.2491], grad_fn=<ThAddBackward>)\n",
      "loss =  5.6416611187160015e-05\n",
      "nex =  tensor([-0.0220, -0.2005, -0.0389,  0.2541])\n",
      "nex_pred =  tensor([-0.0154, -0.2075, -0.0264,  0.2575], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00026279999292455614\n",
      "nex =  tensor([ 0.0189, -0.1824, -0.0155,  0.2610])\n",
      "nex_pred =  tensor([ 0.0201, -0.1865, -0.0154,  0.2672], grad_fn=<ThAddBackward>)\n",
      "loss =  5.580860306508839e-05\n",
      "nex =  tensor([-0.0021, -0.2403, -0.0077,  0.2592])\n",
      "nex_pred =  tensor([-0.0026, -0.2433, -0.0112,  0.2679], grad_fn=<ThAddBackward>)\n",
      "loss =  9.7213065600954e-05\n",
      "nex =  tensor([ 0.0034, -0.2326, -0.0237,  0.2659])\n",
      "nex_pred =  tensor([ 0.0048, -0.2366, -0.0224,  0.2729], grad_fn=<ThAddBackward>)\n",
      "loss =  6.909832882229239e-05\n",
      "nex =  tensor([-0.0388, -0.2129, -0.0150,  0.3101])\n",
      "nex_pred =  tensor([-0.0321, -0.2212, -0.0030,  0.3138], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002696058072615415\n",
      "nex =  tensor([-0.0328, -0.1947,  0.0339,  0.3222])\n",
      "nex_pred =  tensor([-0.0307, -0.2006,  0.0336,  0.3297], grad_fn=<ThAddBackward>)\n",
      "loss =  9.473234240431339e-05\n",
      "nex =  tensor([ 0.0178,  0.1682,  0.0017, -0.3231])\n",
      "nex_pred =  tensor([ 0.0133,  0.1673, -0.0110, -0.3150], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002478323585819453\n",
      "nex =  tensor([-0.0224, -0.1474,  0.0272,  0.3024])\n",
      "nex_pred =  tensor([-0.0186, -0.1540,  0.0298,  0.3078], grad_fn=<ThAddBackward>)\n",
      "loss =  9.414024680154398e-05\n",
      "nex =  tensor([-0.0222,  0.2270, -0.0362, -0.2602])\n",
      "nex_pred =  tensor([-0.0100,  0.2229, -0.0193, -0.2609], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00045376396155916154\n",
      "nex =  tensor([ 0.0033,  0.1950,  0.0500, -0.2741])\n",
      "nex_pred =  tensor([-0.0037,  0.1952,  0.0338, -0.2641], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004117113712709397\n",
      "nex =  tensor([-0.0050,  0.1532, -0.0216, -0.2841])\n",
      "nex_pred =  tensor([-0.0021,  0.1507, -0.0217, -0.2792], grad_fn=<ThAddBackward>)\n",
      "loss =  3.8884736568434164e-05\n",
      "nex =  tensor([ 0.0460, -0.2097,  0.0331,  0.3282])\n",
      "nex_pred =  tensor([ 0.0380, -0.2111,  0.0157,  0.3387], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00047950766747817397\n",
      "nex =  tensor([-0.0448,  0.2111, -0.0018, -0.2657])\n",
      "nex_pred =  tensor([-0.0362,  0.2080,  0.0087, -0.2638], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00019665072613861412\n",
      "nex =  tensor([-0.0404, -0.2205,  0.0128,  0.3015])\n",
      "nex_pred =  tensor([-0.0374, -0.2266,  0.0159,  0.3087], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001062068622559309\n",
      "nex =  tensor([ 0.0227,  0.2020, -0.0206, -0.2549])\n",
      "nex_pred =  tensor([ 0.0254,  0.2000, -0.0199, -0.2508], grad_fn=<ThAddBackward>)\n",
      "loss =  2.9181035642977804e-05\n",
      "nex =  tensor([ 0.0115, -0.2367,  0.0122,  0.3031])\n",
      "nex_pred =  tensor([ 0.0088, -0.2391,  0.0038,  0.3126], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00017450109589844942\n",
      "nex =  tensor([ 0.0335,  0.2163, -0.0063, -0.3024])\n",
      "nex_pred =  tensor([ 0.0310,  0.2153, -0.0141, -0.2960], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010899072367465124\n",
      "nex =  tensor([-0.0439,  0.2420,  0.0434, -0.2330])\n",
      "nex_pred =  tensor([-0.0389,  0.2405,  0.0476, -0.2293], grad_fn=<ThAddBackward>)\n",
      "loss =  5.805787441204302e-05\n",
      "nex =  tensor([ 0.0011, -0.1820,  0.0201,  0.3111])\n",
      "nex_pred =  tensor([ 0.0016, -0.1867,  0.0173,  0.3182], grad_fn=<ThAddBackward>)\n",
      "loss =  8.088439790299162e-05\n",
      "nex =  tensor([ 0.0083,  0.2059, -0.0153, -0.3383])\n",
      "nex_pred =  tensor([ 0.0091,  0.2044, -0.0187, -0.3331], grad_fn=<ThAddBackward>)\n",
      "loss =  4.1799132304731756e-05\n",
      "nex =  tensor([-0.0161,  0.2070,  0.0456, -0.2877])\n",
      "nex_pred =  tensor([-0.0194,  0.2071,  0.0352, -0.2793], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00019152555614709854\n",
      "nex =  tensor([-0.0366,  0.1543,  0.0014, -0.2616])\n",
      "nex_pred =  tensor([-0.0323,  0.1517,  0.0037, -0.2566], grad_fn=<ThAddBackward>)\n",
      "loss =  5.669287565979175e-05\n",
      "nex =  tensor([-0.0016,  0.2330,  0.0159, -0.2603])\n",
      "nex_pred =  tensor([ 0.0001,  0.2318,  0.0149, -0.2552], grad_fn=<ThAddBackward>)\n",
      "loss =  3.031085361726582e-05\n",
      "nex =  tensor([ 0.0116, -0.2339, -0.0452,  0.2522])\n",
      "nex_pred =  tensor([ 0.0142, -0.2383, -0.0403,  0.2581], grad_fn=<ThAddBackward>)\n",
      "loss =  8.511551277479157e-05\n",
      "nex =  tensor([ 0.0070,  0.2071,  0.0096, -0.2658])\n",
      "nex_pred =  tensor([ 0.0069,  0.2059,  0.0053, -0.2597], grad_fn=<ThAddBackward>)\n",
      "loss =  5.690191392204724e-05\n",
      "nex =  tensor([-0.0215, -0.2262, -0.0303,  0.2715])\n",
      "nex_pred =  tensor([-0.0166, -0.2326, -0.0214,  0.2765], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016814413538668305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([-0.0178, -0.1472, -0.0016,  0.3252])\n",
      "nex_pred =  tensor([-0.0099, -0.1557,  0.0098,  0.3269], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00026766443625092506\n",
      "nex =  tensor([ 0.0261,  0.1678, -0.0315, -0.3372])\n",
      "nex_pred =  tensor([ 0.0252,  0.1660, -0.0381, -0.3316], grad_fn=<ThAddBackward>)\n",
      "loss =  7.930467108963057e-05\n",
      "nex =  tensor([ 0.0199, -0.1577, -0.0436,  0.2757])\n",
      "nex_pred =  tensor([ 0.0265, -0.1641, -0.0322,  0.2777], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000216559914406389\n",
      "nex =  tensor([-0.0371, -0.1586,  0.0318,  0.3453])\n",
      "nex_pred =  tensor([-0.0314, -0.1670,  0.0382,  0.3495], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016137174679897726\n",
      "nex =  tensor([-0.0115, -0.2259, -0.0199,  0.2709])\n",
      "nex_pred =  tensor([-0.0087, -0.2309, -0.0158,  0.2772], grad_fn=<ThAddBackward>)\n",
      "loss =  9.020443394547328e-05\n",
      "nex =  tensor([-0.0240, -0.2039, -0.0409,  0.2719])\n",
      "nex_pred =  tensor([-0.0166, -0.2117, -0.0264,  0.2747], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003338708193041384\n",
      "nex =  tensor([ 0.0244, -0.1545,  0.0059,  0.2924])\n",
      "nex_pred =  tensor([ 0.0248, -0.1590,  0.0035,  0.2983], grad_fn=<ThAddBackward>)\n",
      "loss =  6.11663781455718e-05\n",
      "nex =  tensor([-0.0147, -0.2131,  0.0408,  0.2598])\n",
      "nex_pred =  tensor([-0.0183, -0.2146,  0.0286,  0.2717], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003055581182707101\n",
      "nex =  tensor([-0.0285,  0.1979, -0.0225, -0.3059])\n",
      "nex_pred =  tensor([-0.0212,  0.1949, -0.0144, -0.3034], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00013427915109787136\n",
      "nex =  tensor([-0.0159,  0.2292, -0.0458, -0.3413])\n",
      "nex_pred =  tensor([-0.0062,  0.2258, -0.0335, -0.3409], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002546490868553519\n",
      "nex =  tensor([-0.0010,  0.2337, -0.0449, -0.2825])\n",
      "nex_pred =  tensor([ 0.0095,  0.2303, -0.0312, -0.2826], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003102259652223438\n",
      "nex =  tensor([ 0.0183,  0.1546,  0.0101, -0.3130])\n",
      "nex_pred =  tensor([ 0.0122,  0.1539, -0.0056, -0.3039], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00036660200566984713\n",
      "nex =  tensor([ 0.0043,  0.1665, -0.0039, -0.2637])\n",
      "nex_pred =  tensor([ 0.0047,  0.1648, -0.0078, -0.2575], grad_fn=<ThAddBackward>)\n",
      "loss =  5.6657598179299384e-05\n",
      "nex =  tensor([-0.0485,  0.1468,  0.0435, -0.3213])\n",
      "nex_pred =  tensor([-0.0506,  0.1471,  0.0317, -0.3119], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00023399078054353595\n",
      "nex =  tensor([-0.0215, -0.2005, -0.0477,  0.2674])\n",
      "nex_pred =  tensor([-0.0135, -0.2084, -0.0317,  0.2697], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00038756587309762836\n",
      "nex =  tensor([-0.0284, -0.2162, -0.0010,  0.3191])\n",
      "nex_pred =  tensor([-0.0240, -0.2230,  0.0054,  0.3245], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001360782771371305\n",
      "nex =  tensor([-0.0076,  0.2269, -0.0210, -0.2788])\n",
      "nex_pred =  tensor([-0.0003,  0.2243, -0.0128, -0.2768], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00013256407692097127\n",
      "nex =  tensor([-0.0330, -0.2035, -0.0103,  0.2518])\n",
      "nex_pred =  tensor([-0.0288, -0.2095, -0.0043,  0.2577], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012450492067728192\n",
      "nex =  tensor([-0.0489, -0.1940, -0.0186,  0.2518])\n",
      "nex_pred =  tensor([-0.0422, -0.2020, -0.0065,  0.2558], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002717319002840668\n",
      "nex =  tensor([ 0.0413,  0.1967, -0.0472, -0.3248])\n",
      "nex_pred =  tensor([ 0.0427,  0.1947, -0.0490, -0.3208], grad_fn=<ThAddBackward>)\n",
      "loss =  2.454908644722309e-05\n",
      "nex =  tensor([-0.0336,  0.2128, -0.0027, -0.2851])\n",
      "nex_pred =  tensor([-0.0269,  0.2104,  0.0043, -0.2822], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010896314051933587\n",
      "nex =  tensor([ 0.0081,  0.2162,  0.0505, -0.2413])\n",
      "nex_pred =  tensor([ 0.0025,  0.2160,  0.0377, -0.2325], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00027355519705452025\n",
      "nex =  tensor([ 0.0451,  0.1730,  0.0440, -0.3244])\n",
      "nex_pred =  tensor([ 0.0293,  0.1730,  0.0139, -0.3115], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0013211145997047424\n",
      "nex =  tensor([-0.0307,  0.2314, -0.0068, -0.3154])\n",
      "nex_pred =  tensor([-0.0240,  0.2292,  0.0003, -0.3128], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001066858894773759\n",
      "nex =  tensor([ 0.0147, -0.1717,  0.0050,  0.3176])\n",
      "nex_pred =  tensor([ 0.0164, -0.1770,  0.0051,  0.3230], grad_fn=<ThAddBackward>)\n",
      "loss =  5.9755802794825286e-05\n",
      "nex =  tensor([-0.0354,  0.2039,  0.0357, -0.3159])\n",
      "nex_pred =  tensor([-0.0357,  0.2037,  0.0296, -0.3087], grad_fn=<ThAddBackward>)\n",
      "loss =  8.919982792576775e-05\n",
      "nex =  tensor([-0.0012, -0.1497, -0.0445,  0.3219])\n",
      "nex_pred =  tensor([ 0.0101, -0.1590, -0.0245,  0.3205], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000615132856182754\n",
      "nex =  tensor([ 0.0119,  0.2208, -0.0267, -0.2909])\n",
      "nex_pred =  tensor([ 0.0167,  0.2186, -0.0227, -0.2878], grad_fn=<ThAddBackward>)\n",
      "loss =  5.2470706577878445e-05\n",
      "nex =  tensor([ 0.0112,  0.1968, -0.0187, -0.2933])\n",
      "nex_pred =  tensor([ 0.0134,  0.1949, -0.0194, -0.2887], grad_fn=<ThAddBackward>)\n",
      "loss =  3.028795799764339e-05\n",
      "nex =  tensor([-0.0343,  0.1825,  0.0222, -0.3103])\n",
      "nex_pred =  tensor([-0.0335,  0.1816,  0.0176, -0.3035], grad_fn=<ThAddBackward>)\n",
      "loss =  6.798575486754999e-05\n",
      "nex =  tensor([ 0.0279,  0.1503, -0.0006, -0.3017])\n",
      "nex_pred =  tensor([ 0.0223,  0.1493, -0.0151, -0.2931], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003147904062643647\n",
      "nex =  tensor([-0.0017,  0.2391,  0.0347, -0.2747])\n",
      "nex_pred =  tensor([-0.0034,  0.2387,  0.0282, -0.2680], grad_fn=<ThAddBackward>)\n",
      "loss =  9.047233470482752e-05\n",
      "nex =  tensor([ 0.0261, -0.2268,  0.0315,  0.2762])\n",
      "nex_pred =  tensor([ 0.0186, -0.2270,  0.0135,  0.2883], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000525161623954773\n",
      "nex =  tensor([ 0.0106, -0.1625,  0.0459,  0.2753])\n",
      "nex_pred =  tensor([ 0.0062, -0.1650,  0.0330,  0.2857], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00029934835038147867\n",
      "nex =  tensor([ 0.0339, -0.2062,  0.0019,  0.3109])\n",
      "nex_pred =  tensor([ 0.0316, -0.2094, -0.0047,  0.3187], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011933539644815028\n",
      "nex =  tensor([ 0.0016, -0.2266, -0.0414,  0.3212])\n",
      "nex_pred =  tensor([ 0.0076, -0.2335, -0.0299,  0.3246], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002286285161972046\n",
      "nex =  tensor([-0.0470, -0.2291, -0.0220,  0.3006])\n",
      "nex_pred =  tensor([-0.0403, -0.2376, -0.0088,  0.3045], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000306846690364182\n",
      "nex =  tensor([ 0.0243, -0.1757, -0.0374,  0.2527])\n",
      "nex_pred =  tensor([ 0.0277, -0.1805, -0.0320,  0.2571], grad_fn=<ThAddBackward>)\n",
      "loss =  8.205710764741525e-05\n",
      "nex =  tensor([-0.0322,  0.2121, -0.0421, -0.3070])\n",
      "nex_pred =  tensor([-0.0209,  0.2078, -0.0266, -0.3071], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003852798545267433\n",
      "nex =  tensor([-0.0328,  0.1724, -0.0434, -0.3346])\n",
      "nex_pred =  tensor([-0.0245,  0.1684, -0.0337, -0.3329], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00018260515935253352\n",
      "nex =  tensor([-0.0059,  0.1867,  0.0150, -0.2651])\n",
      "nex_pred =  tensor([-0.0061,  0.1854,  0.0102, -0.2585], grad_fn=<ThAddBackward>)\n",
      "loss =  6.917180871823803e-05\n",
      "nex =  tensor([ 0.0292, -0.2003,  0.0255,  0.2631])\n",
      "nex_pred =  tensor([ 0.0229, -0.2013,  0.0101,  0.2740], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00039668637327849865\n",
      "nex =  tensor([-0.0051,  0.1489,  0.0087, -0.2778])\n",
      "nex_pred =  tensor([-0.0066,  0.1477,  0.0006, -0.2701], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012829065963160247\n",
      "nex =  tensor([ 0.0068,  0.2355,  0.0095, -0.2431])\n",
      "nex_pred =  tensor([ 0.0092,  0.2339,  0.0099, -0.2386], grad_fn=<ThAddBackward>)\n",
      "loss =  2.7624670110526495e-05\n",
      "nex =  tensor([ 0.0222,  0.2013,  0.0057, -0.2809])\n",
      "nex_pred =  tensor([ 0.0195,  0.2003, -0.0027, -0.2739], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012717704521492124\n",
      "nex =  tensor([ 0.0474,  0.2367, -0.0044, -0.2976])\n",
      "nex_pred =  tensor([ 0.0435,  0.2358, -0.0135, -0.2911], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001420955959474668\n",
      "nex =  tensor([-0.0022,  0.1478,  0.0342, -0.2460])\n",
      "nex_pred =  tensor([-0.0068,  0.1473,  0.0209, -0.2364], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002894153003580868\n",
      "nex =  tensor([-0.0098, -0.2032,  0.0446,  0.2711])\n",
      "nex_pred =  tensor([-0.0136, -0.2050,  0.0321,  0.2827], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003089306701440364\n",
      "nex =  tensor([-0.0180,  0.1840, -0.0285, -0.3144])\n",
      "nex_pred =  tensor([-0.0121,  0.1811, -0.0231, -0.3114], grad_fn=<ThAddBackward>)\n",
      "loss =  8.25034876470454e-05\n",
      "nex =  tensor([-0.0314, -0.1737,  0.0123,  0.3158])\n",
      "nex_pred =  tensor([-0.0259, -0.1813,  0.0193,  0.3201], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00015594044816680253\n",
      "nex =  tensor([ 0.0201,  0.1579, -0.0059, -0.2629])\n",
      "nex_pred =  tensor([ 0.0181,  0.1564, -0.0135, -0.2559], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011303642531856894\n",
      "nex =  tensor([ 0.0190, -0.1764, -0.0055,  0.2766])\n",
      "nex_pred =  tensor([ 0.0197, -0.1806, -0.0066,  0.2829], grad_fn=<ThAddBackward>)\n",
      "loss =  5.886655708309263e-05\n",
      "nex =  tensor([-0.0388, -0.2037,  0.0006,  0.2436])\n",
      "nex_pred =  tensor([-0.0356, -0.2092,  0.0040,  0.2507], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001019166229525581\n",
      "nex =  tensor([-0.0118,  0.1602,  0.0382, -0.2803])\n",
      "nex_pred =  tensor([-0.0163,  0.1601,  0.0246, -0.2708], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00029760817415080965\n",
      "nex =  tensor([ 0.0342,  0.1641,  0.0495, -0.2735])\n",
      "nex_pred =  tensor([ 0.0208,  0.1642,  0.0234, -0.2611], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0010168981971219182\n",
      "nex =  tensor([-0.0312,  0.1679,  0.0354, -0.2482])\n",
      "nex_pred =  tensor([-0.0311,  0.1669,  0.0300, -0.2407], grad_fn=<ThAddBackward>)\n",
      "loss =  8.673709817230701e-05\n",
      "nex =  tensor([-0.0098, -0.2023,  0.0015,  0.3295])\n",
      "nex_pred =  tensor([-0.0063, -0.2085,  0.0056,  0.3348], grad_fn=<ThAddBackward>)\n",
      "loss =  9.663186938269064e-05\n",
      "nex =  tensor([-0.0468,  0.1922, -0.0193, -0.3303])\n",
      "nex_pred =  tensor([-0.0392,  0.1890, -0.0108, -0.3279], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00014747012755833566\n",
      "nex =  tensor([-0.0362, -0.1966, -0.0488,  0.3033])\n",
      "nex_pred =  tensor([-0.0253, -0.2070, -0.0267,  0.3032], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0007129498408176005\n",
      "nex =  tensor([-0.0278, -0.1512, -0.0034,  0.3237])\n",
      "nex_pred =  tensor([-0.0191, -0.1603,  0.0100,  0.3249], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000341534789185971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([ 0.0479,  0.1731, -0.0215, -0.2731])\n",
      "nex_pred =  tensor([ 0.0446,  0.1717, -0.0303, -0.2666], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001335037813987583\n",
      "nex =  tensor([-0.0440, -0.1658, -0.0109,  0.2564])\n",
      "nex_pred =  tensor([-0.0366, -0.1739,  0.0012,  0.2596], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002775111934170127\n",
      "nex =  tensor([-0.0314,  0.1913,  0.0078, -0.2998])\n",
      "nex_pred =  tensor([-0.0282,  0.1897,  0.0082, -0.2947], grad_fn=<ThAddBackward>)\n",
      "loss =  3.976367588620633e-05\n",
      "nex =  tensor([ 0.0248, -0.1839, -0.0468,  0.2733])\n",
      "nex_pred =  tensor([ 0.0297, -0.1895, -0.0380,  0.2766], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00014394069148693234\n",
      "nex =  tensor([-0.0332, -0.1838,  0.0238,  0.2811])\n",
      "nex_pred =  tensor([-0.0307, -0.1893,  0.0244,  0.2885], grad_fn=<ThAddBackward>)\n",
      "loss =  9.210109419655055e-05\n",
      "nex =  tensor([-0.0156, -0.1818,  0.0333,  0.3338])\n",
      "nex_pred =  tensor([-0.0141, -0.1874,  0.0318,  0.3408], grad_fn=<ThAddBackward>)\n",
      "loss =  8.458959200652316e-05\n",
      "nex =  tensor([-0.0300,  0.2256, -0.0018, -0.2572])\n",
      "nex_pred =  tensor([-0.0220,  0.2228,  0.0076, -0.2552], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016592457541264594\n",
      "nex =  tensor([ 0.0245, -0.1517,  0.0462,  0.3121])\n",
      "nex_pred =  tensor([ 0.0201, -0.1550,  0.0339,  0.3211], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00026360369520261884\n",
      "nex =  tensor([ 0.0480,  0.2408,  0.0417, -0.2625])\n",
      "nex_pred =  tensor([ 0.0371,  0.2404,  0.0228, -0.2527], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0005732200806960464\n",
      "nex =  tensor([-0.0042, -0.1836,  0.0158,  0.3295])\n",
      "nex_pred =  tensor([-0.0020, -0.1893,  0.0164,  0.3354], grad_fn=<ThAddBackward>)\n",
      "loss =  7.294825627468526e-05\n",
      "nex =  tensor([ 0.0488,  0.1717, -0.0243, -0.3115])\n",
      "nex_pred =  tensor([ 0.0444,  0.1704, -0.0355, -0.3046], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00019485646043904126\n",
      "nex =  tensor([-0.0208, -0.1956, -0.0094,  0.2916])\n",
      "nex_pred =  tensor([-0.0160, -0.2022, -0.0022,  0.2963], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00014153958181850612\n",
      "nex =  tensor([ 0.0209, -0.2417, -0.0050,  0.2766])\n",
      "nex_pred =  tensor([ 0.0181, -0.2437, -0.0126,  0.2859], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001564667618367821\n",
      "nex =  tensor([ 0.0193, -0.1742,  0.0463,  0.2579])\n",
      "nex_pred =  tensor([ 0.0126, -0.1754,  0.0291,  0.2697], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00048147200141102076\n",
      "nex =  tensor([ 0.0128, -0.2085, -0.0333,  0.3089])\n",
      "nex_pred =  tensor([ 0.0173, -0.2145, -0.0257,  0.3130], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00013092606968712062\n",
      "nex =  tensor([-0.0398, -0.2216,  0.0335,  0.2573])\n",
      "nex_pred =  tensor([-0.0405, -0.2247,  0.0272,  0.2682], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001677279215073213\n",
      "nex =  tensor([-0.0370, -0.1938, -0.0366,  0.2462])\n",
      "nex_pred =  tensor([-0.0294, -0.2018, -0.0220,  0.2491], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00034284949651919305\n",
      "nex =  tensor([ 0.0165,  0.2264,  0.0114, -0.2925])\n",
      "nex_pred =  tensor([ 0.0145,  0.2256,  0.0045, -0.2860], grad_fn=<ThAddBackward>)\n",
      "loss =  9.641081851441413e-05\n",
      "nex =  tensor([-0.0084, -0.1677,  0.0397,  0.3300])\n",
      "nex_pred =  tensor([-0.0078, -0.1730,  0.0360,  0.3373], grad_fn=<ThAddBackward>)\n",
      "loss =  9.489644435234368e-05\n",
      "nex =  tensor([ 0.0056,  0.2303, -0.0216, -0.3188])\n",
      "nex_pred =  tensor([ 0.0098,  0.2285, -0.0188, -0.3155], grad_fn=<ThAddBackward>)\n",
      "loss =  4.00788921979256e-05\n",
      "nex =  tensor([ 0.0115,  0.2391,  0.0142, -0.2543])\n",
      "nex_pred =  tensor([ 0.0120,  0.2379,  0.0118, -0.2490], grad_fn=<ThAddBackward>)\n",
      "loss =  3.487039793981239e-05\n",
      "nex =  tensor([-0.0419, -0.1482, -0.0375,  0.2972])\n",
      "nex_pred =  tensor([-0.0290, -0.1595, -0.0136,  0.2952], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0008700643084011972\n",
      "nex =  tensor([-0.0339, -0.1521,  0.0212,  0.2901])\n",
      "nex_pred =  tensor([-0.0289, -0.1592,  0.0263,  0.2951], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012765370775014162\n",
      "nex =  tensor([ 0.0398, -0.1833, -0.0250,  0.2801])\n",
      "nex_pred =  tensor([ 0.0404, -0.1873, -0.0250,  0.2857], grad_fn=<ThAddBackward>)\n",
      "loss =  4.763383913086727e-05\n",
      "nex =  tensor([-0.0316, -0.2081, -0.0276,  0.2652])\n",
      "nex_pred =  tensor([-0.0255, -0.2153, -0.0164,  0.2693], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00023216464614961296\n",
      "nex =  tensor([-0.0176,  0.2156, -0.0259, -0.2880])\n",
      "nex_pred =  tensor([-0.0094,  0.2125, -0.0161, -0.2863], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001781431637937203\n",
      "nex =  tensor([-0.0205,  0.1970,  0.0147, -0.2721])\n",
      "nex_pred =  tensor([-0.0183,  0.1955,  0.0136, -0.2665], grad_fn=<ThAddBackward>)\n",
      "loss =  3.907093923771754e-05\n",
      "nex =  tensor([-0.0053, -0.2419,  0.0391,  0.2839])\n",
      "nex_pred =  tensor([-0.0104, -0.2426,  0.0245,  0.2963], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003929806989617646\n",
      "nex =  tensor([ 0.0110, -0.2404,  0.0413,  0.2974])\n",
      "nex_pred =  tensor([ 0.0042, -0.2406,  0.0239,  0.3099], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0005080912378616631\n",
      "nex =  tensor([-0.0064, -0.1586, -0.0309,  0.2392])\n",
      "nex_pred =  tensor([-0.0002, -0.1649, -0.0209,  0.2424], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00018780003301799297\n",
      "nex =  tensor([-0.0307,  0.1485,  0.0365, -0.3026])\n",
      "nex_pred =  tensor([-0.0336,  0.1483,  0.0244, -0.2933], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00023927009897306561\n",
      "nex =  tensor([-0.0036,  0.1855,  0.0395, -0.2316])\n",
      "nex_pred =  tensor([-0.0066,  0.1849,  0.0300, -0.2233], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016824777412693948\n",
      "nex =  tensor([-0.0447, -0.1502,  0.0466,  0.2759])\n",
      "nex_pred =  tensor([-0.0420, -0.1561,  0.0460,  0.2839], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010604972339933738\n",
      "nex =  tensor([-0.0277, -0.2429, -0.0252,  0.2612])\n",
      "nex_pred =  tensor([-0.0242, -0.2485, -0.0189,  0.2675], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001236407842952758\n",
      "nex =  tensor([-0.0271, -0.2342, -0.0426,  0.2728])\n",
      "nex_pred =  tensor([-0.0209, -0.2415, -0.0298,  0.2768], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002728782419580966\n",
      "nex =  tensor([ 0.0509,  0.2396,  0.0266, -0.3008])\n",
      "nex_pred =  tensor([ 0.0407,  0.2392,  0.0081, -0.2913], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0005331958527676761\n",
      "nex =  tensor([ 0.0057,  0.2022,  0.0377, -0.2977])\n",
      "nex_pred =  tensor([-0.0003,  0.2023,  0.0233, -0.2884], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003310783358756453\n",
      "nex =  tensor([ 0.0350, -0.2274, -0.0421,  0.3225])\n",
      "nex_pred =  tensor([ 0.0378, -0.2324, -0.0369,  0.3274], grad_fn=<ThAddBackward>)\n",
      "loss =  8.395708573516458e-05\n",
      "nex =  tensor([-0.0384, -0.1914,  0.0101,  0.3408])\n",
      "nex_pred =  tensor([-0.0321, -0.1998,  0.0193,  0.3446], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00020985599257983267\n",
      "nex =  tensor([ 0.0389, -0.1879,  0.0194,  0.2607])\n",
      "nex_pred =  tensor([ 0.0326, -0.1893,  0.0047,  0.2709], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003603917721193284\n",
      "nex =  tensor([-0.0303, -0.1995,  0.0467,  0.2682])\n",
      "nex_pred =  tensor([-0.0319, -0.2024,  0.0379,  0.2794], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00021517837012652308\n",
      "nex =  tensor([ 0.0286,  0.1842,  0.0175, -0.3287])\n",
      "nex_pred =  tensor([ 0.0204,  0.1838, -0.0008, -0.3191], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004934428143315017\n",
      "nex =  tensor([ 0.0045, -0.2030,  0.0307,  0.2953])\n",
      "nex_pred =  tensor([ 0.0016, -0.2056,  0.0210,  0.3051], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00020562611462082714\n",
      "nex =  tensor([ 0.0361,  0.2033,  0.0024, -0.3394])\n",
      "nex_pred =  tensor([ 0.0297,  0.2026, -0.0124, -0.3312], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000328356894897297\n",
      "nex =  tensor([ 0.0046, -0.1476, -0.0089,  0.3311])\n",
      "nex_pred =  tensor([ 0.0112, -0.1552,  0.0005,  0.3329], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00019350518414285034\n",
      "nex =  tensor([-0.0260, -0.1671, -0.0155,  0.2817])\n",
      "nex_pred =  tensor([-0.0186, -0.1749, -0.0037,  0.2844], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000261874170973897\n",
      "nex =  tensor([ 0.0058,  0.1560, -0.0214, -0.3002])\n",
      "nex_pred =  tensor([ 0.0068,  0.1539, -0.0247, -0.2947], grad_fn=<ThAddBackward>)\n",
      "loss =  4.724416066892445e-05\n",
      "nex =  tensor([ 0.0284, -0.1765,  0.0390,  0.2975])\n",
      "nex_pred =  tensor([ 0.0226, -0.1787,  0.0246,  0.3076], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003483674372546375\n",
      "nex =  tensor([-0.0106,  0.2269,  0.0405, -0.3237])\n",
      "nex_pred =  tensor([-0.0145,  0.2272,  0.0295, -0.3155], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00020429040887393057\n",
      "nex =  tensor([ 0.0092, -0.1646, -0.0258,  0.2721])\n",
      "nex_pred =  tensor([ 0.0141, -0.1705, -0.0184,  0.2756], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012633806909434497\n",
      "nex =  tensor([ 0.0377, -0.2010,  0.0006,  0.3077])\n",
      "nex_pred =  tensor([ 0.0351, -0.2042, -0.0062,  0.3154], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012025418254779652\n",
      "nex =  tensor([-0.0327, -0.2259,  0.0494,  0.3007])\n",
      "nex_pred =  tensor([-0.0348, -0.2286,  0.0401,  0.3122], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00023016324848867953\n",
      "nex =  tensor([-0.0119, -0.1655,  0.0292,  0.2533])\n",
      "nex_pred =  tensor([-0.0122, -0.1692,  0.0237,  0.2622], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012183057697257027\n",
      "nex =  tensor([ 0.0495,  0.2021, -0.0101, -0.3190])\n",
      "nex_pred =  tensor([ 0.0438,  0.2012, -0.0231, -0.3115], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002592152450233698\n",
      "nex =  tensor([ 0.0104, -0.2434, -0.0407,  0.2749])\n",
      "nex_pred =  tensor([ 0.0128, -0.2479, -0.0360,  0.2810], grad_fn=<ThAddBackward>)\n",
      "loss =  8.560679270885885e-05\n",
      "nex =  tensor([ 0.0202,  0.2126, -0.0082, -0.2706])\n",
      "nex_pred =  tensor([ 0.0211,  0.2110, -0.0104, -0.2655], grad_fn=<ThAddBackward>)\n",
      "loss =  3.403399387025274e-05\n",
      "nex =  tensor([ 0.0377,  0.2272, -0.0268, -0.2555])\n",
      "nex_pred =  tensor([ 0.0406,  0.2253, -0.0253, -0.2521], grad_fn=<ThAddBackward>)\n",
      "loss =  2.6003479433711618e-05\n",
      "nex =  tensor([ 0.0145,  0.2243, -0.0369, -0.3526])\n",
      "nex_pred =  tensor([ 0.0183,  0.2223, -0.0350, -0.3494], grad_fn=<ThAddBackward>)\n",
      "loss =  3.2469379220856354e-05\n",
      "nex =  tensor([ 0.0478, -0.1865, -0.0374,  0.2351])\n",
      "nex_pred =  tensor([ 0.0471, -0.1895, -0.0389,  0.2414], grad_fn=<ThAddBackward>)\n",
      "loss =  5.1211136451456696e-05\n",
      "nex =  tensor([-0.0033, -0.1967,  0.0144,  0.3054])\n",
      "nex_pred =  tensor([-0.0026, -0.2012,  0.0123,  0.3127], grad_fn=<ThAddBackward>)\n",
      "loss =  7.950544386403635e-05\n",
      "nex =  tensor([-0.0117, -0.2024,  0.0093,  0.3061])\n",
      "nex_pred =  tensor([-0.0098, -0.2076,  0.0099,  0.3129], grad_fn=<ThAddBackward>)\n",
      "loss =  7.678812107769772e-05\n",
      "nex =  tensor([-0.0051, -0.1749,  0.0318,  0.3466])\n",
      "nex_pred =  tensor([-0.0037, -0.1807,  0.0301,  0.3530], grad_fn=<ThAddBackward>)\n",
      "loss =  7.780583837302402e-05\n",
      "nex =  tensor([ 0.0412,  0.2019,  0.0213, -0.3042])\n",
      "nex_pred =  tensor([ 0.0319,  0.2015,  0.0026, -0.2945], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0005294834263622761\n",
      "nex =  tensor([-0.0445, -0.1917, -0.0404,  0.2577])\n",
      "nex_pred =  tensor([-0.0355, -0.2009, -0.0222,  0.2594], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004958087229169905\n",
      "nex =  tensor([-0.0209,  0.1823,  0.0419, -0.2595])\n",
      "nex_pred =  tensor([-0.0229,  0.1819,  0.0331, -0.2513], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00014955019287299365\n",
      "nex =  tensor([ 0.0102,  0.2067, -0.0259, -0.3036])\n",
      "nex_pred =  tensor([ 0.0139,  0.2046, -0.0242, -0.2999], grad_fn=<ThAddBackward>)\n",
      "loss =  3.4462438634363934e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([-0.0196, -0.2162, -0.0118,  0.3142])\n",
      "nex_pred =  tensor([-0.0149, -0.2230, -0.0044,  0.3191], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00014632650709245354\n",
      "nex =  tensor([ 0.0247, -0.2191, -0.0043,  0.3293])\n",
      "nex_pred =  tensor([ 0.0243, -0.2232, -0.0070,  0.3363], grad_fn=<ThAddBackward>)\n",
      "loss =  7.214889046736062e-05\n",
      "nex =  tensor([-0.0181, -0.2063,  0.0207,  0.3211])\n",
      "nex_pred =  tensor([-0.0165, -0.2115,  0.0200,  0.3284], grad_fn=<ThAddBackward>)\n",
      "loss =  8.367848204215989e-05\n",
      "nex =  tensor([ 0.0138,  0.2288, -0.0481, -0.3007])\n",
      "nex_pred =  tensor([ 0.0219,  0.2259, -0.0386, -0.2997], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001642745774006471\n",
      "nex =  tensor([-0.0469,  0.2397,  0.0427, -0.2937])\n",
      "nex_pred =  tensor([-0.0442,  0.2393,  0.0428, -0.2885], grad_fn=<ThAddBackward>)\n",
      "loss =  3.4453805710654706e-05\n",
      "nex =  tensor([ 0.0086, -0.2339, -0.0401,  0.2788])\n",
      "nex_pred =  tensor([ 0.0118, -0.2389, -0.0341,  0.2844], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010117968486156315\n",
      "nex =  tensor([-0.0179, -0.2076, -0.0380,  0.3097])\n",
      "nex_pred =  tensor([-0.0100, -0.2159, -0.0229,  0.3120], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003625362878665328\n",
      "nex =  tensor([ 0.0375,  0.1679, -0.0399, -0.2893])\n",
      "nex_pred =  tensor([ 0.0381,  0.1659, -0.0430, -0.2845], grad_fn=<ThAddBackward>)\n",
      "loss =  3.6822661058977246e-05\n",
      "nex =  tensor([ 0.0025, -0.1467,  0.0482,  0.3545])\n",
      "nex_pred =  tensor([ 0.0026, -0.1524,  0.0436,  0.3611], grad_fn=<ThAddBackward>)\n",
      "loss =  9.619581396691501e-05\n",
      "nex =  tensor([-0.0131, -0.2306,  0.0066,  0.3174])\n",
      "nex_pred =  tensor([-0.0118, -0.2354,  0.0064,  0.3249], grad_fn=<ThAddBackward>)\n",
      "loss =  8.038521627895534e-05\n",
      "nex =  tensor([-0.0299,  0.1911, -0.0375, -0.3379])\n",
      "nex_pred =  tensor([-0.0219,  0.1875, -0.0283, -0.3362], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016570105799473822\n",
      "nex =  tensor([-0.0449,  0.1988,  0.0373, -0.2857])\n",
      "nex_pred =  tensor([-0.0433,  0.1981,  0.0346, -0.2792], grad_fn=<ThAddBackward>)\n",
      "loss =  5.1670042012119666e-05\n",
      "nex =  tensor([ 0.0266, -0.1837,  0.0439,  0.3121])\n",
      "nex_pred =  tensor([ 0.0206, -0.1859,  0.0288,  0.3224], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00037624832475557923\n",
      "nex =  tensor([ 0.0102, -0.1998,  0.0353,  0.2884])\n",
      "nex_pred =  tensor([ 0.0059, -0.2019,  0.0229,  0.2989], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002869805321097374\n",
      "nex =  tensor([ 0.0269, -0.1708,  0.0232,  0.2948])\n",
      "nex_pred =  tensor([ 0.0238, -0.1739,  0.0140,  0.3031], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00017506262520328164\n",
      "nex =  tensor([ 0.0374,  0.1938, -0.0210, -0.3198])\n",
      "nex_pred =  tensor([ 0.0350,  0.1925, -0.0291, -0.3136], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011127559264423326\n",
      "nex =  tensor([-0.0268,  0.2205, -0.0289, -0.3152])\n",
      "nex_pred =  tensor([-0.0179,  0.2172, -0.0179, -0.3140], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00021447405742947012\n",
      "nex =  tensor([-0.0022, -0.2262, -0.0259,  0.2813])\n",
      "nex_pred =  tensor([ 0.0008, -0.2313, -0.0212,  0.2872], grad_fn=<ThAddBackward>)\n",
      "loss =  9.266583219869062e-05\n",
      "nex =  tensor([ 0.0172, -0.1699, -0.0188,  0.2616])\n",
      "nex_pred =  tensor([ 0.0197, -0.1746, -0.0163,  0.2668], grad_fn=<ThAddBackward>)\n",
      "loss =  6.104574276832864e-05\n",
      "nex =  tensor([-0.0297, -0.2061, -0.0090,  0.3297])\n",
      "nex_pred =  tensor([-0.0233, -0.2142,  0.0017,  0.3333], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00023341695487033576\n",
      "nex =  tensor([ 0.0182, -0.1793,  0.0462,  0.2633])\n",
      "nex_pred =  tensor([ 0.0116, -0.1805,  0.0291,  0.2751], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00047482969239354134\n",
      "nex =  tensor([-0.0248, -0.1716,  0.0430,  0.3387])\n",
      "nex_pred =  tensor([-0.0228, -0.1778,  0.0420,  0.3456], grad_fn=<ThAddBackward>)\n",
      "loss =  9.0983317932114e-05\n",
      "nex =  tensor([ 0.0299, -0.1730, -0.0386,  0.2609])\n",
      "nex_pred =  tensor([ 0.0333, -0.1778, -0.0332,  0.2650], grad_fn=<ThAddBackward>)\n",
      "loss =  8.158919808920473e-05\n",
      "nex =  tensor([-0.0008, -0.1776, -0.0160,  0.3347])\n",
      "nex_pred =  tensor([ 0.0057, -0.1852, -0.0060,  0.3371], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00020533337374217808\n",
      "nex =  tensor([-0.0458,  0.1457,  0.0257, -0.3156])\n",
      "nex_pred =  tensor([-0.0459,  0.1450,  0.0184, -0.3077], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011680305033223704\n",
      "nex =  tensor([-0.0366,  0.2226,  0.0022, -0.2618])\n",
      "nex_pred =  tensor([-0.0288,  0.2199,  0.0114, -0.2596], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00015814814832992852\n",
      "nex =  tensor([ 0.0288, -0.1554,  0.0459,  0.3107])\n",
      "nex_pred =  tensor([ 0.0236, -0.1584,  0.0323,  0.3201], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00030968390638008714\n",
      "nex =  tensor([-0.0306, -0.2032,  0.0192,  0.3400])\n",
      "nex_pred =  tensor([-0.0268, -0.2101,  0.0231,  0.3458], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011052687477786094\n",
      "nex =  tensor([ 0.0472, -0.2144,  0.0466,  0.3562])\n",
      "nex_pred =  tensor([ 0.0378, -0.2156,  0.0264,  0.3672], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000617594865616411\n",
      "nex =  tensor([-0.0371, -0.1544, -0.0334,  0.3051])\n",
      "nex_pred =  tensor([-0.0251, -0.1651, -0.0115,  0.3038], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0007423523929901421\n",
      "nex =  tensor([-0.0238, -0.1594,  0.0359,  0.3248])\n",
      "nex_pred =  tensor([-0.0208, -0.1659,  0.0368,  0.3309], grad_fn=<ThAddBackward>)\n",
      "loss =  8.942699059844017e-05\n",
      "nex =  tensor([ 0.0467, -0.2234, -0.0050,  0.2532])\n",
      "nex_pred =  tensor([ 0.0408, -0.2243, -0.0179,  0.2631], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00030020755366422236\n",
      "nex =  tensor([-0.0078, -0.1980, -0.0148,  0.2875])\n",
      "nex_pred =  tensor([-0.0038, -0.2039, -0.0091,  0.2926], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010849906539078802\n",
      "nex =  tensor([-0.0234,  0.1800,  0.0054, -0.2672])\n",
      "nex_pred =  tensor([-0.0201,  0.1780,  0.0061, -0.2620], grad_fn=<ThAddBackward>)\n",
      "loss =  4.1934858018066734e-05\n",
      "nex =  tensor([ 0.0170, -0.1998, -0.0487,  0.3060])\n",
      "nex_pred =  tensor([ 0.0234, -0.2065, -0.0368,  0.3087], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002338022895855829\n",
      "nex =  tensor([-0.0255,  0.2271,  0.0002, -0.3335])\n",
      "nex_pred =  tensor([-0.0214,  0.2257,  0.0025, -0.3295], grad_fn=<ThAddBackward>)\n",
      "loss =  4.002643981948495e-05\n",
      "nex =  tensor([ 0.0080, -0.1768,  0.0326,  0.3136])\n",
      "nex_pred =  tensor([ 0.0064, -0.1807,  0.0254,  0.3218], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001373410050291568\n",
      "nex =  tensor([-0.0407,  0.1838,  0.0053, -0.3288])\n",
      "nex_pred =  tensor([-0.0374,  0.1822,  0.0053, -0.3236], grad_fn=<ThAddBackward>)\n",
      "loss =  4.057329351780936e-05\n",
      "nex =  tensor([-0.0380, -0.2151, -0.0040,  0.3248])\n",
      "nex_pred =  tensor([-0.0322, -0.2230,  0.0056,  0.3292], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002087470202241093\n",
      "nex =  tensor([ 0.0322,  0.2341,  0.0102, -0.2511])\n",
      "nex_pred =  tensor([ 0.0300,  0.2330,  0.0038, -0.2449], grad_fn=<ThAddBackward>)\n",
      "loss =  8.516074012732133e-05\n",
      "nex =  tensor([ 0.0498,  0.2214,  0.0334, -0.2990])\n",
      "nex_pred =  tensor([ 0.0379,  0.2211,  0.0117, -0.2884], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0007262498838827014\n",
      "nex =  tensor([-0.0136,  0.2185,  0.0197, -0.2878])\n",
      "nex_pred =  tensor([-0.0127,  0.2176,  0.0168, -0.2820], grad_fn=<ThAddBackward>)\n",
      "loss =  4.4322801841190085e-05\n",
      "nex =  tensor([ 0.0267,  0.2223, -0.0068, -0.2713])\n",
      "nex_pred =  tensor([ 0.0269,  0.2209, -0.0100, -0.2661], grad_fn=<ThAddBackward>)\n",
      "loss =  3.9042348362272605e-05\n",
      "nex =  tensor([ 0.0412,  0.2378, -0.0158, -0.2489])\n",
      "nex_pred =  tensor([ 0.0425,  0.2362, -0.0166, -0.2448], grad_fn=<ThAddBackward>)\n",
      "loss =  2.1805180949741043e-05\n",
      "nex =  tensor([ 0.0489,  0.1715, -0.0074, -0.2634])\n",
      "nex_pred =  tensor([ 0.0434,  0.1704, -0.0197, -0.2556], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000246054696617648\n",
      "nex =  tensor([ 0.0338,  0.2385,  0.0048, -0.3250])\n",
      "nex_pred =  tensor([ 0.0296,  0.2379, -0.0057, -0.3180], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00017682620091363788\n",
      "nex =  tensor([-0.0289,  0.1649,  0.0420, -0.3113])\n",
      "nex_pred =  tensor([-0.0324,  0.1652,  0.0291, -0.3019], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002668352972250432\n",
      "nex =  tensor([-0.0036, -0.1520, -0.0133,  0.3264])\n",
      "nex_pred =  tensor([ 0.0040, -0.1601, -0.0017,  0.3278], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002587687049526721\n",
      "nex =  tensor([ 0.0162,  0.2008, -0.0031, -0.3354])\n",
      "nex_pred =  tensor([ 0.0138,  0.1998, -0.0119, -0.3286], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00013002532068639994\n",
      "nex =  tensor([ 0.0265,  0.2087,  0.0409, -0.2929])\n",
      "nex_pred =  tensor([ 0.0170,  0.2087,  0.0218, -0.2826], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0005603942554444075\n",
      "nex =  tensor([-0.0091, -0.2216,  0.0364,  0.3063])\n",
      "nex_pred =  tensor([-0.0118, -0.2242,  0.0269,  0.3168], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00021305876725818962\n",
      "nex =  tensor([ 0.0262,  0.2215, -0.0397, -0.3263])\n",
      "nex_pred =  tensor([ 0.0298,  0.2195, -0.0379, -0.3232], grad_fn=<ThAddBackward>)\n",
      "loss =  3.0192672056728043e-05\n",
      "nex =  tensor([-0.0107,  0.1779,  0.0467, -0.2696])\n",
      "nex_pred =  tensor([-0.0156,  0.1779,  0.0332, -0.2600], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00029754944262094796\n",
      "nex =  tensor([-0.0186,  0.1858,  0.0181, -0.2924])\n",
      "nex_pred =  tensor([-0.0185,  0.1848,  0.0130, -0.2856], grad_fn=<ThAddBackward>)\n",
      "loss =  7.316806295420974e-05\n",
      "nex =  tensor([-0.0047,  0.2370, -0.0267, -0.2663])\n",
      "nex_pred =  tensor([ 0.0043,  0.2341, -0.0155, -0.2654], grad_fn=<ThAddBackward>)\n",
      "loss =  0.000213394348975271\n",
      "nex =  tensor([ 0.0397, -0.2216, -0.0212,  0.2585])\n",
      "nex_pred =  tensor([ 0.0371, -0.2238, -0.0273,  0.2667], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011665550118777901\n",
      "nex =  tensor([ 0.0380, -0.2037,  0.0145,  0.3333])\n",
      "nex_pred =  tensor([ 0.0343, -0.2068,  0.0052,  0.3415], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00017651997040957212\n",
      "nex =  tensor([-0.0284,  0.1728,  0.0474, -0.2863])\n",
      "nex_pred =  tensor([-0.0315,  0.1730,  0.0357, -0.2771], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00023105661966837943\n",
      "nex =  tensor([ 0.0095, -0.2293,  0.0407,  0.3033])\n",
      "nex_pred =  tensor([ 0.0036, -0.2303,  0.0252,  0.3151], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00041449893615208566\n",
      "nex =  tensor([-0.0236, -0.1862,  0.0466,  0.2914])\n",
      "nex_pred =  tensor([-0.0245, -0.1899,  0.0394,  0.3013], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016469227557536215\n",
      "nex =  tensor([ 0.0058, -0.1689,  0.0363,  0.3439])\n",
      "nex_pred =  tensor([ 0.0055, -0.1739,  0.0315,  0.3509], grad_fn=<ThAddBackward>)\n",
      "loss =  9.751961624715477e-05\n",
      "nex =  tensor([ 0.0106, -0.2424, -0.0304,  0.2794])\n",
      "nex_pred =  tensor([ 0.0120, -0.2465, -0.0284,  0.2861], grad_fn=<ThAddBackward>)\n",
      "loss =  6.889516225783154e-05\n",
      "nex =  tensor([-0.0147, -0.1513,  0.0130,  0.3126])\n",
      "nex_pred =  tensor([-0.0098, -0.1583,  0.0184,  0.3167], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011962629650952294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([-0.0230, -0.2096,  0.0171,  0.3387])\n",
      "nex_pred =  tensor([-0.0201, -0.2158,  0.0194,  0.3450], grad_fn=<ThAddBackward>)\n",
      "loss =  9.190656419377774e-05\n",
      "nex =  tensor([-0.0315, -0.1647,  0.0051,  0.3213])\n",
      "nex_pred =  tensor([-0.0244, -0.1732,  0.0154,  0.3242], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002367040142416954\n",
      "nex =  tensor([ 0.0097, -0.2416,  0.0041,  0.3367])\n",
      "nex_pred =  tensor([ 0.0090, -0.2454,  0.0004,  0.3447], grad_fn=<ThAddBackward>)\n",
      "loss =  9.127661905949935e-05\n",
      "nex =  tensor([-0.0161, -0.2438, -0.0448,  0.2543])\n",
      "nex_pred =  tensor([-0.0117, -0.2496, -0.0355,  0.2595], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001662598515395075\n",
      "nex =  tensor([ 0.0064, -0.1708,  0.0351,  0.2955])\n",
      "nex_pred =  tensor([ 0.0043, -0.1743,  0.0268,  0.3043], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016319363203365356\n",
      "nex =  tensor([-0.0195,  0.2089,  0.0485, -0.2793])\n",
      "nex_pred =  tensor([-0.0223,  0.2090,  0.0388, -0.2711], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016914353182073683\n",
      "nex =  tensor([-0.0203, -0.2057, -0.0494,  0.2711])\n",
      "nex_pred =  tensor([-0.0123, -0.2136, -0.0333,  0.2734], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003885345358867198\n",
      "nex =  tensor([-0.0325, -0.1614, -0.0302,  0.2984])\n",
      "nex_pred =  tensor([-0.0219, -0.1712, -0.0112,  0.2983], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0005676713190041482\n",
      "nex =  tensor([-0.0370,  0.1999, -0.0354, -0.2904])\n",
      "nex_pred =  tensor([-0.0262,  0.1956, -0.0207, -0.2900], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003513707488309592\n",
      "nex =  tensor([-0.0459,  0.1751, -0.0477, -0.3567])\n",
      "nex_pred =  tensor([-0.0365,  0.1705, -0.0356, -0.3558], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002576324623078108\n",
      "nex =  tensor([ 0.0059,  0.1532,  0.0480, -0.2383])\n",
      "nex_pred =  tensor([-0.0017,  0.1531,  0.0304, -0.2275], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00048531146603636444\n",
      "nex =  tensor([ 0.0315,  0.1973,  0.0093, -0.2858])\n",
      "nex_pred =  tensor([ 0.0264,  0.1965, -0.0029, -0.2778], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00024060092982836068\n",
      "nex =  tensor([ 0.0428,  0.1632, -0.0325, -0.3081])\n",
      "nex_pred =  tensor([ 0.0404,  0.1616, -0.0407, -0.3020], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011202349560335279\n",
      "nex =  tensor([-0.0023, -0.1688, -0.0461,  0.2554])\n",
      "nex_pred =  tensor([ 0.0054, -0.1758, -0.0322,  0.2574], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003032970707863569\n",
      "nex =  tensor([ 0.0159,  0.2330,  0.0459, -0.2884])\n",
      "nex_pred =  tensor([ 0.0086,  0.2331,  0.0307, -0.2791], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0003702241519931704\n",
      "nex =  tensor([-0.0330,  0.1628,  0.0066, -0.3261])\n",
      "nex_pred =  tensor([-0.0317,  0.1614,  0.0027, -0.3198], grad_fn=<ThAddBackward>)\n",
      "loss =  6.026940536685288e-05\n",
      "nex =  tensor([-0.0367, -0.2368,  0.0278,  0.3055])\n",
      "nex_pred =  tensor([-0.0364, -0.2410,  0.0247,  0.3149], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011715936852851883\n",
      "nex =  tensor([ 0.0122,  0.2244, -0.0169, -0.2798])\n",
      "nex_pred =  tensor([ 0.0160,  0.2225, -0.0145, -0.2762], grad_fn=<ThAddBackward>)\n",
      "loss =  3.669113357318565e-05\n",
      "nex =  tensor([-0.0312, -0.2177, -0.0216,  0.3293])\n",
      "nex_pred =  tensor([-0.0239, -0.2262, -0.0082,  0.3323], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00031573011074215174\n",
      "nex =  tensor([ 0.0453,  0.1937,  0.0308, -0.2566])\n",
      "nex_pred =  tensor([ 0.0350,  0.1932,  0.0114, -0.2463], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0005870025488547981\n",
      "nex =  tensor([-0.0057,  0.1705, -0.0392, -0.3524])\n",
      "nex_pred =  tensor([-0.0017,  0.1677, -0.0378, -0.3487], grad_fn=<ThAddBackward>)\n",
      "loss =  3.927441866835579e-05\n",
      "nex =  tensor([-0.0414,  0.2352,  0.0401, -0.3020])\n",
      "nex_pred =  tensor([-0.0396,  0.2349,  0.0385, -0.2963], grad_fn=<ThAddBackward>)\n",
      "loss =  3.809397094300948e-05\n",
      "nex =  tensor([-0.0313, -0.1883,  0.0343,  0.2710])\n",
      "nex_pred =  tensor([-0.0308, -0.1926,  0.0304,  0.2802], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011837840429507196\n",
      "nex =  tensor([ 0.0353,  0.1774, -0.0141, -0.2923])\n",
      "nex_pred =  tensor([ 0.0323,  0.1761, -0.0232, -0.2855], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001395066356053576\n",
      "nex =  tensor([ 0.0296, -0.2287, -0.0479,  0.3143])\n",
      "nex_pred =  tensor([ 0.0333, -0.2340, -0.0406,  0.3188], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011550719500519335\n",
      "nex =  tensor([ 0.0215,  0.1468, -0.0462, -0.3293])\n",
      "nex_pred =  tensor([ 0.0227,  0.1443, -0.0491, -0.3247], grad_fn=<ThAddBackward>)\n",
      "loss =  3.747617301996797e-05\n",
      "nex =  tensor([ 0.0315, -0.2270, -0.0139,  0.3205])\n",
      "nex_pred =  tensor([ 0.0308, -0.2307, -0.0166,  0.3275], grad_fn=<ThAddBackward>)\n",
      "loss =  7.044220546958968e-05\n",
      "nex =  tensor([ 0.0465,  0.2148,  0.0464, -0.3127])\n",
      "nex_pred =  tensor([ 0.0320,  0.2148,  0.0202, -0.3008], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0010379335144534707\n",
      "nex =  tensor([-0.0408,  0.2452, -0.0319, -0.2537])\n",
      "nex_pred =  tensor([-0.0257,  0.2403, -0.0098, -0.2562], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0007493351586163044\n",
      "nex =  tensor([ 0.0480,  0.1681,  0.0047, -0.2936])\n",
      "nex_pred =  tensor([ 0.0393,  0.1673, -0.0134, -0.2842], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004902833024971187\n",
      "nex =  tensor([-0.0159, -0.1844, -0.0417,  0.3108])\n",
      "nex_pred =  tensor([-0.0063, -0.1934, -0.0237,  0.3114], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0004956303164362907\n",
      "nex =  tensor([ 0.0275, -0.2330,  0.0233,  0.3487])\n",
      "nex_pred =  tensor([ 0.0230, -0.2355,  0.0120,  0.3581], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00024268211564049125\n",
      "nex =  tensor([-0.0341,  0.1711,  0.0015, -0.2444])\n",
      "nex_pred =  tensor([-0.0285,  0.1683,  0.0063, -0.2402], grad_fn=<ThAddBackward>)\n",
      "loss =  7.944243407109752e-05\n",
      "nex =  tensor([-0.0052, -0.2270, -0.0313,  0.2771])\n",
      "nex_pred =  tensor([-0.0015, -0.2324, -0.0248,  0.2825], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011616101983236149\n",
      "nex =  tensor([-0.0132, -0.1772,  0.0373,  0.3494])\n",
      "nex_pred =  tensor([-0.0116, -0.1831,  0.0358,  0.3560], grad_fn=<ThAddBackward>)\n",
      "loss =  8.35705577628687e-05\n",
      "nex =  tensor([-0.0257, -0.1501, -0.0089,  0.2895])\n",
      "nex_pred =  tensor([-0.0178, -0.1583,  0.0031,  0.2916], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0002792192390188575\n",
      "nex =  tensor([ 0.0213,  0.1827, -0.0167, -0.3198])\n",
      "nex_pred =  tensor([ 0.0200,  0.1813, -0.0235, -0.3137], grad_fn=<ThAddBackward>)\n",
      "loss =  8.752595022087917e-05\n",
      "nex =  tensor([-0.0096,  0.1836,  0.0034, -0.3115])\n",
      "nex_pred =  tensor([-0.0094,  0.1823, -0.0014, -0.3052], grad_fn=<ThAddBackward>)\n",
      "loss =  6.450865475926548e-05\n",
      "nex =  tensor([-0.0095, -0.2086,  0.0481,  0.3230])\n",
      "nex_pred =  tensor([-0.0123, -0.2115,  0.0378,  0.3334], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00023222317395266145\n",
      "nex =  tensor([-0.0158,  0.2029,  0.0078, -0.3066])\n",
      "nex_pred =  tensor([-0.0143,  0.2017,  0.0056, -0.3009], grad_fn=<ThAddBackward>)\n",
      "loss =  4.088625428266823e-05\n",
      "nex =  tensor([-0.0471, -0.2116, -0.0136,  0.3014])\n",
      "nex_pred =  tensor([-0.0403, -0.2202, -0.0010,  0.3051], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00029266986530274153\n",
      "nex =  tensor([ 0.0293, -0.1946,  0.0105,  0.2500])\n",
      "nex_pred =  tensor([ 0.0249, -0.1963, -0.0010,  0.2597], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00024860422126948833\n",
      "nex =  tensor([ 0.0412,  0.2122, -0.0088, -0.3049])\n",
      "nex_pred =  tensor([ 0.0376,  0.2112, -0.0183, -0.2982], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001483603846281767\n",
      "nex =  tensor([ 0.0247, -0.1925, -0.0229,  0.3257])\n",
      "nex_pred =  tensor([ 0.0281, -0.1983, -0.0180,  0.3298], grad_fn=<ThAddBackward>)\n",
      "loss =  8.497982344124466e-05\n",
      "nex =  tensor([ 0.0340, -0.2111,  0.0290,  0.3413])\n",
      "nex_pred =  tensor([ 0.0287, -0.2136,  0.0161,  0.3507], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00028759133419953287\n",
      "nex =  tensor([-0.0095,  0.1585,  0.0130, -0.2497])\n",
      "nex_pred =  tensor([-0.0096,  0.1570,  0.0078, -0.2427], grad_fn=<ThAddBackward>)\n",
      "loss =  7.906940300017595e-05\n",
      "nex =  tensor([ 0.0331,  0.2215, -0.0397, -0.2969])\n",
      "nex_pred =  tensor([ 0.0369,  0.2195, -0.0372, -0.2939], grad_fn=<ThAddBackward>)\n",
      "loss =  3.3340344089083374e-05\n",
      "nex =  tensor([ 0.0478,  0.2243, -0.0208, -0.3475])\n",
      "nex_pred =  tensor([ 0.0443,  0.2233, -0.0305, -0.3413], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00014547065075021237\n",
      "nex =  tensor([ 0.0394,  0.2414,  0.0420, -0.3194])\n",
      "nex_pred =  tensor([ 0.0278,  0.2415,  0.0208, -0.3089], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0006932972464710474\n",
      "nex =  tensor([-0.0475,  0.2297,  0.0201, -0.2409])\n",
      "nex_pred =  tensor([-0.0396,  0.2272,  0.0293, -0.2386], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00015786800940986723\n",
      "nex =  tensor([ 0.0430,  0.2229,  0.0055, -0.2635])\n",
      "nex_pred =  tensor([ 0.0388,  0.2220, -0.0041, -0.2565], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00016130048607010394\n",
      "nex =  tensor([-0.0130,  0.2394,  0.0049, -0.2540])\n",
      "nex_pred =  tensor([-0.0073,  0.2374,  0.0106, -0.2511], grad_fn=<ThAddBackward>)\n",
      "loss =  7.64130090828985e-05\n",
      "nex =  tensor([-0.0035,  0.2068,  0.0357, -0.3044])\n",
      "nex_pred =  tensor([-0.0078,  0.2068,  0.0238, -0.2959], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00023321900516748428\n",
      "nex =  tensor([-0.0140,  0.2134,  0.0059, -0.3363])\n",
      "nex_pred =  tensor([-0.0131,  0.2124,  0.0025, -0.3306], grad_fn=<ThAddBackward>)\n",
      "loss =  4.5305434468900785e-05\n",
      "nex =  tensor([-0.0190, -0.2200, -0.0242,  0.2642])\n",
      "nex_pred =  tensor([-0.0150, -0.2258, -0.0174,  0.2698], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00012614969455171376\n",
      "nex =  tensor([-0.0006, -0.2260,  0.0102,  0.3239])\n",
      "nex_pred =  tensor([-0.0006, -0.2302,  0.0074,  0.3317], grad_fn=<ThAddBackward>)\n",
      "loss =  8.648645598441362e-05\n",
      "nex =  tensor([ 0.0253, -0.1649,  0.0296,  0.3199])\n",
      "nex_pred =  tensor([ 0.0227, -0.1687,  0.0212,  0.3276], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001518310746178031\n",
      "nex =  tensor([-0.0498, -0.1901,  0.0337,  0.3024])\n",
      "nex_pred =  tensor([-0.0466, -0.1966,  0.0358,  0.3098], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00011195716797374189\n",
      "nex =  tensor([ 0.0104, -0.1919,  0.0041,  0.3274])\n",
      "nex_pred =  tensor([ 0.0119, -0.1971,  0.0042,  0.3332], grad_fn=<ThAddBackward>)\n",
      "loss =  6.361335545079783e-05\n",
      "nex =  tensor([ 0.0219,  0.2076,  0.0411, -0.2657])\n",
      "nex_pred =  tensor([ 0.0142,  0.2074,  0.0250, -0.2561], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00041262307786382735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([ 0.0078,  0.1671,  0.0155, -0.2682])\n",
      "nex_pred =  tensor([ 0.0046,  0.1662,  0.0053, -0.2600], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001813460694393143\n",
      "nex =  tensor([-0.0122,  0.1956, -0.0415, -0.3218])\n",
      "nex_pred =  tensor([-0.0047,  0.1923, -0.0332, -0.3200], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00013964943354949355\n",
      "nex =  tensor([ 0.0214, -0.1610, -0.0015,  0.2433])\n",
      "nex_pred =  tensor([ 0.0210, -0.1644, -0.0053,  0.2503], grad_fn=<ThAddBackward>)\n",
      "loss =  7.675691449549049e-05\n",
      "nex =  tensor([ 0.0374, -0.1565,  0.0146,  0.3251])\n",
      "nex_pred =  tensor([ 0.0359, -0.1609,  0.0091,  0.3313], grad_fn=<ThAddBackward>)\n",
      "loss =  8.941436681197956e-05\n",
      "nex =  tensor([ 0.0310, -0.1808,  0.0148,  0.2571])\n",
      "nex_pred =  tensor([ 0.0266, -0.1828,  0.0035,  0.2664], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00023895387130323797\n",
      "nex =  tensor([ 0.0301,  0.2090, -0.0242, -0.3058])\n",
      "nex_pred =  tensor([ 0.0307,  0.2074, -0.0273, -0.3009], grad_fn=<ThAddBackward>)\n",
      "loss =  3.604258017730899e-05\n",
      "nex =  tensor([ 0.0378, -0.1780, -0.0247,  0.3113])\n",
      "nex_pred =  tensor([ 0.0401, -0.1831, -0.0217,  0.3156], grad_fn=<ThAddBackward>)\n",
      "loss =  5.859995508217253e-05\n",
      "nex =  tensor([-0.0231,  0.2098, -0.0479, -0.3031])\n",
      "nex_pred =  tensor([-0.0119,  0.2055, -0.0327, -0.3033], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00037480462924577296\n",
      "nex =  tensor([ 0.0478, -0.2111, -0.0149,  0.3144])\n",
      "nex_pred =  tensor([ 0.0458, -0.2145, -0.0196,  0.3214], grad_fn=<ThAddBackward>)\n",
      "loss =  8.582793088862672e-05\n",
      "nex =  tensor([-0.0239,  0.1820,  0.0172, -0.3159])\n",
      "nex_pred =  tensor([-0.0240,  0.1811,  0.0113, -0.3090], grad_fn=<ThAddBackward>)\n",
      "loss =  8.439661178272218e-05\n",
      "nex =  tensor([-0.0466, -0.2040,  0.0475,  0.2639])\n",
      "nex_pred =  tensor([-0.0471, -0.2074,  0.0409,  0.2751], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00018096710846293718\n",
      "nex =  tensor([-0.0404,  0.1619, -0.0394, -0.2883])\n",
      "nex_pred =  tensor([-0.0308,  0.1572, -0.0268, -0.2870], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00027658711769618094\n",
      "nex =  tensor([-0.0077,  0.1769,  0.0192, -0.3056])\n",
      "nex_pred =  tensor([-0.0102,  0.1762,  0.0096, -0.2976], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001634527143323794\n",
      "nex =  tensor([ 0.0213, -0.1978, -0.0067,  0.2506])\n",
      "nex_pred =  tensor([ 0.0199, -0.2007, -0.0118,  0.2586], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00010066108370665461\n",
      "nex =  tensor([-0.0087,  0.2405,  0.0044, -0.3048])\n",
      "nex_pred =  tensor([-0.0056,  0.2393,  0.0055, -0.3005], grad_fn=<ThAddBackward>)\n",
      "loss =  2.9861948860343546e-05\n",
      "nex =  tensor([ 0.0380,  0.1469, -0.0285, -0.3018])\n",
      "nex_pred =  tensor([ 0.0351,  0.1452, -0.0378, -0.2952], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0001409826654708013\n",
      "nex =  tensor([-0.0178,  0.1559, -0.0375, -0.3010])\n",
      "nex_pred =  tensor([-0.0115,  0.1524, -0.0316, -0.2980], grad_fn=<ThAddBackward>)\n",
      "loss =  9.521093306830153e-05\n",
      "nex = "
     ]
    }
   ],
   "source": [
    "t = []\n",
    "for epoch in range(500): \n",
    "    \n",
    "    curr = env.reset()\n",
    "    for i in range(200):\n",
    "\n",
    "        # Generate a random step \n",
    "        st = random.randint(0,1)\n",
    "\n",
    "        # Get simulated result from the environment\n",
    "        nex, rew, done, info = env.step(st)\n",
    "        nex = torch.from_numpy(nex).float()\n",
    "        \n",
    "        # Check if done and then break\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Create input for our network and generate prediction\n",
    "        input = torch.from_numpy(np.append(curr,st)).float()\n",
    "        nex_pred = model(input)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(nex_pred, nex) \n",
    "        \n",
    "        if i %100 == 0:\n",
    "            print(\"nex = \", nex)\n",
    "            print(\"nex_pred = \", nex_pred)\n",
    "            print(\"loss = \", loss.item())\n",
    "\n",
    "\n",
    "        # Backprop\n",
    "        # opt.zero_grad()\n",
    "        # loss.backward()\n",
    "        # opt.step()\n",
    "        \n",
    "        curr = nex\n",
    "        \n",
    "    t.append(loss.item())\n",
    "    \n",
    "    # epoch % 1000 == 0 and print(\"Epoch %d done\" % epoch)\n",
    "    \n",
    "plt.plot(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04015303, -0.03614903,  0.03989927,  0.02288957])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = env.reset()\n",
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "?model.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "?nn.BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
