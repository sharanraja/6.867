{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(5, 64)\n",
    "        # self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        # self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "η = 1e-3\n",
    "model = Net()\n",
    "opt = Adam(model.parameters(), lr = η)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11ecfcbe0>]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = []\n",
    "for epoch in range(1000): \n",
    "    \n",
    "    curr = env.reset()\n",
    "    for i in range(200):\n",
    "\n",
    "        # Generate a random step \n",
    "        st = random.randint(0,1)\n",
    "\n",
    "        # Get simulated result from the environment\n",
    "        nex, rew, done, info = env.step(st)\n",
    "        nex = torch.from_numpy(nex).float()\n",
    "\n",
    "        # Check if done and then break\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Create input for our network and generate prediction\n",
    "        input = torch.from_numpy(np.append(curr,st)).float()\n",
    "        nex_pred = model(input)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(nex_pred, nex)\n",
    "\n",
    "        # Backprop\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        curr = nex\n",
    "        \n",
    "    l.append(loss.item())\n",
    "    \n",
    "    epoch % 1000 == 0 and print(\"Epoch %d done\" % epoch)\n",
    "    \n",
    "plt.plot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0040070414543151855"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([ 0.0354,  0.1953,  0.0380, -0.2946])\n",
      "nex_pred =  tensor([-0.0396,  0.2218,  0.0196, -0.2894], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01873822510242462\n",
      "nex =  tensor([ 0.0272,  0.1549,  0.0326, -0.2572])\n",
      "nex_pred =  tensor([-0.0409,  0.1810,  0.0159, -0.2566], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01846432313323021\n",
      "nex =  tensor([-0.0372, -0.2155,  0.0442,  0.2900])\n",
      "nex_pred =  tensor([-0.0477, -0.1918,  0.0366,  0.2826], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002153719076886773\n",
      "nex =  tensor([-0.0256,  0.1810,  0.0096, -0.3041])\n",
      "nex_pred =  tensor([-0.0503,  0.1910,  0.0095, -0.3272], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003511992748826742\n",
      "nex =  tensor([ 0.0280, -0.2097,  0.0208,  0.3218])\n",
      "nex_pred =  tensor([-0.0253, -0.2034,  0.0217,  0.3099], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007826524786651134\n",
      "nex =  tensor([-0.0483,  0.1662, -0.0283, -0.3438])\n",
      "nex_pred =  tensor([-0.0468,  0.1649, -0.0143, -0.3814], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004177589900791645\n",
      "nex =  tensor([ 0.0250, -0.1846, -0.0038,  0.2660])\n",
      "nex_pred =  tensor([-0.0172, -0.1900,  0.0027,  0.2466], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006887639872729778\n",
      "nex =  tensor([ 0.0056,  0.1777, -0.0151, -0.3259])\n",
      "nex_pred =  tensor([-0.0341,  0.1798, -0.0118, -0.3462], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005401718430221081\n",
      "nex =  tensor([-0.0263, -0.1545,  0.0346,  0.2776])\n",
      "nex_pred =  tensor([-0.0409, -0.1523,  0.0386,  0.2599], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0017088401364162564\n",
      "nex =  tensor([-0.0379,  0.2104, -0.0451, -0.3139])\n",
      "nex_pred =  tensor([-0.0385,  0.1739, -0.0150, -0.3668], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013160577975213528\n",
      "nex =  tensor([-0.0306,  0.2385, -0.0463, -0.2782])\n",
      "nex_pred =  tensor([-0.0356,  0.1816, -0.0084, -0.3390], grad_fn=<ThAddBackward>)\n",
      "loss =  0.022633688524365425\n",
      "nex =  tensor([ 0.0378,  0.1986, -0.0253, -0.3392])\n",
      "nex_pred =  tensor([-0.0215,  0.1921, -0.0207, -0.3553], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009696660563349724\n",
      "nex =  tensor([-0.0252,  0.1968, -0.0010, -0.2844])\n",
      "nex_pred =  tensor([-0.0470,  0.1884,  0.0084, -0.3170], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004880466498434544\n",
      "nex =  tensor([-0.0267, -0.1683,  0.0270,  0.2635])\n",
      "nex_pred =  tensor([-0.0387, -0.1633,  0.0293,  0.2461], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0015170769765973091\n",
      "nex =  tensor([-0.0410, -0.1678,  0.0358,  0.3285])\n",
      "nex_pred =  tensor([-0.0471, -0.1758,  0.0470,  0.3028], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0023760709445923567\n",
      "nex =  tensor([-0.0471, -0.1616, -0.0283,  0.2770])\n",
      "nex_pred =  tensor([-0.0293, -0.2047,  0.0020,  0.2261], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01744837686419487\n",
      "nex =  tensor([-0.0147, -0.1629,  0.0076,  0.2473])\n",
      "nex_pred =  tensor([-0.0297, -0.1676,  0.0147,  0.2245], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0027503527235239744\n",
      "nex =  tensor([-0.0172, -0.1667,  0.0338,  0.2677])\n",
      "nex_pred =  tensor([-0.0382, -0.1572,  0.0333,  0.2549], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0021784959826618433\n",
      "nex =  tensor([-0.0299,  0.1822, -0.0167, -0.2599])\n",
      "nex_pred =  tensor([-0.0440,  0.1609,  0.0011, -0.3019], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008572873659431934\n",
      "nex =  tensor([ 0.0151,  0.1527, -0.0008, -0.3146])\n",
      "nex_pred =  tensor([-0.0354,  0.1712, -0.0080, -0.3238], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008668234571814537\n",
      "nex =  tensor([-0.0064, -0.1973, -0.0483,  0.2403])\n",
      "nex_pred =  tensor([-0.0127, -0.2293, -0.0263,  0.2031], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009323053061962128\n",
      "nex =  tensor([ 0.0475,  0.1998,  0.0499, -0.2393])\n",
      "nex_pred =  tensor([-0.0390,  0.2189,  0.0323, -0.2324], grad_fn=<ThAddBackward>)\n",
      "loss =  0.025721434503793716\n",
      "nex =  tensor([-0.0190, -0.1565, -0.0214,  0.2611])\n",
      "nex_pred =  tensor([-0.0234, -0.1882,  0.0013,  0.2207], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010380566120147705\n",
      "nex =  tensor([ 0.0480, -0.2315, -0.0425,  0.2434])\n",
      "nex_pred =  tensor([-0.0010, -0.2428, -0.0349,  0.2253], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008520775474607944\n",
      "nex =  tensor([-0.0376, -0.1972,  0.0485,  0.2622])\n",
      "nex_pred =  tensor([-0.0478, -0.1690,  0.0380,  0.2581], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003077085828408599\n",
      "nex =  tensor([ 0.0344,  0.1801, -0.0131, -0.3183])\n",
      "nex_pred =  tensor([-0.0260,  0.1826, -0.0138, -0.3304], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010345610789954662\n",
      "nex =  tensor([ 0.0082, -0.2185,  0.0378,  0.2941])\n",
      "nex_pred =  tensor([-0.0342, -0.1930,  0.0279,  0.2928], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006918792612850666\n",
      "nex =  tensor([ 0.0134,  0.2149,  0.0382, -0.2325])\n",
      "nex_pred =  tensor([-0.0459,  0.2160,  0.0339, -0.2447], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011556874960660934\n",
      "nex =  tensor([ 0.0105, -0.1693,  0.0200,  0.2804])\n",
      "nex_pred =  tensor([-0.0276, -0.1681,  0.0237,  0.2648], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005211641080677509\n",
      "nex =  tensor([-0.0168, -0.2204, -0.0502,  0.2469])\n",
      "nex_pred =  tensor([-0.0153, -0.2501, -0.0294,  0.2107], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007834077812731266\n",
      "nex =  tensor([ 0.0351, -0.1735, -0.0370,  0.2448])\n",
      "nex_pred =  tensor([-0.0051, -0.1999, -0.0193,  0.2145], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011628402397036552\n",
      "nex =  tensor([-0.0410,  0.2305,  0.0017, -0.2984])\n",
      "nex_pred =  tensor([-0.0524,  0.2149,  0.0160, -0.3376], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005582038778811693\n",
      "nex =  tensor([ 0.0138,  0.1932,  0.0071, -0.2572])\n",
      "nex_pred =  tensor([-0.0374,  0.1869,  0.0098, -0.2773], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009543843567371368\n",
      "nex =  tensor([ 0.0138,  0.2275,  0.0330, -0.2762])\n",
      "nex_pred =  tensor([-0.0444,  0.2326,  0.0274, -0.2866], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009868466295301914\n",
      "nex =  tensor([-0.0417,  0.1536, -0.0105, -0.3413])\n",
      "nex_pred =  tensor([-0.0499,  0.1688, -0.0074, -0.3680], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002724169520661235\n",
      "nex =  tensor([ 0.0228, -0.2145, -0.0342,  0.2769])\n",
      "nex_pred =  tensor([-0.0104, -0.2367, -0.0191,  0.2475], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00760852824896574\n",
      "nex =  tensor([-0.0449,  0.2171,  0.0304, -0.2516])\n",
      "nex_pred =  tensor([-0.0615,  0.2130,  0.0358, -0.2839], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004052383825182915\n",
      "nex =  tensor([ 0.0344, -0.2097,  0.0501,  0.3383])\n",
      "nex_pred =  tensor([-0.0322, -0.1854,  0.0422,  0.3409], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012628293596208096\n",
      "nex =  tensor([ 0.0211, -0.2014, -0.0133,  0.3194])\n",
      "nex_pred =  tensor([-0.0177, -0.2231,  0.0028,  0.2887], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008384186774492264\n",
      "nex =  tensor([-0.0475, -0.2249,  0.0428,  0.3301])\n",
      "nex_pred =  tensor([-0.0516, -0.2112,  0.0418,  0.3155], grad_fn=<ThAddBackward>)\n",
      "loss =  0.001036431873217225\n",
      "nex =  tensor([-0.0288, -0.2212,  0.0293,  0.2807])\n",
      "nex_pred =  tensor([-0.0412, -0.2026,  0.0241,  0.2700], grad_fn=<ThAddBackward>)\n",
      "loss =  0.001778452773578465\n",
      "nex =  tensor([ 0.0022,  0.1769, -0.0387, -0.3073])\n",
      "nex_pred =  tensor([-0.0286,  0.1575, -0.0221, -0.3411], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007674557156860828\n",
      "nex =  tensor([ 0.0199, -0.2353, -0.0371,  0.2960])\n",
      "nex_pred =  tensor([-0.0111, -0.2591, -0.0215,  0.2656], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007091227453202009\n",
      "nex =  tensor([-0.0186, -0.1489, -0.0450,  0.2818])\n",
      "nex_pred =  tensor([-0.0175, -0.2057, -0.0081,  0.2263], grad_fn=<ThAddBackward>)\n",
      "loss =  0.02379298396408558\n",
      "nex =  tensor([ 0.0305,  0.1836, -0.0274, -0.2669])\n",
      "nex_pred =  tensor([-0.0231,  0.1606, -0.0143, -0.2944], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013227635063230991\n",
      "nex =  tensor([-0.0174, -0.2138, -0.0077,  0.2660])\n",
      "nex_pred =  tensor([-0.0275, -0.2190, -0.0003,  0.2417], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002264253795146942\n",
      "nex =  tensor([-0.0212,  0.2414,  0.0027, -0.2443])\n",
      "nex_pred =  tensor([-0.0463,  0.2089,  0.0218, -0.2856], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010892604477703571\n",
      "nex =  tensor([-0.0078,  0.1664,  0.0197, -0.2461])\n",
      "nex_pred =  tensor([-0.0476,  0.1738,  0.0170, -0.2652], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006763414945453405\n",
      "nex =  tensor([ 0.0490,  0.1515, -0.0249, -0.2881])\n",
      "nex_pred =  tensor([-0.0187,  0.1488, -0.0233, -0.3011], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014418896287679672\n",
      "nex =  tensor([ 0.0376,  0.1748, -0.0181, -0.3285])\n",
      "nex_pred =  tensor([-0.0237,  0.1787, -0.0191, -0.3396], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010418690741062164\n",
      "nex =  tensor([-0.0209,  0.1539,  0.0131, -0.3221])\n",
      "nex_pred =  tensor([-0.0502,  0.1813,  0.0044, -0.3357], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005235721357166767\n",
      "nex =  tensor([ 0.0315,  0.1613, -0.0427, -0.3477])\n",
      "nex_pred =  tensor([-0.0191,  0.1580, -0.0358, -0.3659], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00761898048222065\n",
      "nex =  tensor([-0.0036,  0.1530,  0.0048, -0.2823])\n",
      "nex_pred =  tensor([-0.0425,  0.1651,  0.0018, -0.2995], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006109104957431555\n",
      "nex =  tensor([ 0.0105, -0.2098, -0.0099,  0.3191])\n",
      "nex_pred =  tensor([-0.0214, -0.2279,  0.0043,  0.2894], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006358316633850336\n",
      "nex =  tensor([ 0.0421,  0.2259,  0.0161, -0.3023])\n",
      "nex_pred =  tensor([-0.0311,  0.2289,  0.0104, -0.3078], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01430762093514204\n",
      "nex =  tensor([-0.0498, -0.2159, -0.0369,  0.2624])\n",
      "nex_pred =  tensor([-0.0278, -0.2453, -0.0146,  0.2209], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0103073101490736\n",
      "nex =  tensor([ 0.0454,  0.1947,  0.0489, -0.3248])\n",
      "nex_pred =  tensor([-0.0397,  0.2382,  0.0204, -0.3075], grad_fn=<ThAddBackward>)\n",
      "loss =  0.02665901742875576\n",
      "nex =  tensor([ 0.0171,  0.1897, -0.0090, -0.3195])\n",
      "nex_pred =  tensor([-0.0322,  0.1908, -0.0076, -0.3364], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007320661563426256\n",
      "nex =  tensor([ 0.0349,  0.1866,  0.0402, -0.3245])\n",
      "nex_pred =  tensor([-0.0406,  0.2260,  0.0158, -0.3133], grad_fn=<ThAddBackward>)\n",
      "loss =  0.02109120972454548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([ 0.0332, -0.2223,  0.0136,  0.3277])\n",
      "nex_pred =  tensor([-0.0224, -0.2182,  0.0154,  0.3144], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008257918059825897\n",
      "nex =  tensor([-0.0276, -0.1542,  0.0244,  0.3195])\n",
      "nex_pred =  tensor([-0.0399, -0.1703,  0.0397,  0.2899], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004260935354977846\n",
      "nex =  tensor([ 0.0183,  0.2351, -0.0085, -0.2726])\n",
      "nex_pred =  tensor([-0.0313,  0.2077,  0.0053, -0.3019], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011817414313554764\n",
      "nex =  tensor([-0.0390,  0.2235, -0.0288, -0.3357])\n",
      "nex_pred =  tensor([-0.0434,  0.1996, -0.0064, -0.3811], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007753781042993069\n",
      "nex =  tensor([ 0.0115,  0.2126,  0.0427, -0.2704])\n",
      "nex_pred =  tensor([-0.0479,  0.2280,  0.0315, -0.2760], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011314024217426777\n",
      "nex =  tensor([ 0.0006,  0.2184, -0.0333, -0.2612])\n",
      "nex_pred =  tensor([-0.0299,  0.1751, -0.0071, -0.3062], grad_fn=<ThAddBackward>)\n",
      "loss =  0.016129452735185623\n",
      "nex =  tensor([-0.0076, -0.1855, -0.0093,  0.3016])\n",
      "nex_pred =  tensor([-0.0253, -0.2085,  0.0084,  0.2673], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006591425277292728\n",
      "nex =  tensor([ 0.0449, -0.2241, -0.0458,  0.3038])\n",
      "nex_pred =  tensor([-0.0027, -0.2564, -0.0261,  0.2709], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012481272220611572\n",
      "nex =  tensor([ 0.0016, -0.1716, -0.0384,  0.2566])\n",
      "nex_pred =  tensor([-0.0136, -0.2073, -0.0144,  0.2166], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011815575882792473\n",
      "nex =  tensor([-0.0399, -0.1948, -0.0322,  0.2404])\n",
      "nex_pred =  tensor([-0.0256, -0.2198, -0.0126,  0.2022], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008510696701705456\n",
      "nex =  tensor([-0.0411,  0.1646,  0.0220, -0.3245])\n",
      "nex_pred =  tensor([-0.0587,  0.1939,  0.0138, -0.3420], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004215283319354057\n",
      "nex =  tensor([-0.0464, -0.1683,  0.0162,  0.2746])\n",
      "nex_pred =  tensor([-0.0413, -0.1766,  0.0267,  0.2458], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003170065814629197\n",
      "nex =  tensor([ 0.0456,  0.1502, -0.0458, -0.2868])\n",
      "nex_pred =  tensor([-0.0142,  0.1331, -0.0341, -0.3096], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013681955635547638\n",
      "nex =  tensor([-0.0143,  0.2156, -0.0006, -0.2945])\n",
      "nex_pred =  tensor([-0.0437,  0.2043,  0.0088, -0.3248], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0054731532000005245\n",
      "nex =  tensor([-0.0135, -0.1820,  0.0449,  0.3458])\n",
      "nex_pred =  tensor([-0.0431, -0.1795,  0.0502,  0.3305], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00291455234400928\n",
      "nex =  tensor([ 0.0272, -0.1971,  0.0062,  0.3423])\n",
      "nex_pred =  tensor([-0.0221, -0.2106,  0.0187,  0.3176], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00853064376860857\n",
      "nex =  tensor([-0.0309, -0.1690, -0.0287,  0.3310])\n",
      "nex_pred =  tensor([-0.0269, -0.2223,  0.0070,  0.2755], grad_fn=<ThAddBackward>)\n",
      "loss =  0.019292077049613\n",
      "nex =  tensor([ 0.0414,  0.1857, -0.0069, -0.3011])\n",
      "nex_pred =  tensor([-0.0255,  0.1863, -0.0085, -0.3117], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012874886393547058\n",
      "nex =  tensor([-0.0030,  0.1876, -0.0068, -0.2833])\n",
      "nex_pred =  tensor([-0.0388,  0.1796,  0.0006, -0.3102], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00624871626496315\n",
      "nex =  tensor([ 0.0002, -0.2019,  0.0329,  0.2659])\n",
      "nex_pred =  tensor([-0.0338, -0.1789,  0.0242,  0.2621], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005284890532493591\n",
      "nex =  tensor([ 0.0426,  0.1973,  0.0239, -0.2705])\n",
      "nex_pred =  tensor([-0.0334,  0.2071,  0.0140, -0.2727], grad_fn=<ThAddBackward>)\n",
      "loss =  0.017665237188339233\n",
      "nex =  tensor([-0.0211, -0.2377, -0.0497,  0.2659])\n",
      "nex_pred =  tensor([-0.0172, -0.2676, -0.0290,  0.2293], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007421224843710661\n",
      "nex =  tensor([ 0.0228, -0.2061, -0.0446,  0.3157])\n",
      "nex_pred =  tensor([-0.0088, -0.2488, -0.0178,  0.2733], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014036695472896099\n",
      "nex =  tensor([ 0.0350, -0.2306, -0.0146,  0.2852])\n",
      "nex_pred =  tensor([-0.0130, -0.2338, -0.0105,  0.2679], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0071184756234288216\n",
      "nex =  tensor([-0.0390,  0.2297, -0.0041, -0.2797])\n",
      "nex_pred =  tensor([-0.0500,  0.2052,  0.0146, -0.3233], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008159482851624489\n",
      "nex =  tensor([-0.0247, -0.2039, -0.0394,  0.3272])\n",
      "nex_pred =  tensor([-0.0226, -0.2530, -0.0072,  0.2756], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01576324552297592\n",
      "nex =  tensor([ 0.0308,  0.1500,  0.0228, -0.2369])\n",
      "nex_pred =  tensor([-0.0371,  0.1656,  0.0116, -0.2416], grad_fn=<ThAddBackward>)\n",
      "loss =  0.017684202641248703\n",
      "nex =  tensor([ 0.0342, -0.1840,  0.0237,  0.2837])\n",
      "nex_pred =  tensor([-0.0228, -0.1728,  0.0214,  0.2767], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010067930445075035\n",
      "nex =  tensor([-0.0027, -0.2165,  0.0167,  0.2524])\n",
      "nex_pred =  tensor([-0.0299, -0.1975,  0.0099,  0.2447], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0036254210863262415\n",
      "nex =  tensor([-0.0448,  0.1602,  0.0458, -0.2557])\n",
      "nex_pred =  tensor([-0.0662,  0.1883,  0.0346, -0.2731], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005416345316916704\n",
      "nex =  tensor([-0.0253, -0.2195,  0.0294,  0.2537])\n",
      "nex_pred =  tensor([-0.0393, -0.1938,  0.0197,  0.2477], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0029282323084771633\n",
      "nex =  tensor([-0.0231, -0.2125,  0.0068,  0.3429])\n",
      "nex_pred =  tensor([-0.0355, -0.2286,  0.0215,  0.3112], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004048879723995924\n",
      "nex =  tensor([-0.0469,  0.2371, -0.0313, -0.2851])\n",
      "nex_pred =  tensor([-0.0447,  0.1920,  0.0013, -0.3432], grad_fn=<ThAddBackward>)\n",
      "loss =  0.017283279448747635\n",
      "nex =  tensor([ 0.0036, -0.1976, -0.0244,  0.2573])\n",
      "nex_pred =  tensor([-0.0171, -0.2149, -0.0110,  0.2284], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005358231719583273\n",
      "nex =  tensor([ 0.0005,  0.1612, -0.0076, -0.3384])\n",
      "nex_pred =  tensor([-0.0380,  0.1775, -0.0109, -0.3528], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005241988692432642\n",
      "nex =  tensor([ 0.0368, -0.1493, -0.0309,  0.2466])\n",
      "nex_pred =  tensor([-0.0060, -0.1787, -0.0112,  0.2144], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014083119109272957\n",
      "nex =  tensor([-0.0434, -0.1551,  0.0074,  0.2881])\n",
      "nex_pred =  tensor([-0.0384, -0.1770,  0.0257,  0.2515], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006608001422137022\n",
      "nex =  tensor([-0.0231, -0.1999,  0.0304,  0.2645])\n",
      "nex_pred =  tensor([-0.0391, -0.1822,  0.0254,  0.2545], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002093617571517825\n",
      "nex =  tensor([-0.0104,  0.1834, -0.0230, -0.3435])\n",
      "nex_pred =  tensor([-0.0368,  0.1819, -0.0147, -0.3703], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0038079707883298397\n",
      "nex =  tensor([ 0.0038, -0.2110, -0.0212,  0.2448])\n",
      "nex_pred =  tensor([-0.0177, -0.2185, -0.0135,  0.2221], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0033694098237901926\n",
      "nex =  tensor([-0.0075, -0.1557,  0.0123,  0.3078])\n",
      "nex_pred =  tensor([-0.0310, -0.1739,  0.0280,  0.2782], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005827703047543764\n",
      "nex =  tensor([-0.0062, -0.1981, -0.0278,  0.2792])\n",
      "nex_pred =  tensor([-0.0194, -0.2249, -0.0087,  0.2432], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0074369218200445175\n",
      "nex =  tensor([-0.0057,  0.1609, -0.0453, -0.3311])\n",
      "nex_pred =  tensor([-0.0293,  0.1488, -0.0296, -0.3638], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005447120405733585\n",
      "nex =  tensor([-0.0308, -0.2358, -0.0274,  0.2512])\n",
      "nex_pred =  tensor([-0.0253, -0.2467, -0.0166,  0.2233], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0030235364101827145\n",
      "nex =  tensor([ 0.0117, -0.1533, -0.0193,  0.3176])\n",
      "nex_pred =  tensor([-0.0178, -0.1948,  0.0089,  0.2743], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01490774191915989\n",
      "nex =  tensor([-0.0392, -0.2211,  0.0391,  0.2959])\n",
      "nex_pred =  tensor([-0.0471, -0.2010,  0.0337,  0.2856], grad_fn=<ThAddBackward>)\n",
      "loss =  0.001614333363249898\n",
      "nex =  tensor([-0.0360,  0.1853,  0.0279, -0.2548])\n",
      "nex_pred =  tensor([-0.0583,  0.1922,  0.0276, -0.2806], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003816886804997921\n",
      "nex =  tensor([-0.0309, -0.1458, -0.0124,  0.2567])\n",
      "nex_pred =  tensor([-0.0286, -0.1748,  0.0092,  0.2167], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009799351915717125\n",
      "nex =  tensor([ 0.0243,  0.2453, -0.0442, -0.3088])\n",
      "nex_pred =  tensor([-0.0196,  0.1998, -0.0182, -0.3487], grad_fn=<ThAddBackward>)\n",
      "loss =  0.015759211033582687\n",
      "nex =  tensor([ 0.0168, -0.2126, -0.0050,  0.2831])\n",
      "nex_pred =  tensor([-0.0199, -0.2160,  0.0003,  0.2635], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004996422212570906\n",
      "nex =  tensor([-0.0346, -0.1831,  0.0273,  0.2857])\n",
      "nex_pred =  tensor([-0.0418, -0.1801,  0.0313,  0.2658], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0013915497111156583\n",
      "nex =  tensor([-0.0509, -0.2409,  0.0215,  0.2806])\n",
      "nex_pred =  tensor([-0.0450, -0.2247,  0.0183,  0.2652], grad_fn=<ThAddBackward>)\n",
      "loss =  0.001449519768357277\n",
      "nex =  tensor([-0.0199,  0.1490,  0.0407, -0.2959])\n",
      "nex_pred =  tensor([-0.0575,  0.1903,  0.0216, -0.3005], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010471239686012268\n",
      "nex =  tensor([ 0.0251,  0.1475, -0.0292, -0.3129])\n",
      "nex_pred =  tensor([-0.0247,  0.1484, -0.0255, -0.3304], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008051051758229733\n",
      "nex =  tensor([-0.0025, -0.2012, -0.0189,  0.2421])\n",
      "nex_pred =  tensor([-0.0197, -0.2102, -0.0101,  0.2177], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003338298061862588\n",
      "nex =  tensor([-0.0418, -0.1978, -0.0416,  0.2665])\n",
      "nex_pred =  tensor([-0.0244, -0.2362, -0.0145,  0.2203], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013777833431959152\n",
      "nex =  tensor([ 0.0483, -0.1756,  0.0125,  0.3357])\n",
      "nex_pred =  tensor([-0.0178, -0.1861,  0.0235,  0.3161], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013055474497377872\n",
      "nex =  tensor([-0.0403, -0.2146, -0.0293,  0.2362])\n",
      "nex_pred =  tensor([-0.0266, -0.2303, -0.0149,  0.2039], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005206657107919455\n",
      "nex =  tensor([ 0.0084,  0.2288,  0.0259, -0.2466])\n",
      "nex_pred =  tensor([-0.0439,  0.2199,  0.0281, -0.2660], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0094608748331666\n",
      "nex =  tensor([ 0.0446, -0.1871,  0.0433,  0.2564])\n",
      "nex_pred =  tensor([-0.0245, -0.1525,  0.0271,  0.2665], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01959855854511261\n",
      "nex =  tensor([-0.0198, -0.2428, -0.0171,  0.3205])\n",
      "nex_pred =  tensor([-0.0276, -0.2612, -0.0026,  0.2880], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004135618917644024\n",
      "nex =  tensor([-0.0019,  0.2359,  0.0059, -0.2808])\n",
      "nex_pred =  tensor([-0.0415,  0.2190,  0.0154, -0.3090], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007454984355717897\n",
      "nex =  tensor([ 0.0052, -0.1819, -0.0224,  0.3325])\n",
      "nex_pred =  tensor([-0.0195, -0.2219,  0.0047,  0.2888], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012783337384462357\n",
      "nex =  tensor([-0.0248,  0.1569, -0.0296, -0.2764])\n",
      "nex_pred =  tensor([-0.0392,  0.1407, -0.0125, -0.3159], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0072651635855436325\n",
      "nex =  tensor([-0.0300,  0.1545, -0.0386, -0.3004])\n",
      "nex_pred =  tensor([-0.0384,  0.1393, -0.0196, -0.3412], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006811021361500025\n",
      "nex =  tensor([-0.0352,  0.2278,  0.0079, -0.3220])\n",
      "nex_pred =  tensor([-0.0525,  0.2245,  0.0153, -0.3532], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0033876767847687006\n",
      "nex =  tensor([ 0.0231, -0.1559, -0.0478,  0.2298])\n",
      "nex_pred =  tensor([-0.0047, -0.1934, -0.0234,  0.1921], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01481874380260706\n",
      "nex =  tensor([ 0.0146, -0.2126, -0.0010,  0.2564])\n",
      "nex_pred =  tensor([-0.0207, -0.2063, -0.0012,  0.2427], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004420222714543343\n",
      "nex =  tensor([ 0.0384, -0.1481,  0.0308,  0.2546])\n",
      "nex_pred =  tensor([-0.0221, -0.1353,  0.0271,  0.2514], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012867244891822338\n",
      "nex =  tensor([ 0.0260,  0.1634, -0.0038, -0.3047])\n",
      "nex_pred =  tensor([-0.0312,  0.1740, -0.0084, -0.3149], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010126099921762943\n",
      "nex =  tensor([-0.0472,  0.2070,  0.0030, -0.3397])\n",
      "nex_pred =  tensor([-0.0550,  0.2119,  0.0092, -0.3713], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002801868598908186\n",
      "nex =  tensor([-0.0420, -0.2344,  0.0076,  0.2909])\n",
      "nex_pred =  tensor([-0.0391, -0.2320,  0.0117,  0.2682], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0014557673130184412\n",
      "nex =  tensor([-0.0470,  0.1992, -0.0363, -0.3398])\n",
      "nex_pred =  tensor([-0.0439,  0.1795, -0.0133, -0.3862], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0077333818189799786\n",
      "nex =  tensor([ 0.0232, -0.2276,  0.0354,  0.3090])\n",
      "nex_pred =  tensor([-0.0303, -0.2027,  0.0257,  0.3086], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009268335998058319\n",
      "nex =  tensor([-0.0446, -0.1497,  0.0167,  0.2886])\n",
      "nex_pred =  tensor([-0.0412, -0.1670,  0.0325,  0.2554], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005062939133495092\n",
      "nex =  tensor([-0.0287, -0.2044, -0.0095,  0.2461])\n",
      "nex_pred =  tensor([-0.0292, -0.2099, -0.0015,  0.2203], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002362367697060108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([-0.0253, -0.1897, -0.0088,  0.2775])\n",
      "nex_pred =  tensor([-0.0293, -0.2072,  0.0060,  0.2447], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004806928336620331\n",
      "nex =  tensor([-0.0479,  0.2098,  0.0208, -0.2896])\n",
      "nex_pred =  tensor([-0.0600,  0.2121,  0.0253, -0.3210], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003208750858902931\n",
      "nex =  tensor([-0.0127,  0.1644, -0.0218, -0.2506])\n",
      "nex_pred =  tensor([-0.0376,  0.1445, -0.0059, -0.2882], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008918552659451962\n",
      "nex =  tensor([-0.0489, -0.2322,  0.0489,  0.3164])\n",
      "nex_pred =  tensor([-0.0532, -0.2085,  0.0420,  0.3077], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0017661054152995348\n",
      "nex =  tensor([-0.0413,  0.1450,  0.0147, -0.2416])\n",
      "nex_pred =  tensor([-0.0564,  0.1530,  0.0163, -0.2709], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004049084149301052\n",
      "nex =  tensor([ 0.0165, -0.2400, -0.0103,  0.3235])\n",
      "nex_pred =  tensor([-0.0202, -0.2499, -0.0016,  0.2997], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005181504879146814\n",
      "nex =  tensor([-0.0424, -0.1963, -0.0027,  0.3049])\n",
      "nex_pred =  tensor([-0.0364, -0.2169,  0.0147,  0.2681], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0057913935743272305\n",
      "nex =  tensor([-0.0334, -0.1884, -0.0088,  0.3070])\n",
      "nex_pred =  tensor([-0.0324, -0.2152,  0.0118,  0.2674], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007497719023376703\n",
      "nex =  tensor([ 0.0156,  0.2014,  0.0171, -0.2771])\n",
      "nex_pred =  tensor([-0.0396,  0.2048,  0.0140, -0.2906], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009472993202507496\n",
      "nex =  tensor([-0.0386,  0.1643,  0.0224, -0.2913])\n",
      "nex_pred =  tensor([-0.0579,  0.1848,  0.0173, -0.3124], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0037499754689633846\n",
      "nex =  tensor([ 0.0461,  0.2419,  0.0302, -0.2973])\n",
      "nex_pred =  tensor([-0.0336,  0.2483,  0.0210, -0.2977], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01676080748438835\n",
      "nex =  tensor([ 0.0436, -0.2240, -0.0473,  0.2621])\n",
      "nex_pred =  tensor([-0.0013, -0.2467, -0.0331,  0.2362], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009687643498182297\n",
      "nex =  tensor([-0.0364, -0.2137, -0.0108,  0.3147])\n",
      "nex_pred =  tensor([-0.0332, -0.2368,  0.0075,  0.2771], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006006382871419191\n",
      "nex =  tensor([ 0.0395, -0.1760,  0.0039,  0.2717])\n",
      "nex_pred =  tensor([-0.0157, -0.1778,  0.0084,  0.2565], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010099953040480614\n",
      "nex =  tensor([ 0.0275,  0.2126, -0.0377, -0.3008])\n",
      "nex_pred =  tensor([-0.0208,  0.1812, -0.0192, -0.3331], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01268841978162527\n",
      "nex =  tensor([-0.0255,  0.1839, -0.0285, -0.2795])\n",
      "nex_pred =  tensor([-0.0395,  0.1595, -0.0080, -0.3226], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009099782444536686\n",
      "nex =  tensor([ 0.0220,  0.2365,  0.0106, -0.2442])\n",
      "nex_pred =  tensor([-0.0354,  0.2144,  0.0192, -0.2676], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012906182557344437\n",
      "nex =  tensor([ 0.0452, -0.2446,  0.0278,  0.2734])\n",
      "nex_pred =  tensor([-0.0215, -0.2075,  0.0096,  0.2820], grad_fn=<ThAddBackward>)\n",
      "loss =  0.016827035695314407\n",
      "nex =  tensor([-0.0371, -0.2100, -0.0456,  0.2765])\n",
      "nex_pred =  tensor([-0.0225, -0.2499, -0.0182,  0.2304], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01329097617417574\n",
      "nex =  tensor([ 0.0206, -0.1641, -0.0128,  0.3015])\n",
      "nex_pred =  tensor([-0.0169, -0.1920,  0.0073,  0.2675], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010886660777032375\n",
      "nex =  tensor([-0.0435, -0.1505, -0.0257,  0.3334])\n",
      "nex_pred =  tensor([-0.0308, -0.2096,  0.0140,  0.2731], grad_fn=<ThAddBackward>)\n",
      "loss =  0.02402820810675621\n",
      "nex =  tensor([-0.0062,  0.1923, -0.0054, -0.2671])\n",
      "nex_pred =  tensor([-0.0400,  0.1788,  0.0045, -0.2974], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007105136755853891\n",
      "nex =  tensor([ 0.0504, -0.1499, -0.0031,  0.3134])\n",
      "nex_pred =  tensor([-0.0120, -0.1736,  0.0149,  0.2859], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01576504111289978\n",
      "nex =  tensor([ 0.0039, -0.1690, -0.0340,  0.2430])\n",
      "nex_pred =  tensor([-0.0137, -0.1983, -0.0137,  0.2073], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00959501601755619\n",
      "nex =  tensor([ 0.0372, -0.1532, -0.0204,  0.2396])\n",
      "nex_pred =  tensor([-0.0085, -0.1718, -0.0069,  0.2141], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011377972550690174\n",
      "nex =  tensor([ 0.0244,  0.1727, -0.0347, -0.2801])\n",
      "nex_pred =  tensor([-0.0230,  0.1516, -0.0208, -0.3090], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011217725463211536\n",
      "nex =  tensor([-0.0495, -0.2107, -0.0094,  0.3271])\n",
      "nex_pred =  tensor([-0.0374, -0.2387,  0.0124,  0.2849], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008114309050142765\n",
      "nex =  tensor([-0.0131,  0.1777, -0.0097, -0.2722])\n",
      "nex_pred =  tensor([-0.0410,  0.1673,  0.0002, -0.3036], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0060631390661001205\n",
      "nex =  tensor([ 0.0462, -0.2066,  0.0394,  0.2917])\n",
      "nex_pred =  tensor([-0.0245, -0.1774,  0.0269,  0.2975], grad_fn=<ThAddBackward>)\n",
      "loss =  0.016660986468195915\n",
      "nex =  tensor([-0.0185,  0.1467, -0.0356, -0.2592])\n",
      "nex_pred =  tensor([-0.0357,  0.1256, -0.0163, -0.3003], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009327966719865799\n",
      "nex =  tensor([-0.0153, -0.2269,  0.0082,  0.3023])\n",
      "nex_pred =  tensor([-0.0326, -0.2258,  0.0122,  0.2818], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0019412487745285034\n",
      "nex =  tensor([-0.0171,  0.2216,  0.0502, -0.2360])\n",
      "nex_pred =  tensor([-0.0585,  0.2275,  0.0455, -0.2529], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006282012909650803\n",
      "nex =  tensor([-0.0319,  0.1824, -0.0231, -0.3401])\n",
      "nex_pred =  tensor([-0.0432,  0.1789, -0.0112, -0.3738], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0036533779930323362\n",
      "nex =  tensor([ 0.0464, -0.1812,  0.0112,  0.2593])\n",
      "nex_pred =  tensor([-0.0155, -0.1718,  0.0091,  0.2520], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012431188486516476\n",
      "nex =  tensor([ 0.0477, -0.2210, -0.0369,  0.2845])\n",
      "nex_pred =  tensor([-0.0037, -0.2419, -0.0235,  0.2586], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010762027464807034\n",
      "nex =  tensor([ 0.0173,  0.2220, -0.0450, -0.2623])\n",
      "nex_pred =  tensor([-0.0217,  0.1709, -0.0155, -0.3078], grad_fn=<ThAddBackward>)\n",
      "loss =  0.020370809361338615\n",
      "nex =  tensor([-0.0036, -0.2143,  0.0036,  0.2503])\n",
      "nex_pred =  tensor([-0.0265, -0.2050,  0.0023,  0.2364], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002437398536130786\n",
      "nex =  tensor([ 0.0444,  0.1803,  0.0169, -0.2409])\n",
      "nex_pred =  tensor([-0.0311,  0.1833,  0.0109, -0.2475], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01899092271924019\n",
      "nex =  tensor([ 0.0402, -0.1901, -0.0034,  0.3152])\n",
      "nex_pred =  tensor([-0.0152, -0.2039,  0.0083,  0.2916], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010660240426659584\n",
      "nex =  tensor([-0.0094,  0.2316, -0.0243, -0.2624])\n",
      "nex_pred =  tensor([-0.0353,  0.1896,  0.0013, -0.3083], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014810907654464245\n",
      "nex =  tensor([ 0.0185,  0.2430,  0.0388, -0.3197])\n",
      "nex_pred =  tensor([-0.0446,  0.2592,  0.0272, -0.3225], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010863753035664558\n",
      "nex =  tensor([ 0.0261, -0.1912,  0.0043,  0.3307])\n",
      "nex_pred =  tensor([-0.0214, -0.2051,  0.0168,  0.3057], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00842074491083622\n",
      "nex =  tensor([ 0.0329,  0.1573,  0.0028, -0.2754])\n",
      "nex_pred =  tensor([-0.0309,  0.1671, -0.0033, -0.2839], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013434028252959251\n",
      "nex =  tensor([ 0.0147,  0.1964, -0.0080, -0.3178])\n",
      "nex_pred =  tensor([-0.0331,  0.1952, -0.0055, -0.3363], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00705143203958869\n",
      "nex =  tensor([-0.0205, -0.2061, -0.0061,  0.2531])\n",
      "nex_pred =  tensor([-0.0282, -0.2094,  0.0003,  0.2295], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002036171266809106\n",
      "nex =  tensor([-0.0063, -0.1958,  0.0048,  0.2484])\n",
      "nex_pred =  tensor([-0.0272, -0.1912,  0.0063,  0.2315], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002359835198149085\n",
      "nex =  tensor([ 0.0240, -0.2329, -0.0379,  0.3152])\n",
      "nex_pred =  tensor([-0.0104, -0.2626, -0.0191,  0.2812], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009083742275834084\n",
      "nex =  tensor([ 0.0143,  0.2391, -0.0090, -0.2568])\n",
      "nex_pred =  tensor([-0.0323,  0.2053,  0.0082, -0.2902], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013463878072798252\n",
      "nex =  tensor([-0.0425,  0.1975, -0.0457, -0.2963])\n",
      "nex_pred =  tensor([-0.0397,  0.1600, -0.0146, -0.3514], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014982064254581928\n",
      "nex =  tensor([ 0.0386, -0.2219,  0.0450,  0.3400])\n",
      "nex_pred =  tensor([-0.0299, -0.1974,  0.0364,  0.3422], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013098693452775478\n",
      "nex =  tensor([ 0.0148,  0.1493,  0.0486, -0.3130])\n",
      "nex_pred =  tensor([-0.0493,  0.2031,  0.0188, -0.3010], grad_fn=<ThAddBackward>)\n",
      "loss =  0.02294616401195526\n",
      "nex =  tensor([-0.0264, -0.2300, -0.0341,  0.2687])\n",
      "nex_pred =  tensor([-0.0228, -0.2518, -0.0174,  0.2347], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005400271620601416\n",
      "nex =  tensor([ 0.0009, -0.2205,  0.0372,  0.3378])\n",
      "nex_pred =  tensor([-0.0375, -0.2073,  0.0353,  0.3282], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004303655121475458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([ 0.0382,  0.1468, -0.0090, -0.3040])\n",
      "nex_pred =  tensor([-0.0263,  0.1603, -0.0156, -0.3103], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01302644144743681\n",
      "nex =  tensor([ 0.0124,  0.1850, -0.0170, -0.2612])\n",
      "nex_pred =  tensor([-0.0313,  0.1658, -0.0051, -0.2906], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010249309241771698\n",
      "nex =  tensor([-0.0490, -0.2304,  0.0396,  0.2881])\n",
      "nex_pred =  tensor([-0.0497, -0.2063,  0.0321,  0.2787], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0019390730885788798\n",
      "nex =  tensor([ 0.0128,  0.2043,  0.0176, -0.2492])\n",
      "nex_pred =  tensor([-0.0405,  0.1992,  0.0186, -0.2675], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009916441515088081\n",
      "nex =  tensor([ 0.0126,  0.1727, -0.0329, -0.2892])\n",
      "nex_pred =  tensor([-0.0271,  0.1545, -0.0191, -0.3194], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008891811594367027\n",
      "nex =  tensor([-0.0412,  0.2062,  0.0026, -0.3136])\n",
      "nex_pred =  tensor([-0.0529,  0.2041,  0.0109, -0.3470], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003506010863929987\n",
      "nex =  tensor([-0.0168, -0.1728,  0.0319,  0.3041])\n",
      "nex_pred =  tensor([-0.0388, -0.1721,  0.0371,  0.2859], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0024118314031511545\n",
      "nex =  tensor([ 0.0502, -0.1662,  0.0214,  0.3345])\n",
      "nex_pred =  tensor([-0.0195, -0.1725,  0.0305,  0.3189], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013853000476956367\n",
      "nex =  tensor([-0.0198, -0.1668, -0.0388,  0.3250])\n",
      "nex_pred =  tensor([-0.0210, -0.2252, -0.0009,  0.2677], grad_fn=<ThAddBackward>)\n",
      "loss =  0.022077485918998718\n",
      "nex =  tensor([ 0.0437,  0.1626, -0.0147, -0.2821])\n",
      "nex_pred =  tensor([-0.0229,  0.1610, -0.0145, -0.2948], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013984410092234612\n",
      "nex =  tensor([ 0.0106, -0.2128,  0.0351,  0.3098])\n",
      "nex_pred =  tensor([-0.0333, -0.1947,  0.0297,  0.3042], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006126713007688522\n",
      "nex =  tensor([ 0.0373, -0.1619,  0.0235,  0.3402])\n",
      "nex_pred =  tensor([-0.0235, -0.1715,  0.0348,  0.3216], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011245735920965672\n",
      "nex =  tensor([ 0.0470, -0.2023,  0.0467,  0.3343])\n",
      "nex_pred =  tensor([-0.0277, -0.1798,  0.0395,  0.3371], grad_fn=<ThAddBackward>)\n",
      "loss =  0.015492522157728672\n",
      "nex =  tensor([ 0.0447, -0.1903, -0.0318,  0.2913])\n",
      "nex_pred =  tensor([-0.0057, -0.2184, -0.0132,  0.2597], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013274352997541428\n",
      "nex =  tensor([-0.0375, -0.1813,  0.0412,  0.3419])\n",
      "nex_pred =  tensor([-0.0483, -0.1842,  0.0498,  0.3200], grad_fn=<ThAddBackward>)\n",
      "loss =  0.001732388511300087\n",
      "nex =  tensor([ 0.0362,  0.2289, -0.0298, -0.3505])\n",
      "nex_pred =  tensor([-0.0204,  0.2116, -0.0196, -0.3722], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009658008813858032\n",
      "nex =  tensor([ 0.0420, -0.1993,  0.0496,  0.3551])\n",
      "nex_pred =  tensor([-0.0305, -0.1817,  0.0460,  0.3551], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013532386161386967\n",
      "nex =  tensor([-0.0037, -0.1962,  0.0340,  0.3341])\n",
      "nex_pred =  tensor([-0.0374, -0.1925,  0.0377,  0.3187], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003596761729568243\n",
      "nex =  tensor([-0.0487, -0.1938,  0.0039,  0.2679])\n",
      "nex_pred =  tensor([-0.0387, -0.2013,  0.0140,  0.2384], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0033910281490534544\n",
      "nex =  tensor([ 0.0221,  0.2274,  0.0435, -0.2907])\n",
      "nex_pred =  tensor([-0.0449,  0.2446,  0.0303, -0.2919], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013295305892825127\n",
      "nex =  tensor([-0.0087,  0.2211,  0.0502, -0.2325])\n",
      "nex_pred =  tensor([-0.0560,  0.2268,  0.0447, -0.2471], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007717242930084467\n",
      "nex =  tensor([ 0.0336,  0.1957, -0.0449, -0.3096])\n",
      "nex_pred =  tensor([-0.0173,  0.1683, -0.0276, -0.3393], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012213918380439281\n",
      "nex =  tensor([-0.0327, -0.2271, -0.0236,  0.3114])\n",
      "nex_pred =  tensor([-0.0287, -0.2544, -0.0036,  0.2722], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006972560193389654\n",
      "nex =  tensor([ 0.0283,  0.2276, -0.0311, -0.3076])\n",
      "nex_pred =  tensor([-0.0223,  0.1974, -0.0141, -0.3380], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012148790992796421\n",
      "nex =  tensor([-0.0434,  0.1881,  0.0451, -0.2483])\n",
      "nex_pred =  tensor([-0.0654,  0.2037,  0.0394, -0.2702], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003906359430402517\n",
      "nex =  tensor([ 0.0397, -0.2243,  0.0475,  0.3300])\n",
      "nex_pred =  tensor([-0.0300, -0.1945,  0.0358,  0.3356], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014656217768788338\n",
      "nex =  tensor([ 0.0256,  0.1487,  0.0410, -0.2851])\n",
      "nex_pred =  tensor([-0.0438,  0.1904,  0.0163, -0.2767], grad_fn=<ThAddBackward>)\n",
      "loss =  0.022268369793891907\n",
      "nex =  tensor([-0.0254, -0.1544, -0.0057,  0.2637])\n",
      "nex_pred =  tensor([-0.0293, -0.1770,  0.0121,  0.2283], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006841625552624464\n",
      "nex =  tensor([ 0.0122, -0.1791,  0.0351,  0.3474])\n",
      "nex_pred =  tensor([-0.0337, -0.1809,  0.0424,  0.3315], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006172596011310816\n",
      "nex =  tensor([-0.0064, -0.1966,  0.0363,  0.2626])\n",
      "nex_pred =  tensor([-0.0363, -0.1729,  0.0274,  0.2588], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004677931312471628\n",
      "nex =  tensor([ 0.0400, -0.1821, -0.0010,  0.2982])\n",
      "nex_pred =  tensor([-0.0151, -0.1923,  0.0085,  0.2772], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010460658930242062\n",
      "nex =  tensor([ 0.0037,  0.1735,  0.0083, -0.3183])\n",
      "nex_pred =  tensor([-0.0412,  0.1911,  0.0015, -0.3297], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006910908035933971\n",
      "nex =  tensor([-0.0012, -0.1938, -0.0378,  0.3274])\n",
      "nex_pred =  tensor([-0.0169, -0.2414, -0.0070,  0.2789], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01524059846997261\n",
      "nex =  tensor([-0.0206,  0.1864, -0.0493, -0.3512])\n",
      "nex_pred =  tensor([-0.0325,  0.1671, -0.0278, -0.3912], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006411855574697256\n",
      "nex =  tensor([ 0.0434, -0.1904,  0.0246,  0.3393])\n",
      "nex_pred =  tensor([-0.0227, -0.1893,  0.0293,  0.3268], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011585450731217861\n",
      "nex =  tensor([ 0.0044,  0.1578, -0.0394, -0.3393])\n",
      "nex_pred =  tensor([-0.0280,  0.1538, -0.0294, -0.3649], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004842129070311785\n",
      "nex =  tensor([-0.0402,  0.2169, -0.0414, -0.3038])\n",
      "nex_pred =  tensor([-0.0401,  0.1777, -0.0105, -0.3584], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014482595026493073\n",
      "nex =  tensor([-0.0409, -0.1725, -0.0213,  0.3007])\n",
      "nex_pred =  tensor([-0.0305, -0.2127,  0.0071,  0.2522], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013959761708974838\n",
      "nex =  tensor([ 0.0166,  0.1579, -0.0365, -0.2558])\n",
      "nex_pred =  tensor([-0.0250,  0.1337, -0.0199, -0.2891], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012188313528895378\n",
      "nex =  tensor([-0.0425, -0.2247, -0.0162,  0.3326])\n",
      "nex_pred =  tensor([-0.0340, -0.2541,  0.0058,  0.2905], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007895958609879017\n",
      "nex =  tensor([-0.0026,  0.1713,  0.0433, -0.2487])\n",
      "nex_pred =  tensor([-0.0527,  0.1946,  0.0298, -0.2561], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010747610591351986\n",
      "nex =  tensor([ 0.0138, -0.1747,  0.0159,  0.3392])\n",
      "nex_pred =  tensor([-0.0278, -0.1892,  0.0298,  0.3139], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0072610750794410706\n",
      "nex =  tensor([-0.0105,  0.1837, -0.0441, -0.3212])\n",
      "nex_pred =  tensor([-0.0308,  0.1613, -0.0236, -0.3601], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0076340059749782085\n",
      "nex =  tensor([ 0.0141, -0.1734, -0.0264,  0.2431])\n",
      "nex_pred =  tensor([-0.0132, -0.1943, -0.0111,  0.2134], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007667228579521179\n",
      "nex =  tensor([-0.0139,  0.2334,  0.0231, -0.3076])\n",
      "nex_pred =  tensor([-0.0502,  0.2362,  0.0231, -0.3281], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004504852462559938\n",
      "nex =  tensor([ 0.0218, -0.2291,  0.0329,  0.3225])\n",
      "nex_pred =  tensor([-0.0305, -0.2092,  0.0262,  0.3187], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008018574677407742\n",
      "nex =  tensor([ 0.0405, -0.1934, -0.0061,  0.3283])\n",
      "nex_pred =  tensor([-0.0148, -0.2115,  0.0082,  0.3018], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011205238290131092\n",
      "nex =  tensor([-0.0200, -0.1661,  0.0421,  0.3193])\n",
      "nex_pred =  tensor([-0.0429, -0.1648,  0.0475,  0.3027], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0022950596176087856\n",
      "nex =  tensor([ 0.0051,  0.2239,  0.0123, -0.2437])\n",
      "nex_pred =  tensor([-0.0411,  0.2060,  0.0206, -0.2701], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009729436598718166\n",
      "nex =  tensor([ 0.0503,  0.2278, -0.0056, -0.2897])\n",
      "nex_pred =  tensor([-0.0226,  0.2121, -0.0010, -0.3045], grad_fn=<ThAddBackward>)\n",
      "loss =  0.015574568882584572\n",
      "nex =  tensor([-0.0301, -0.1585, -0.0289,  0.3319])\n",
      "nex_pred =  tensor([-0.0265, -0.2153,  0.0089,  0.2743], grad_fn=<ThAddBackward>)\n",
      "loss =  0.02152368612587452\n",
      "nex =  tensor([-0.0145, -0.1790, -0.0048,  0.2911])\n",
      "nex_pred =  tensor([-0.0279, -0.1990,  0.0113,  0.2581], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005641452036798\n",
      "nex =  tensor([-0.0400, -0.2046,  0.0047,  0.2694])\n",
      "nex_pred =  tensor([-0.0368, -0.2074,  0.0118,  0.2437], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002144747180864215\n",
      "nex =  tensor([ 0.0339, -0.1642,  0.0258,  0.3046])\n",
      "nex_pred =  tensor([-0.0239, -0.1631,  0.0300,  0.2924], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010043651796877384\n",
      "nex =  tensor([-0.0021, -0.2373,  0.0041,  0.2859])\n",
      "nex_pred =  tensor([-0.0276, -0.2299,  0.0038,  0.2708], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0025142505764961243\n",
      "nex =  tensor([-0.0244, -0.2329, -0.0336,  0.2635])\n",
      "nex_pred =  tensor([-0.0223, -0.2519, -0.0187,  0.2316], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004525636788457632\n",
      "nex =  tensor([ 0.0321, -0.2023,  0.0016,  0.3294])\n",
      "nex_pred =  tensor([-0.0192, -0.2136,  0.0122,  0.3063], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008776918053627014\n",
      "nex =  tensor([ 0.0498, -0.1576, -0.0504,  0.2493])\n",
      "nex_pred =  tensor([ 0.0021, -0.1979, -0.0252,  0.2124], grad_fn=<ThAddBackward>)\n",
      "loss =  0.019419780001044273\n",
      "nex =  tensor([-0.0238,  0.1578, -0.0254, -0.3337])\n",
      "nex_pred =  tensor([-0.0403,  0.1601, -0.0168, -0.3628], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003244752297177911\n",
      "nex =  tensor([ 0.0132, -0.2082, -0.0136,  0.3351])\n",
      "nex_pred =  tensor([-0.0202, -0.2334,  0.0046,  0.3013], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008170523680746555\n",
      "nex =  tensor([ 0.0055, -0.2223, -0.0213,  0.2545])\n",
      "nex_pred =  tensor([-0.0176, -0.2288, -0.0143,  0.2326], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0032622588332742453\n",
      "nex =  tensor([-0.0185, -0.2363, -0.0390,  0.2541])\n",
      "nex_pred =  tensor([-0.0191, -0.2550, -0.0245,  0.2237], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004242576193064451\n",
      "nex =  tensor([ 0.0196, -0.1776, -0.0469,  0.3240])\n",
      "nex_pred =  tensor([-0.0089, -0.2329, -0.0122,  0.2729], grad_fn=<ThAddBackward>)\n",
      "loss =  0.02062699757516384\n",
      "nex =  tensor([ 0.0265, -0.2082, -0.0181,  0.3184])\n",
      "nex_pred =  tensor([-0.0150, -0.2305, -0.0022,  0.2880], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00891188345849514\n",
      "nex =  tensor([ 0.0104, -0.2135,  0.0271,  0.2796])\n",
      "nex_pred =  tensor([-0.0302, -0.1932,  0.0197,  0.2747], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006051457952708006\n",
      "nex =  tensor([-0.0476, -0.1855, -0.0055,  0.3225])\n",
      "nex_pred =  tensor([-0.0375, -0.2169,  0.0184,  0.2786], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009568081237375736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([ 0.0204,  0.2053, -0.0487, -0.3485])\n",
      "nex_pred =  tensor([-0.0202,  0.1816, -0.0309, -0.3796], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008574011735618114\n",
      "nex =  tensor([ 0.0153,  0.2400, -0.0226, -0.2670])\n",
      "nex_pred =  tensor([-0.0282,  0.1992, -0.0006, -0.3050], grad_fn=<ThAddBackward>)\n",
      "loss =  0.015250825323164463\n",
      "nex =  tensor([ 0.0270, -0.1614, -0.0422,  0.2557])\n",
      "nex_pred =  tensor([-0.0060, -0.1993, -0.0177,  0.2173], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01499586459249258\n",
      "nex =  tensor([-0.0403, -0.2170, -0.0450,  0.3153])\n",
      "nex_pred =  tensor([-0.0248, -0.2653, -0.0130,  0.2634], grad_fn=<ThAddBackward>)\n",
      "loss =  0.016266997903585434\n",
      "nex =  tensor([ 0.0248,  0.2313,  0.0194, -0.3140])\n",
      "nex_pred =  tensor([-0.0373,  0.2367,  0.0142, -0.3227], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010204714722931385\n",
      "nex =  tensor([-0.0434, -0.1664, -0.0475,  0.2980])\n",
      "nex_pred =  tensor([-0.0239, -0.2274, -0.0075,  0.2371], grad_fn=<ThAddBackward>)\n",
      "loss =  0.02709384076297283\n",
      "nex =  tensor([ 0.0343, -0.1999,  0.0340,  0.3114])\n",
      "nex_pred =  tensor([-0.0267, -0.1835,  0.0293,  0.3078], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010781568475067616\n",
      "nex =  tensor([ 0.0457, -0.2037,  0.0472,  0.2789])\n",
      "nex_pred =  tensor([-0.0262, -0.1666,  0.0301,  0.2903], grad_fn=<ThAddBackward>)\n",
      "loss =  0.019851384684443474\n",
      "nex =  tensor([-0.0064,  0.1810, -0.0442, -0.3107])\n",
      "nex_pred =  tensor([-0.0296,  0.1568, -0.0235, -0.3496], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00844796933233738\n",
      "nex =  tensor([-0.0414, -0.1645,  0.0451,  0.3206])\n",
      "nex_pred =  tensor([-0.0494, -0.1650,  0.0520,  0.3005], grad_fn=<ThAddBackward>)\n",
      "loss =  0.001410110155120492\n",
      "nex =  tensor([ 0.0217, -0.1645, -0.0387,  0.2977])\n",
      "nex_pred =  tensor([-0.0096, -0.2105, -0.0091,  0.2528], grad_fn=<ThAddBackward>)\n",
      "loss =  0.017425358295440674\n",
      "nex =  tensor([-0.0153, -0.1727, -0.0115,  0.2483])\n",
      "nex_pred =  tensor([-0.0249, -0.1884,  0.0018,  0.2181], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004715196322649717\n",
      "nex =  tensor([-0.0320,  0.1953,  0.0149, -0.2497])\n",
      "nex_pred =  tensor([-0.0534,  0.1884,  0.0221, -0.2820], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005014471244066954\n",
      "nex =  tensor([ 0.0032, -0.1795, -0.0053,  0.2527])\n",
      "nex_pred =  tensor([-0.0220, -0.1872,  0.0029,  0.2295], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0041876304894685745\n",
      "nex =  tensor([-0.0098, -0.1483,  0.0003,  0.2831])\n",
      "nex_pred =  tensor([-0.0274, -0.1714,  0.0183,  0.2496], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007134165149182081\n",
      "nex =  tensor([ 0.0421, -0.2215,  0.0135,  0.2998])\n",
      "nex_pred =  tensor([-0.0191, -0.2092,  0.0099,  0.2927], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010548059828579426\n",
      "nex =  tensor([ 0.0025,  0.1458,  0.0003, -0.2890])\n",
      "nex_pred =  tensor([-0.0395,  0.1595, -0.0036, -0.3043], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006786555051803589\n",
      "nex =  tensor([-0.0388, -0.1786, -0.0166,  0.3054])\n",
      "nex_pred =  tensor([-0.0315, -0.2144,  0.0093,  0.2597], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011507400311529636\n",
      "nex =  tensor([ 0.0055,  0.2392,  0.0263, -0.2449])\n",
      "nex_pred =  tensor([-0.0448,  0.2262,  0.0307, -0.2665], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009274238720536232\n",
      "nex =  tensor([-0.0328,  0.1455,  0.0060, -0.2717])\n",
      "nex_pred =  tensor([-0.0515,  0.1562,  0.0068, -0.2978], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0037114396691322327\n",
      "nex =  tensor([-0.0439, -0.1565,  0.0254,  0.2985])\n",
      "nex_pred =  tensor([-0.0438, -0.1680,  0.0381,  0.2694], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003338491078466177\n",
      "nex =  tensor([ 0.0257, -0.2014, -0.0212,  0.3233])\n",
      "nex_pred =  tensor([-0.0145, -0.2293, -0.0019,  0.2891], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010291296988725662\n",
      "nex =  tensor([ 0.0249, -0.2334,  0.0231,  0.2902])\n",
      "nex_pred =  tensor([-0.0259, -0.2105,  0.0137,  0.2879], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008554472588002682\n",
      "nex =  tensor([-0.0279,  0.2225, -0.0314, -0.3133])\n",
      "nex_pred =  tensor([-0.0392,  0.1916, -0.0072, -0.3594], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009811913594603539\n",
      "nex =  tensor([-0.0490,  0.2445,  0.0221, -0.2561])\n",
      "nex_pred =  tensor([-0.0602,  0.2259,  0.0357, -0.2964], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006386382970958948\n",
      "nex =  tensor([ 0.0151,  0.2368,  0.0264, -0.3263])\n",
      "nex_pred =  tensor([-0.0423,  0.2480,  0.0189, -0.3341], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008738799020648003\n",
      "nex =  tensor([-0.0434, -0.1965, -0.0328,  0.2950])\n",
      "nex_pred =  tensor([-0.0281, -0.2367, -0.0047,  0.2467], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013857636600732803\n",
      "nex =  tensor([-0.0456,  0.2224,  0.0318, -0.3247])\n",
      "nex_pred =  tensor([-0.0625,  0.2379,  0.0289, -0.3478], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002692260080948472\n",
      "nex =  tensor([ 0.0367,  0.1761,  0.0069, -0.2589])\n",
      "nex_pred =  tensor([-0.0307,  0.1779,  0.0035, -0.2694], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014790107496082783\n",
      "nex =  tensor([ 0.0409, -0.1979, -0.0335,  0.2914])\n",
      "nex_pred =  tensor([-0.0063, -0.2257, -0.0153,  0.2598], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012152611277997494\n",
      "nex =  tensor([ 0.0193, -0.1645, -0.0443,  0.2557])\n",
      "nex_pred =  tensor([-0.0074, -0.2040, -0.0186,  0.2156], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014768663793802261\n",
      "nex =  tensor([-0.0353, -0.2452,  0.0347,  0.3453])\n",
      "nex_pred =  tensor([-0.0469, -0.2335,  0.0342,  0.3302], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0011791216675192118\n",
      "nex =  tensor([-0.0451, -0.1795,  0.0157,  0.2664])\n",
      "nex_pred =  tensor([-0.0407, -0.1822,  0.0230,  0.2408], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0022544930689036846\n",
      "nex =  tensor([-0.0130, -0.2221, -0.0462,  0.2725])\n",
      "nex_pred =  tensor([-0.0162, -0.2548, -0.0242,  0.2341], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008565443567931652\n",
      "nex =  tensor([ 0.0287, -0.2064, -0.0027,  0.2981])\n",
      "nex_pred =  tensor([-0.0179, -0.2121,  0.0040,  0.2782], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007265000604093075\n",
      "nex =  tensor([-0.0309, -0.2039,  0.0346,  0.2616])\n",
      "nex_pred =  tensor([-0.0423, -0.1822,  0.0275,  0.2530], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0021495497785508633\n",
      "nex =  tensor([-0.0332, -0.2384,  0.0263,  0.3089])\n",
      "nex_pred =  tensor([-0.0427, -0.2248,  0.0242,  0.2945], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0012422294821590185\n",
      "nex =  tensor([-0.0305, -0.1725, -0.0225,  0.3061])\n",
      "nex_pred =  tensor([-0.0277, -0.2135,  0.0061,  0.2582], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01358169037848711\n",
      "nex =  tensor([ 0.0149, -0.1510, -0.0054,  0.2770])\n",
      "nex_pred =  tensor([-0.0193, -0.1722,  0.0108,  0.2473], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008756974712014198\n",
      "nex =  tensor([ 0.0196,  0.2094,  0.0311, -0.2942])\n",
      "nex_pred =  tensor([-0.0423,  0.2249,  0.0204, -0.2989], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011605005711317062\n",
      "nex =  tensor([-0.0494, -0.2077,  0.0076,  0.2805])\n",
      "nex_pred =  tensor([-0.0404, -0.2116,  0.0158,  0.2530], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0026086627040058374\n",
      "nex =  tensor([ 0.0140, -0.2174,  0.0451,  0.3228])\n",
      "nex_pred =  tensor([-0.0357, -0.1936,  0.0369,  0.3219], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007914690300822258\n",
      "nex =  tensor([ 0.0309,  0.2430, -0.0214, -0.2740])\n",
      "nex_pred =  tensor([-0.0239,  0.2051, -0.0026, -0.3061], grad_fn=<ThAddBackward>)\n",
      "loss =  0.015802571550011635\n",
      "nex =  tensor([ 0.0309,  0.1726, -0.0025, -0.2806])\n",
      "nex_pred =  tensor([-0.0299,  0.1745, -0.0041, -0.2935], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011709033511579037\n",
      "nex =  tensor([ 0.0046,  0.2116,  0.0234, -0.3027])\n",
      "nex_pred =  tensor([-0.0448,  0.2222,  0.0176, -0.3149], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007384076714515686\n",
      "nex =  tensor([-0.0225, -0.2415, -0.0464,  0.2713])\n",
      "nex_pred =  tensor([-0.0187, -0.2695, -0.0268,  0.2355], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006703448016196489\n",
      "nex =  tensor([ 0.0362, -0.1449, -0.0418,  0.3219])\n",
      "nex_pred =  tensor([-0.0057, -0.2030, -0.0048,  0.2707], grad_fn=<ThAddBackward>)\n",
      "loss =  0.025526197627186775\n",
      "nex =  tensor([ 0.0288,  0.2418, -0.0284, -0.2952])\n",
      "nex_pred =  tensor([-0.0226,  0.2051, -0.0090, -0.3280], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014206024818122387\n",
      "nex =  tensor([ 0.0081, -0.1745, -0.0053,  0.2607])\n",
      "nex_pred =  tensor([-0.0209, -0.1851,  0.0046,  0.2363], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005245574284344912\n",
      "nex =  tensor([ 0.0361,  0.2188, -0.0349, -0.3518])\n",
      "nex_pred =  tensor([-0.0191,  0.2018, -0.0241, -0.3740], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00945362914353609\n",
      "nex =  tensor([ 0.0376,  0.1820, -0.0139, -0.2560])\n",
      "nex_pred =  tensor([-0.0247,  0.1665, -0.0069, -0.2767], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014510729350149632\n",
      "nex =  tensor([-0.0144,  0.1536,  0.0338, -0.2429])\n",
      "nex_pred =  tensor([-0.0537,  0.1741,  0.0241, -0.2566], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007741459645330906\n",
      "nex =  tensor([-0.0383,  0.1661, -0.0105, -0.3259])\n",
      "nex_pred =  tensor([-0.0487,  0.1727, -0.0043, -0.3559], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0029649611096829176\n",
      "nex =  tensor([-0.0254,  0.2117, -0.0426, -0.3405])\n",
      "nex_pred =  tensor([-0.0355,  0.1847, -0.0184, -0.3849], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008399360813200474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([ 0.0155,  0.2405, -0.0088, -0.2506])\n",
      "nex_pred =  tensor([-0.0320,  0.2047,  0.0091, -0.2846], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014406198635697365\n",
      "nex =  tensor([ 0.0468, -0.2167,  0.0065,  0.3236])\n",
      "nex_pred =  tensor([-0.0168, -0.2164,  0.0100,  0.3092], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010851651430130005\n",
      "nex =  tensor([ 0.0093, -0.1489,  0.0013,  0.2844])\n",
      "nex_pred =  tensor([-0.0228, -0.1686,  0.0170,  0.2553], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007818173617124557\n",
      "nex =  tensor([ 0.0457,  0.2091, -0.0157, -0.2514])\n",
      "nex_pred =  tensor([-0.0214,  0.1821, -0.0042, -0.2749], grad_fn=<ThAddBackward>)\n",
      "loss =  0.017894191667437553\n",
      "nex =  tensor([ 0.0422, -0.1628, -0.0144,  0.2908])\n",
      "nex_pred =  tensor([-0.0106, -0.1864,  0.0026,  0.2619], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013276109471917152\n",
      "nex =  tensor([ 0.0130,  0.2128,  0.0246, -0.2803])\n",
      "nex_pred =  tensor([-0.0424,  0.2182,  0.0200, -0.2923], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00924611371010542\n",
      "nex =  tensor([-0.0064,  0.1462, -0.0138, -0.2750])\n",
      "nex_pred =  tensor([-0.0382,  0.1455, -0.0082, -0.3010], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005500643514096737\n",
      "nex =  tensor([-0.0029, -0.1978,  0.0068,  0.3026])\n",
      "nex_pred =  tensor([-0.0287, -0.2049,  0.0155,  0.2788], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0037482574116438627\n",
      "nex =  tensor([ 0.0010,  0.1657,  0.0284, -0.3146])\n",
      "nex_pred =  tensor([-0.0477,  0.1990,  0.0122, -0.3177], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010551675222814083\n",
      "nex =  tensor([ 0.0178, -0.2275,  0.0425,  0.3019])\n",
      "nex_pred =  tensor([-0.0334, -0.1964,  0.0294,  0.3051], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00988989882171154\n",
      "nex =  tensor([ 0.0301,  0.1929,  0.0404, -0.2430])\n",
      "nex_pred =  tensor([-0.0418,  0.2074,  0.0278, -0.2448], grad_fn=<ThAddBackward>)\n",
      "loss =  0.017602520063519478\n",
      "nex =  tensor([ 0.0408, -0.1825,  0.0417,  0.2724])\n",
      "nex_pred =  tensor([-0.0255, -0.1551,  0.0301,  0.2778], grad_fn=<ThAddBackward>)\n",
      "loss =  0.015938561409711838\n",
      "nex =  tensor([ 0.0449, -0.2172,  0.0454,  0.2621])\n",
      "nex_pred =  tensor([-0.0256, -0.1730,  0.0237,  0.2770], grad_fn=<ThAddBackward>)\n",
      "loss =  0.02197871543467045\n",
      "nex =  tensor([-0.0199, -0.2297, -0.0482,  0.2813])\n",
      "nex_pred =  tensor([-0.0177, -0.2649, -0.0246,  0.2409], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009374014101922512\n",
      "nex =  tensor([ 0.0047,  0.2018, -0.0023, -0.3304])\n",
      "nex_pred =  tensor([-0.0378,  0.2055, -0.0014, -0.3486], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005540020298212767\n",
      "nex =  tensor([ 0.0269,  0.2381,  0.0107, -0.2652])\n",
      "nex_pred =  tensor([-0.0340,  0.2217,  0.0162, -0.2844], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012243199162185192\n",
      "nex =  tensor([ 0.0244, -0.1648,  0.0279,  0.2724])\n",
      "nex_pred =  tensor([-0.0258, -0.1552,  0.0266,  0.2644], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00836125761270523\n",
      "nex =  tensor([-0.0254, -0.2396, -0.0014,  0.2742])\n",
      "nex_pred =  tensor([-0.0318, -0.2356,  0.0009,  0.2546], grad_fn=<ThAddBackward>)\n",
      "loss =  0.001225921674631536\n",
      "nex =  tensor([-0.0430,  0.2070,  0.0091, -0.2528])\n",
      "nex_pred =  tensor([-0.0550,  0.1920,  0.0220, -0.2924], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006370307877659798\n",
      "nex =  tensor([ 0.0371,  0.2106,  0.0296, -0.2546])\n",
      "nex_pred =  tensor([-0.0364,  0.2150,  0.0222, -0.2598], grad_fn=<ThAddBackward>)\n",
      "loss =  0.016520874574780464\n",
      "nex =  tensor([ 0.0391,  0.2041,  0.0163, -0.3342])\n",
      "nex_pred =  tensor([-0.0325,  0.2235,  0.0036, -0.3333], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014403087086975574\n",
      "nex =  tensor([ 0.0009, -0.1493, -0.0185,  0.3031])\n",
      "nex_pred =  tensor([-0.0203, -0.1893,  0.0089,  0.2597], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013837709091603756\n",
      "nex =  tensor([-0.0100, -0.2156, -0.0309,  0.2671])\n",
      "nex_pred =  tensor([-0.0193, -0.2368, -0.0151,  0.2347], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0053006005473434925\n",
      "nex =  tensor([-0.0083, -0.1473, -0.0115,  0.3074])\n",
      "nex_pred =  tensor([-0.0246, -0.1852,  0.0152,  0.2645], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012498622760176659\n",
      "nex =  tensor([-0.0119, -0.2251, -0.0383,  0.3285])\n",
      "nex_pred =  tensor([-0.0198, -0.2658, -0.0118,  0.2837], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011065560393035412\n",
      "nex =  tensor([ 0.0188, -0.2294, -0.0438,  0.2671])\n",
      "nex_pred =  tensor([-0.0086, -0.2525, -0.0286,  0.2381], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0066477516666054726\n",
      "nex =  tensor([ 0.0217, -0.1492, -0.0025,  0.2797])\n",
      "nex_pred =  tensor([-0.0184, -0.1686,  0.0126,  0.2521], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009356331080198288\n",
      "nex =  tensor([-0.0338,  0.1456, -0.0051, -0.2557])\n",
      "nex_pred =  tensor([-0.0487,  0.1440,  0.0028, -0.2891], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004736045375466347\n",
      "nex =  tensor([ 0.0477,  0.1850, -0.0188, -0.3250])\n",
      "nex_pred =  tensor([-0.0204,  0.1845, -0.0190, -0.3353], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01256402488797903\n",
      "nex =  tensor([-0.0090,  0.1845, -0.0304, -0.2709])\n",
      "nex_pred =  tensor([-0.0340,  0.1574, -0.0104, -0.3111], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010245040990412235\n",
      "nex =  tensor([-0.0321,  0.1925, -0.0047, -0.3055])\n",
      "nex_pred =  tensor([-0.0482,  0.1885,  0.0043, -0.3382], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0039293463341891766\n",
      "nex =  tensor([-0.0071,  0.1677, -0.0080, -0.2995])\n",
      "nex_pred =  tensor([-0.0399,  0.1701, -0.0045, -0.3230], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004786372650414705\n",
      "nex =  tensor([-0.0196, -0.1899,  0.0093,  0.2478])\n",
      "nex_pred =  tensor([-0.0318, -0.1856,  0.0115,  0.2293], grad_fn=<ThAddBackward>)\n",
      "loss =  0.001642103074118495\n",
      "nex =  tensor([-0.0272, -0.2277,  0.0500,  0.3555])\n",
      "nex_pred =  tensor([-0.0492, -0.2119,  0.0478,  0.3456], grad_fn=<ThAddBackward>)\n",
      "loss =  0.001960471738129854\n",
      "nex =  tensor([ 0.0208, -0.1566, -0.0410,  0.3263])\n",
      "nex_pred =  tensor([-0.0100, -0.2141, -0.0043,  0.2738], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0229098629206419\n",
      "nex =  tensor([-0.0203, -0.2223,  0.0459,  0.3274])\n",
      "nex_pred =  tensor([-0.0451, -0.2027,  0.0408,  0.3199], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0027119088917970657\n",
      "nex =  tensor([-0.0012, -0.1759,  0.0475,  0.3246])\n",
      "nex_pred =  tensor([-0.0398, -0.1662,  0.0480,  0.3155], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004466062877327204\n",
      "nex =  tensor([-0.0441, -0.2057,  0.0126,  0.2528])\n",
      "nex_pred =  tensor([-0.0395, -0.1985,  0.0142,  0.2327], grad_fn=<ThAddBackward>)\n",
      "loss =  0.001447124290280044\n",
      "nex =  tensor([ 0.0389,  0.1692, -0.0342, -0.3079])\n",
      "nex_pred =  tensor([-0.0190,  0.1585, -0.0264, -0.3276], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011021145619452\n",
      "nex =  tensor([ 0.0327,  0.1902, -0.0424, -0.2691])\n",
      "nex_pred =  tensor([-0.0183,  0.1553, -0.0221, -0.3031], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01613115705549717\n",
      "nex =  tensor([ 0.0248, -0.2074, -0.0213,  0.2923])\n",
      "nex_pred =  tensor([-0.0137, -0.2257, -0.0079,  0.2645], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007713403552770615\n",
      "nex =  tensor([ 0.0080, -0.2068, -0.0149,  0.2492])\n",
      "nex_pred =  tensor([-0.0184, -0.2116, -0.0088,  0.2283], grad_fn=<ThAddBackward>)\n",
      "loss =  0.003664931748062372\n",
      "nex =  tensor([ 0.0299, -0.1596,  0.0396,  0.2575])\n",
      "nex_pred =  tensor([-0.0269, -0.1389,  0.0316,  0.2584], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012124924920499325\n",
      "nex =  tensor([ 0.0185, -0.1776, -0.0197,  0.2490])\n",
      "nex_pred =  tensor([-0.0141, -0.1932, -0.0075,  0.2228], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006980287842452526\n",
      "nex =  tensor([-0.0121,  0.2055, -0.0394, -0.2604])\n",
      "nex_pred =  tensor([-0.0322,  0.1616, -0.0104, -0.3101], grad_fn=<ThAddBackward>)\n",
      "loss =  0.016897549852728844\n",
      "nex =  tensor([-0.0196, -0.1719, -0.0397,  0.2574])\n",
      "nex_pred =  tensor([-0.0187, -0.2115, -0.0127,  0.2126], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013756927102804184\n",
      "nex =  tensor([-0.0341,  0.1847,  0.0409, -0.2475])\n",
      "nex_pred =  tensor([-0.0614,  0.1990,  0.0353, -0.2679], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004468440543860197\n",
      "nex =  tensor([-0.0072,  0.2235, -0.0007, -0.3244])\n",
      "nex_pred =  tensor([-0.0416,  0.2182,  0.0053, -0.3496], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004777598660439253\n",
      "nex =  tensor([ 0.0220,  0.1539, -0.0238, -0.3225])\n",
      "nex_pred =  tensor([-0.0271,  0.1586, -0.0223, -0.3383], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007460375316441059\n",
      "nex =  tensor([-0.0305, -0.2293,  0.0423,  0.3486])\n",
      "nex_pred =  tensor([-0.0477, -0.2171,  0.0419,  0.3352], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0014847854617983103\n",
      "nex =  tensor([ 0.0198, -0.1785, -0.0161,  0.2495])\n",
      "nex_pred =  tensor([-0.0147, -0.1913, -0.0056,  0.2252], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0066775185987353325\n",
      "nex =  tensor([-0.0085, -0.1467,  0.0079,  0.3137])\n",
      "nex_pred =  tensor([-0.0301, -0.1726,  0.0281,  0.2793], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007872110232710838\n",
      "nex =  tensor([-0.0255,  0.1546, -0.0485, -0.3073])\n",
      "nex_pred =  tensor([-0.0343,  0.1347, -0.0265, -0.3497], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007914145477116108\n",
      "nex =  tensor([-0.0099,  0.2261,  0.0034, -0.2500])\n",
      "nex_pred =  tensor([-0.0432,  0.2019,  0.0173, -0.2846], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009162590838968754\n",
      "nex =  tensor([ 0.0204, -0.2341, -0.0328,  0.2418])\n",
      "nex_pred =  tensor([-0.0105, -0.2405, -0.0269,  0.2230], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004082377068698406\n",
      "nex =  tensor([ 0.0067,  0.2173, -0.0193, -0.2860])\n",
      "nex_pred =  tensor([-0.0321,  0.1914, -0.0034, -0.3193], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009831074625253677\n",
      "nex =  tensor([ 0.0476,  0.2020,  0.0299, -0.2538])\n",
      "nex_pred =  tensor([-0.0334,  0.2102,  0.0195, -0.2546], grad_fn=<ThAddBackward>)\n",
      "loss =  0.020488081499934196\n",
      "nex =  tensor([-0.0329,  0.1911,  0.0353, -0.2571])\n",
      "nex_pred =  tensor([-0.0595,  0.2020,  0.0319, -0.2792], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004084550775587559\n",
      "nex =  tensor([-0.0137,  0.2238, -0.0457, -0.3165])\n",
      "nex_pred =  tensor([-0.0309,  0.1844, -0.0173, -0.3641], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01259599532932043\n",
      "nex =  tensor([ 0.0437,  0.2083, -0.0436, -0.2879])\n",
      "nex_pred =  tensor([-0.0145,  0.1721, -0.0239, -0.3191], grad_fn=<ThAddBackward>)\n",
      "loss =  0.016784442588686943\n",
      "nex =  tensor([-0.0161, -0.2066, -0.0116,  0.2621])\n",
      "nex_pred =  tensor([-0.0259, -0.2156, -0.0021,  0.2356], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002894054399803281\n",
      "nex =  tensor([ 0.0263,  0.1479, -0.0159, -0.2809])\n",
      "nex_pred =  tensor([-0.0279,  0.1491, -0.0147, -0.2972], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010056941770017147\n",
      "nex =  tensor([ 0.0069,  0.2061, -0.0270, -0.2545])\n",
      "nex_pred =  tensor([-0.0299,  0.1702, -0.0057, -0.2940], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014184257946908474\n",
      "nex =  tensor([ 0.0404,  0.1869, -0.0449, -0.2862])\n",
      "nex_pred =  tensor([-0.0154,  0.1566, -0.0273, -0.3160], grad_fn=<ThAddBackward>)\n",
      "loss =  0.015065203420817852\n",
      "nex =  tensor([ 0.0054,  0.2041, -0.0227, -0.3382])\n",
      "nex_pred =  tensor([-0.0319,  0.1950, -0.0133, -0.3641], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00563735282048583\n",
      "nex =  tensor([-0.0040,  0.1685,  0.0335, -0.2925])\n",
      "nex_pred =  tensor([-0.0506,  0.1980,  0.0188, -0.2984], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009682211093604565\n",
      "nex =  tensor([-0.0404,  0.1670, -0.0469, -0.3546])\n",
      "nex_pred =  tensor([-0.0393,  0.1561, -0.0266, -0.3957], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005586693063378334\n",
      "nex =  tensor([-0.0405,  0.1657, -0.0216, -0.2941])\n",
      "nex_pred =  tensor([-0.0461,  0.1558, -0.0064, -0.3339], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005692523904144764\n",
      "nex =  tensor([ 0.0106,  0.2353,  0.0162, -0.2915])\n",
      "nex_pred =  tensor([-0.0406,  0.2298,  0.0179, -0.3096], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007955582812428474\n",
      "nex =  tensor([-0.0276, -0.2190, -0.0040,  0.2930])\n",
      "nex_pred =  tensor([-0.0321, -0.2284,  0.0062,  0.2647], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0027659141924232244\n",
      "nex =  tensor([ 0.0212, -0.1515,  0.0137,  0.2944])\n",
      "nex_pred =  tensor([-0.0234, -0.1624,  0.0245,  0.2729], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008071591146290302\n",
      "nex =  tensor([-0.0233, -0.2134,  0.0304,  0.3308])\n",
      "nex_pred =  tensor([-0.0417, -0.2089,  0.0338,  0.3128], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0017508546588942409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([-0.0038,  0.1528,  0.0353, -0.3190])\n",
      "nex_pred =  tensor([-0.0512,  0.1964,  0.0141, -0.3182], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012943343259394169\n",
      "nex =  tensor([ 0.0039,  0.1465,  0.0120, -0.2635])\n",
      "nex_pred =  tensor([-0.0422,  0.1612,  0.0058, -0.2771], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008513042703270912\n",
      "nex =  tensor([-0.0153, -0.2103,  0.0339,  0.3133])\n",
      "nex_pred =  tensor([-0.0399, -0.1986,  0.0328,  0.3009], grad_fn=<ThAddBackward>)\n",
      "loss =  0.002363306237384677\n",
      "nex =  tensor([ 0.0313, -0.2440,  0.0145,  0.3033])\n",
      "nex_pred =  tensor([-0.0225, -0.2267,  0.0080,  0.2978], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008356069214642048\n",
      "nex =  tensor([ 0.0389,  0.1925,  0.0467, -0.2547])\n",
      "nex_pred =  tensor([-0.0409,  0.2155,  0.0285, -0.2491], grad_fn=<ThAddBackward>)\n",
      "loss =  0.02231607586145401\n",
      "nex =  tensor([-0.0281, -0.1448, -0.0412,  0.2468])\n",
      "nex_pred =  tensor([-0.0198, -0.1921, -0.0094,  0.1963], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0201856829226017\n",
      "nex =  tensor([-0.0133,  0.1545,  0.0053, -0.3065])\n",
      "nex_pred =  tensor([-0.0456,  0.1725,  0.0009, -0.3233], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004853922873735428\n",
      "nex =  tensor([-0.0445,  0.1663,  0.0410, -0.2471])\n",
      "nex_pred =  tensor([-0.0647,  0.1865,  0.0340, -0.2684], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004325232468545437\n",
      "nex =  tensor([ 0.0147, -0.2171,  0.0146,  0.2924])\n",
      "nex_pred =  tensor([-0.0261, -0.2075,  0.0131,  0.2808], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005204551387578249\n",
      "nex =  tensor([-0.0336,  0.2153, -0.0295, -0.3130])\n",
      "nex_pred =  tensor([-0.0415,  0.1879, -0.0063, -0.3590], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009051891043782234\n",
      "nex =  tensor([ 0.0377, -0.2291, -0.0472,  0.2390])\n",
      "nex_pred =  tensor([-0.0021, -0.2449, -0.0367,  0.2176], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007147092372179031\n",
      "nex =  tensor([ 0.0190,  0.2023, -0.0246, -0.3178])\n",
      "nex_pred =  tensor([-0.0272,  0.1878, -0.0143, -0.3430], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008160583674907684\n",
      "nex =  tensor([-0.0039,  0.2142,  0.0074, -0.2468])\n",
      "nex_pred =  tensor([-0.0426,  0.1965,  0.0172, -0.2766], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008539432659745216\n",
      "nex =  tensor([ 0.0318, -0.1859,  0.0142,  0.2686])\n",
      "nex_pred =  tensor([-0.0204, -0.1775,  0.0130,  0.2590], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008797721937298775\n",
      "nex =  tensor([ 0.0287,  0.2087,  0.0174, -0.2649])\n",
      "nex_pred =  tensor([-0.0356,  0.2074,  0.0149, -0.2768], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012645690701901913\n",
      "nex =  tensor([ 0.0338, -0.2007, -0.0461,  0.2458])\n",
      "nex_pred =  tensor([-0.0034, -0.2263, -0.0294,  0.2168], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00977951567620039\n",
      "nex =  tensor([ 0.0418,  0.1964,  0.0456, -0.2308])\n",
      "nex_pred =  tensor([-0.0396,  0.2109,  0.0312, -0.2284], grad_fn=<ThAddBackward>)\n",
      "loss =  0.022776875644922256\n",
      "nex =  tensor([ 0.0393,  0.2355, -0.0327, -0.2585])\n",
      "nex_pred =  tensor([-0.0183,  0.1888, -0.0094, -0.2943], grad_fn=<ThAddBackward>)\n",
      "loss =  0.020733073353767395\n",
      "nex =  tensor([-0.0153, -0.1510,  0.0205,  0.3455])\n",
      "nex_pred =  tensor([-0.0365, -0.1757,  0.0411,  0.3121], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006876747123897076\n",
      "nex =  tensor([ 0.0193, -0.1451, -0.0088,  0.3149])\n",
      "nex_pred =  tensor([-0.0184, -0.1796,  0.0156,  0.2773], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013331656344234943\n",
      "nex =  tensor([-0.0179, -0.2302,  0.0413,  0.2819])\n",
      "nex_pred =  tensor([-0.0417, -0.1991,  0.0287,  0.2807], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004623601213097572\n",
      "nex =  tensor([-0.0025,  0.1816, -0.0113, -0.3132])\n",
      "nex_pred =  tensor([-0.0375,  0.1809, -0.0064, -0.3368], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004995508119463921\n",
      "nex =  tensor([-0.0225, -0.2182,  0.0307,  0.2789])\n",
      "nex_pred =  tensor([-0.0398, -0.1982,  0.0245,  0.2699], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0022894032299518585\n",
      "nex =  tensor([-0.0208, -0.2103, -0.0329,  0.2687])\n",
      "nex_pred =  tensor([-0.0215, -0.2364, -0.0139,  0.2321], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006948851514607668\n",
      "nex =  tensor([-0.0233, -0.2450, -0.0036,  0.2580])\n",
      "nex_pred =  tensor([-0.0302, -0.2362, -0.0041,  0.2417], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0010928880656138062\n",
      "nex =  tensor([ 0.0276,  0.2304, -0.0416, -0.3041])\n",
      "nex_pred =  tensor([-0.0195,  0.1909, -0.0190, -0.3402], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014569124206900597\n",
      "nex =  tensor([ 0.0086,  0.1798, -0.0137, -0.3026])\n",
      "nex_pred =  tensor([-0.0335,  0.1759, -0.0085, -0.3251], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006587511394172907\n",
      "nex =  tensor([ 0.0198, -0.1519,  0.0071,  0.3183])\n",
      "nex_pred =  tensor([-0.0228, -0.1736,  0.0246,  0.2894], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009692694060504436\n",
      "nex =  tensor([ 0.0002, -0.1550,  0.0444,  0.3440])\n",
      "nex_pred =  tensor([-0.0389, -0.1589,  0.0533,  0.3279], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004958681296557188\n",
      "nex =  tensor([-0.0020,  0.2047,  0.0147, -0.2642])\n",
      "nex_pred =  tensor([-0.0442,  0.2005,  0.0173, -0.2865], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006885912269353867\n",
      "nex =  tensor([-0.0459, -0.1735, -0.0444,  0.2477])\n",
      "nex_pred =  tensor([-0.0239, -0.2167, -0.0141,  0.1979], grad_fn=<ThAddBackward>)\n",
      "loss =  0.018600011244416237\n",
      "nex =  tensor([-0.0152, -0.1903, -0.0203,  0.2967])\n",
      "nex_pred =  tensor([-0.0242, -0.2198,  0.0009,  0.2576], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008293641731142998\n",
      "nex =  tensor([-0.0341, -0.2382,  0.0366,  0.2920])\n",
      "nex_pred =  tensor([-0.0451, -0.2129,  0.0278,  0.2849], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0023446306586265564\n",
      "nex =  tensor([ 0.0461,  0.2316, -0.0176, -0.3197])\n",
      "nex_pred =  tensor([-0.0206,  0.2140, -0.0102, -0.3376], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012896237894892693\n",
      "nex =  tensor([ 0.0400, -0.1678, -0.0325,  0.2545])\n",
      "nex_pred =  tensor([-0.0053, -0.1944, -0.0147,  0.2242], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012915987521409988\n",
      "nex =  tensor([ 0.0429, -0.2184,  0.0427,  0.3258])\n",
      "nex_pred =  tensor([-0.0276, -0.1923,  0.0329,  0.3292], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014515193179249763\n",
      "nex =  tensor([-0.0180, -0.1692,  0.0410,  0.3460])\n",
      "nex_pred =  tensor([-0.0431, -0.1742,  0.0506,  0.3258], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0029881733935326338\n",
      "nex =  tensor([-0.0098, -0.2130,  0.0492,  0.3287])\n",
      "nex_pred =  tensor([-0.0432, -0.1927,  0.0437,  0.3236], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004005380906164646\n",
      "nex =  tensor([-0.0403,  0.2319, -0.0299, -0.3037])\n",
      "nex_pred =  tensor([-0.0432,  0.1953, -0.0019, -0.3558], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01258330512791872\n",
      "nex =  tensor([ 0.0276, -0.1945, -0.0050,  0.2681])\n",
      "nex_pred =  tensor([-0.0164, -0.1979,  0.0002,  0.2500], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006948503199964762\n",
      "nex =  tensor([-0.0337, -0.1869, -0.0105,  0.2897])\n",
      "nex_pred =  tensor([-0.0314, -0.2108,  0.0084,  0.2519], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006833192892372608\n",
      "nex =  tensor([ 0.0053,  0.1844, -0.0072, -0.2571])\n",
      "nex_pred =  tensor([-0.0361,  0.1706,  0.0017, -0.2849], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008716041222214699\n",
      "nex =  tensor([-0.0186, -0.1871, -0.0186,  0.3303])\n",
      "nex_pred =  tensor([-0.0266, -0.2256,  0.0082,  0.2851], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011329226195812225\n",
      "nex =  tensor([-0.0474, -0.2133,  0.0121,  0.2531])\n",
      "nex_pred =  tensor([-0.0403, -0.2046,  0.0130,  0.2335], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0015295161865651608\n",
      "nex =  tensor([-0.0269, -0.1484,  0.0284,  0.2884])\n",
      "nex_pred =  tensor([-0.0397, -0.1553,  0.0379,  0.2648], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0026186059694737196\n",
      "nex =  tensor([-0.0378,  0.1606,  0.0473, -0.2438])\n",
      "nex_pred =  tensor([-0.0645,  0.1867,  0.0360, -0.2599], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005966307129710913\n",
      "nex =  tensor([-0.0206,  0.1986,  0.0349, -0.2426])\n",
      "nex_pred =  tensor([-0.0555,  0.2034,  0.0329, -0.2638], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005359183996915817\n",
      "nex =  tensor([ 0.0201, -0.1487,  0.0140,  0.2931])\n",
      "nex_pred =  tensor([-0.0236, -0.1601,  0.0251,  0.2713], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008020834065973759\n",
      "nex =  tensor([ 0.0209,  0.1581, -0.0418, -0.3325])\n",
      "nex_pred =  tensor([-0.0224,  0.1516, -0.0324, -0.3553], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006809513084590435\n",
      "nex =  tensor([ 0.0088,  0.1554, -0.0102, -0.2778])\n",
      "nex_pred =  tensor([-0.0346,  0.1557, -0.0074, -0.2986], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007291523739695549\n",
      "nex =  tensor([-0.0299, -0.2384,  0.0442,  0.3490])\n",
      "nex_pred =  tensor([-0.0482, -0.2220,  0.0414,  0.3382], grad_fn=<ThAddBackward>)\n",
      "loss =  0.001710815355181694\n",
      "nex =  tensor([ 0.0240,  0.2080,  0.0400, -0.2687])\n",
      "nex_pred =  tensor([-0.0434,  0.2235,  0.0277, -0.2711], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014422407373785973\n",
      "nex =  tensor([ 0.0171,  0.1728, -0.0159, -0.2698])\n",
      "nex_pred =  tensor([-0.0303,  0.1614, -0.0081, -0.2942], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009467393159866333\n",
      "nex =  tensor([-0.0319,  0.1926, -0.0075, -0.2557])\n",
      "nex_pred =  tensor([-0.0471,  0.1727,  0.0086, -0.2965], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007932037115097046\n",
      "nex =  tensor([ 0.0369,  0.1951,  0.0043, -0.3007])\n",
      "nex_pred =  tensor([-0.0298,  0.1998, -0.0001, -0.3091], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012653080746531487\n",
      "nex =  tensor([ 0.0453,  0.2378, -0.0403, -0.2686])\n",
      "nex_pred =  tensor([-0.0144,  0.1882, -0.0154, -0.3050], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0218941792845726\n",
      "nex =  tensor([ 0.0072, -0.1511, -0.0386,  0.2428])\n",
      "nex_pred =  tensor([-0.0114, -0.1887, -0.0135,  0.2024], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01393247488886118\n",
      "nex =  tensor([-0.0403,  0.1823,  0.0330, -0.2589])\n",
      "nex_pred =  tensor([-0.0611,  0.1947,  0.0301, -0.2829], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0036593815311789513\n",
      "nex =  tensor([ 0.0334,  0.1792, -0.0436, -0.3379])\n",
      "nex_pred =  tensor([-0.0180,  0.1663, -0.0328, -0.3603], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008878933265805244\n",
      "nex =  tensor([-0.0465,  0.1571, -0.0486, -0.2872])\n",
      "nex_pred =  tensor([-0.0404,  0.1293, -0.0206, -0.3394], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012892312370240688\n",
      "nex =  tensor([ 0.0061, -0.2135, -0.0417,  0.3049])\n",
      "nex_pred =  tensor([-0.0135, -0.2514, -0.0173,  0.2640], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010899878107011318\n",
      "nex =  tensor([ 0.0225,  0.2244, -0.0058, -0.2626])\n",
      "nex_pred =  tensor([-0.0309,  0.2002,  0.0057, -0.2893], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012370451353490353\n",
      "nex =  tensor([ 0.0350,  0.2098, -0.0218, -0.3339])\n",
      "nex_pred =  tensor([-0.0231,  0.2001, -0.0160, -0.3519], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009647556580603123\n",
      "nex =  tensor([-0.0114,  0.1627,  0.0434, -0.2645])\n",
      "nex_pred =  tensor([-0.0555,  0.1929,  0.0278, -0.2714], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010019310750067234\n",
      "nex =  tensor([ 0.0325, -0.1713,  0.0401,  0.3546])\n",
      "nex_pred =  tensor([-0.0299, -0.1706,  0.0463,  0.3435], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010212830267846584\n",
      "nex =  tensor([-0.0283, -0.2217, -0.0403,  0.2777])\n",
      "nex_pred =  tensor([-0.0218, -0.2534, -0.0181,  0.2376], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008772526867687702\n",
      "nex =  tensor([ 0.0053,  0.1883,  0.0469, -0.2426])\n",
      "nex_pred =  tensor([-0.0511,  0.2070,  0.0342, -0.2488], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011997265741229057\n",
      "nex =  tensor([-0.0146,  0.2189,  0.0330, -0.2922])\n",
      "nex_pred =  tensor([-0.0532,  0.2295,  0.0282, -0.3086], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005156559869647026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nex =  tensor([-0.0388,  0.1738,  0.0414, -0.3052])\n",
      "nex_pred =  tensor([-0.0633,  0.2082,  0.0277, -0.3178], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005999395623803139\n",
      "nex =  tensor([-0.0336,  0.2343, -0.0052, -0.2762])\n",
      "nex_pred =  tensor([-0.0480,  0.2068,  0.0143, -0.3197], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008883370086550713\n",
      "nex =  tensor([-0.0434,  0.1662, -0.0023, -0.2974])\n",
      "nex_pred =  tensor([-0.0524,  0.1702,  0.0042, -0.3297], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0034500977490097284\n",
      "nex =  tensor([-0.0404,  0.2134,  0.0007, -0.2925])\n",
      "nex_pred =  tensor([-0.0520,  0.2015,  0.0133, -0.3304], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005132434889674187\n",
      "nex =  tensor([-0.0484, -0.2149,  0.0066,  0.2727])\n",
      "nex_pred =  tensor([-0.0397, -0.2152,  0.0126,  0.2476], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0021265908144414425\n",
      "nex =  tensor([ 0.0228, -0.1701,  0.0419,  0.3465])\n",
      "nex_pred =  tensor([-0.0326, -0.1680,  0.0471,  0.3354], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00829483661800623\n",
      "nex =  tensor([ 0.0012, -0.1734, -0.0409,  0.2956])\n",
      "nex_pred =  tensor([-0.0142, -0.2206, -0.0102,  0.2479], grad_fn=<ThAddBackward>)\n",
      "loss =  0.016491932794451714\n",
      "nex =  tensor([-0.0172, -0.1550, -0.0074,  0.3316])\n",
      "nex_pred =  tensor([-0.0290, -0.1952,  0.0211,  0.2862], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012627743184566498\n",
      "nex =  tensor([ 0.0335,  0.2305, -0.0439, -0.3528])\n",
      "nex_pred =  tensor([-0.0173,  0.2033, -0.0269, -0.3813], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010402580723166466\n",
      "nex =  tensor([ 0.0069,  0.2290, -0.0178, -0.3305])\n",
      "nex_pred =  tensor([-0.0325,  0.2124, -0.0061, -0.3587], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006853604689240456\n",
      "nex =  tensor([ 0.0320, -0.2150,  0.0348,  0.2938])\n",
      "nex_pred =  tensor([-0.0271, -0.1892,  0.0242,  0.2952], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011661984026432037\n",
      "nex =  tensor([-0.0283,  0.1786, -0.0490, -0.3038])\n",
      "nex_pred =  tensor([-0.0348,  0.1486, -0.0223, -0.3517], grad_fn=<ThAddBackward>)\n",
      "loss =  0.011084061115980148\n",
      "nex =  tensor([ 0.0207, -0.1990,  0.0032,  0.2763])\n",
      "nex_pred =  tensor([-0.0208, -0.1982,  0.0063,  0.2600], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005833442322909832\n",
      "nex =  tensor([-0.0309, -0.1613,  0.0100,  0.2811])\n",
      "nex_pred =  tensor([-0.0357, -0.1758,  0.0237,  0.2505], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004174593836069107\n",
      "nex =  tensor([-0.0451,  0.1962,  0.0026, -0.2935])\n",
      "nex_pred =  tensor([-0.0541,  0.1918,  0.0123, -0.3294], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004155789967626333\n",
      "nex =  tensor([-0.0494,  0.1824, -0.0018, -0.3305])\n",
      "nex_pred =  tensor([-0.0544,  0.1899,  0.0042, -0.3622], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0029607866890728474\n",
      "nex =  tensor([ 0.0449,  0.2424,  0.0084, -0.3229])\n",
      "nex_pred =  tensor([-0.0280,  0.2401,  0.0058, -0.3305], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013253948651254177\n",
      "nex =  tensor([ 0.0476,  0.1564,  0.0079, -0.3346])\n",
      "nex_pred =  tensor([-0.0281,  0.1873, -0.0099, -0.3279], grad_fn=<ThAddBackward>)\n",
      "loss =  0.018952403217554092\n",
      "nex =  tensor([-0.0351, -0.1593,  0.0377,  0.3428])\n",
      "nex_pred =  tensor([-0.0464, -0.1715,  0.0516,  0.3162], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0030978519935160875\n",
      "nex =  tensor([-0.0050,  0.1689, -0.0479, -0.2581])\n",
      "nex_pred =  tensor([-0.0282,  0.1320, -0.0215, -0.3039], grad_fn=<ThAddBackward>)\n",
      "loss =  0.015050331130623817\n",
      "nex =  tensor([-0.0217,  0.2045,  0.0249, -0.2397])\n",
      "nex_pred =  tensor([-0.0529,  0.1993,  0.0288, -0.2669], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005556180607527494\n",
      "nex =  tensor([-0.0314,  0.2439,  0.0291, -0.2450])\n",
      "nex_pred =  tensor([-0.0568,  0.2285,  0.0383, -0.2778], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005872501526027918\n",
      "nex =  tensor([-0.0274, -0.2070,  0.0452,  0.2929])\n",
      "nex_pred =  tensor([-0.0454, -0.1846,  0.0382,  0.2863], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0025281819980591536\n",
      "nex =  tensor([-0.0152,  0.1789,  0.0075, -0.2892])\n",
      "nex_pred =  tensor([-0.0465,  0.1848,  0.0082, -0.3117], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00447391951456666\n",
      "nex =  tensor([ 0.0077,  0.1867,  0.0145, -0.3367])\n",
      "nex_pred =  tensor([-0.0417,  0.2094,  0.0042, -0.3436], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008082546293735504\n",
      "nex =  tensor([ 0.0279,  0.2287, -0.0338, -0.2852])\n",
      "nex_pred =  tensor([-0.0216,  0.1901, -0.0127, -0.3202], grad_fn=<ThAddBackward>)\n",
      "loss =  0.015237604267895222\n",
      "nex =  tensor([ 0.0221,  0.1545,  0.0396, -0.3057])\n",
      "nex_pred =  tensor([-0.0445,  0.1986,  0.0145, -0.2970], grad_fn=<ThAddBackward>)\n",
      "loss =  0.020527200773358345\n",
      "nex =  tensor([ 0.0213, -0.2286,  0.0130,  0.2758])\n",
      "nex_pred =  tensor([-0.0236, -0.2113,  0.0066,  0.2695], grad_fn=<ThAddBackward>)\n",
      "loss =  0.006673804949969053\n",
      "nex =  tensor([ 0.0261,  0.2385, -0.0271, -0.3309])\n",
      "nex_pred =  tensor([-0.0240,  0.2135, -0.0127, -0.3586], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010017704218626022\n",
      "nex =  tensor([ 0.0466,  0.2237, -0.0124, -0.3193])\n",
      "nex_pred =  tensor([-0.0220,  0.2124, -0.0086, -0.3336], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012852256186306477\n",
      "nex =  tensor([ 0.0064,  0.2203, -0.0164, -0.2821])\n",
      "nex_pred =  tensor([-0.0329,  0.1943, -0.0008, -0.3152], grad_fn=<ThAddBackward>)\n",
      "loss =  0.009947732090950012\n",
      "nex =  tensor([ 0.0379, -0.1522, -0.0002,  0.3239])\n",
      "nex_pred =  tensor([-0.0163, -0.1777,  0.0192,  0.2940], grad_fn=<ThAddBackward>)\n",
      "loss =  0.013527330942451954\n",
      "nex =  tensor([-0.0007, -0.1873, -0.0076,  0.2879])\n",
      "nex_pred =  tensor([-0.0236, -0.2039,  0.0062,  0.2584], grad_fn=<ThAddBackward>)\n",
      "loss =  0.005420438013970852\n",
      "nex =  tensor([ 0.0320, -0.2272,  0.0236,  0.3333])\n",
      "nex_pred =  tensor([-0.0257, -0.2158,  0.0217,  0.3247], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008713996037840843\n",
      "nex =  tensor([-0.0209,  0.2269, -0.0071, -0.2865])\n",
      "nex_pred =  tensor([-0.0437,  0.2044,  0.0089, -0.3244], grad_fn=<ThAddBackward>)\n",
      "loss =  0.007428490556776524\n",
      "nex =  tensor([-0.0181,  0.2419, -0.0317, -0.2813])\n",
      "nex_pred =  tensor([-0.0358,  0.1957, -0.0021, -0.3320], grad_fn=<ThAddBackward>)\n",
      "loss =  0.015796933323144913\n",
      "nex =  tensor([ 0.0050,  0.1638,  0.0473, -0.2632])\n",
      "nex_pred =  tensor([-0.0516,  0.1971,  0.0279, -0.2635], grad_fn=<ThAddBackward>)\n",
      "loss =  0.014946780167520046\n",
      "nex =  tensor([-0.0397, -0.2191, -0.0193,  0.2579])\n",
      "nex_pred =  tensor([-0.0299, -0.2320, -0.0068,  0.2267], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0040727038867771626\n",
      "nex =  tensor([-0.0314, -0.1806, -0.0478,  0.2515])\n",
      "nex_pred =  tensor([-0.0194, -0.2234, -0.0185,  0.2040], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01615334115922451\n",
      "nex =  tensor([ 0.0004, -0.2027,  0.0202,  0.2577])\n",
      "nex_pred =  tensor([-0.0300, -0.1864,  0.0151,  0.2491], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00394328311085701\n",
      "nex =  tensor([ 0.0313, -0.2093,  0.0162,  0.2973])\n",
      "nex_pred =  tensor([-0.0223, -0.1997,  0.0146,  0.2877], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008378816768527031\n",
      "nex =  tensor([ 0.0051, -0.1951,  0.0385,  0.3242])\n",
      "nex_pred =  tensor([-0.0359, -0.1846,  0.0381,  0.3144], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004969400819391012\n",
      "nex =  tensor([-0.0372, -0.2261, -0.0385,  0.3218])\n",
      "nex_pred =  tensor([-0.0261, -0.2683, -0.0102,  0.2737], grad_fn=<ThAddBackward>)\n",
      "loss =  0.01263468712568283\n",
      "nex =  tensor([ 0.0315,  0.2242,  0.0238, -0.2942])\n",
      "nex_pred =  tensor([-0.0365,  0.2302,  0.0170, -0.3004], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012766088359057903\n",
      "nex =  tensor([ 0.0396, -0.2106, -0.0004,  0.2964])\n",
      "nex_pred =  tensor([-0.0157, -0.2113,  0.0032,  0.2807], grad_fn=<ThAddBackward>)\n",
      "loss =  0.00908281933516264\n",
      "nex =  tensor([ 0.0315,  0.1948, -0.0366, -0.2701])\n",
      "nex_pred =  tensor([-0.0202,  0.1625, -0.0182, -0.3025], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0151594877243042\n",
      "nex =  tensor([-0.0331, -0.1588,  0.0102,  0.2830])\n",
      "nex_pred =  tensor([-0.0363, -0.1748,  0.0248,  0.2513], grad_fn=<ThAddBackward>)\n",
      "loss =  0.004550439305603504\n",
      "nex =  tensor([ 0.0204,  0.2144,  0.0327, -0.3314])\n",
      "nex_pred =  tensor([-0.0427,  0.2397,  0.0174, -0.3311], grad_fn=<ThAddBackward>)\n",
      "loss =  0.012228251434862614\n",
      "nex =  tensor([ 0.0133, -0.2183, -0.0444,  0.2817])\n",
      "nex_pred =  tensor([-0.0102, -0.2497, -0.0240,  0.2464], grad_fn=<ThAddBackward>)\n",
      "loss =  0.008891844190657139\n",
      "nex =  tensor([ 0.0047,  0.2141,  0.0447, -0.2370])\n",
      "nex_pred =  tensor([-0.0504,  0.2206,  0.0380, -0.2483], grad_fn=<ThAddBackward>)\n",
      "loss =  0.010095450095832348\n",
      "nex =  tensor([-0.0462, -0.2069,  0.0185,  0.3427])\n",
      "nex_pred =  tensor([-0.0447, -0.2194,  0.0322,  0.3112], grad_fn=<ThAddBackward>)\n",
      "loss =  0.0033191926777362823\n",
      "nex =  tensor([ 0.0021,  0.2409, -0.0333, -0.2706])\n",
      "nex_pred =  tensor([-0.0293,  0.1924, -0.0049, -0.3171], grad_fn=<ThAddBackward>)\n",
      "loss =  0.017345035448670387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11ee715f8>]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = []\n",
    "for epoch in range(500): \n",
    "    \n",
    "    curr = env.reset()\n",
    "    for i in range(200):\n",
    "\n",
    "        # Generate a random step \n",
    "        st = random.randint(0,1)\n",
    "\n",
    "        # Get simulated result from the environment\n",
    "        nex, rew, done, info = env.step(st)\n",
    "        nex = torch.from_numpy(nex).float()\n",
    "        \n",
    "        # Check if done and then break\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Create input for our network and generate prediction\n",
    "        input = torch.from_numpy(np.append(curr,st)).float()\n",
    "        nex_pred = model(input)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(nex_pred, nex) / torch.norm(nex)\n",
    "        \n",
    "        if i %100 == 0:\n",
    "            print(\"nex = \", nex)\n",
    "            print(\"nex_pred = \", nex_pred)\n",
    "            print(\"loss = \", loss.item())\n",
    "\n",
    "\n",
    "        # Backprop\n",
    "        # opt.zero_grad()\n",
    "        # loss.backward()\n",
    "        # opt.step()\n",
    "        \n",
    "        curr = nex\n",
    "        \n",
    "    t.append(loss.item())\n",
    "    \n",
    "    # epoch % 1000 == 0 and print(\"Epoch %d done\" % epoch)\n",
    "    \n",
    "plt.plot(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04015303, -0.03614903,  0.03989927,  0.02288957])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = env.reset()\n",
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "?model.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "?nn.BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
